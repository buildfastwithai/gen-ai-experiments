{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_m6KHELVZEF"
      },
      "source": [
        "# Beginner's Guide to Understanding LLM Pricing üí∞\n",
        "\n",
        "Welcome to this guide on Large Language Model (LLM) pricing! If you're building with AI, understanding costs is crucial. In this notebook, we'll break down:\n",
        "\n",
        "1.  **Tokens**: What they are and why they matter.\n",
        "2.  **Pricing Models**: Input vs. Output costs.\n",
        "3.  **Cost Calculator**: A Python tool to estimate your bills.\n",
        "4.  **Model Comparison**: Choosing the right model for your budget.\n",
        "\n",
        "Let's dive in! üöÄ"
      ],
      "id": "O_m6KHELVZEF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1dBxM9kZpjTDwh_1U01VTmGR3HgO74zk4?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn by building. Get expert mentorship and work on real AI projects.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)"
      ],
      "metadata": {
        "id": "Os6QPjgqOZJ5"
      },
      "id": "Os6QPjgqOZJ5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install tiktoken openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8r4doAbKgQk9",
        "outputId": "d1fa3f6e-6ba4-41e6-d8bb-a094611ffabf"
      },
      "id": "8r4doAbKgQk9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.16.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oAAlYIMVZEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c2986b-1ca7-4434-d9c4-4df9f61d4449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: OPENAI_API_KEY not found in userdata. Some examples might need it.\n",
            "Please set it in the 'Secrets' tab in Google Colab.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Setup environment variables using colab.userdata\n",
        "try:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"API Key loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"Warning: OPENAI_API_KEY not found in userdata. Some examples might need it.\")\n",
        "    print(\"Please set it in the 'Secrets' tab in Google Colab.\")"
      ],
      "id": "0oAAlYIMVZEI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fYciqg2VZEJ"
      },
      "source": [
        "## 1. What are Tokens? üî†\n",
        "\n",
        "LLMs don't read words like we do; they read \"tokens\".\n",
        "\n",
        "-   A token can be a word, part of a word, or even a space.\n",
        "-   **Rule of Thumb**: 1,000 tokens $\\approx$ 750 words.\n",
        "-   **Pricing**: You are charged per **1 million tokens (1M)**.\n",
        "\n",
        "### The Theory: Byte Pair Encoding (BPE)\n",
        "Most modern LLMs use a tokenization method called **Causal Language Modeling** relying on **Byte Pair Encoding (BPE)**.\n",
        "\n",
        "Instead of defining every word in the dictionary, the model learns designed \"sub-word\" units.\n",
        "- Common words like \"apple\" are single tokens.\n",
        "- Rare words like \"antidisestablishmentarianism\" are split into multiple tokens (`anti`, `dis`, `establishment`...).\n",
        "\n",
        "This efficiency allows the model to handle any text, even made-up words, while keeping the vocabulary memory manageable."
      ],
      "id": "1fYciqg2VZEJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgIZ9oBGVZEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc435a1f-85f4-4777-d8cb-3362257709ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'Generative AI is transforming the world!'\n",
            "Word Count: 6\n",
            "Token Count: 8\n",
            "Token IDs: [5926, 1799, 20837, 382, 64779, 290, 2375, 0]\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def count_tokens(text, model=\"gpt-4o\"):\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    tokens = encoding.encode(text)\n",
        "    return len(tokens), tokens\n",
        "\n",
        "text_sample = \"Generative AI is transforming the world!\"\n",
        "count, token_list = count_tokens(text_sample)\n",
        "\n",
        "print(f\"Text: '{text_sample}'\")\n",
        "print(f\"Word Count: {len(text_sample.split())}\")\n",
        "print(f\"Token Count: {count}\")\n",
        "print(f\"Token IDs: {token_list}\")"
      ],
      "id": "TgIZ9oBGVZEK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwYGk8PbVZEL"
      },
      "source": [
        "## 2. The Pricing Model üè∑Ô∏è\n",
        "\n",
        "Most API providers (OpenAI, Anthropic, etc.) split costs into two parts:\n",
        "\n",
        "1.  **Input Tokens (Prompt)**: What you send to the AI. usually cheaper.\n",
        "2.  **Output Tokens (Completion)**: What the AI writes back. Usually more expensive (3x - 4x input price).\n",
        "\n",
        "### Deep Dive: Why is Output more expensive?\n",
        "You might think processing text is harder than writing it, but for an LLM (Transformer architecture), it's the opposite!\n",
        "\n",
        "1.  **Parallel vs. Serial**: When the model reads your **Input**, it processes all tokens *in parallel* (all at once). This is very optimized for GPUs.\n",
        "2.  **Autoregression**: When the model generates **Output**, it must do it *one token at a time*. It predicts token A, adds it to the list, then predicts token B, and so on. This serial process is much slower and consumes more specialized compute resources (memory bandwidth), justifying the higher price."
      ],
      "id": "YwYGk8PbVZEL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbaTqzfeVZEL"
      },
      "source": [
        "## 3. Cost Estimator Calculator üßÆ\n",
        "\n",
        "Let's build a calculator to estimate how much a task will cost.\n",
        "\n",
        "**Example Rates (approximate):**\n",
        "- **GPT-4o**: $2.50 / 1M input, $10.00 / 1M output\n",
        "- **GPT-4o-mini**: $0.15 / 1M input, $0.60 / 1M output\n",
        "- **Claude 3.5 Sonnet**: $3.00 / 1M input, $15.00 / 1M output"
      ],
      "id": "XbaTqzfeVZEL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIn8BgtEVZEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92745097-f156-4339-b0d0-d61bd67922b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Model': 'gpt-4o', 'Input Tokens': 1100, 'Output Tokens': 100, 'Total Cost ($)': '$0.003750'}\n",
            "{'Model': 'gpt-4o-mini', 'Input Tokens': 1100, 'Output Tokens': 100, 'Total Cost ($)': '$0.000225'}\n"
          ]
        }
      ],
      "source": [
        "def calculate_cost(input_text, output_text, model_name=\"gpt-4o\"):\n",
        "    # Standard pricing per 1M tokens (as of late 2024 - verify current rates)\n",
        "    pricing = {\n",
        "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
        "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
        "        \"claude-3-5-sonnet\": {\"input\": 3.00, \"output\": 15.00}\n",
        "    }\n",
        "\n",
        "    if model_name not in pricing:\n",
        "        return \"Model pricing not found.\"\n",
        "\n",
        "    in_count, _ = count_tokens(input_text, model_name)\n",
        "    out_count, _ = count_tokens(output_text, model_name)\n",
        "\n",
        "    input_cost = (in_count / 1_000_000) * pricing[model_name][\"input\"]\n",
        "    output_cost = (out_count / 1_000_000) * pricing[model_name][\"output\"]\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Input Tokens\": in_count,\n",
        "        \"Output Tokens\": out_count,\n",
        "        \"Total Cost ($)\": f\"${total_cost:.6f}\"\n",
        "    }\n",
        "\n",
        "# Example Usage\n",
        "user_prompt = \"Summarize this 5000-word annual report...\" * 100\n",
        "ai_response = \"Here is the summary...\" * 20\n",
        "\n",
        "print(calculate_cost(user_prompt, ai_response, \"gpt-4o\"))\n",
        "print(calculate_cost(user_prompt, ai_response, \"gpt-4o-mini\"))"
      ],
      "id": "bIn8BgtEVZEM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_7b2pkpVZEM"
      },
      "source": [
        "## 4. Comparison & Strategy üìä\n",
        "\n",
        "| Model Tier | Best For | Typical Price Range (Input/Output per 1M) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Flagship** (GPT-4o, Claude 3.5 Sonnet) | Complex logic, coding, creative writing | $2.50 - $3.00 / $10.00 - $15.00 |\n",
        "| **Efficient** (GPT-4o-mini, Haiku) | Summaries, simple chat, high volume | $0.15 - $0.25 / $0.60 - $1.25 |\n",
        "| **Open Source** (Hosted Llama 3) | Privacy, specific tasks, lowest cost | Varies (often very cheap on Groq/TogetherAI) |\n",
        "\n",
        "### üí° Pro Tips for Saving Money:\n",
        "1.  **Use efficient models** for simple tasks.\n",
        "2.  **Optimize prompts** to be concise (reduce input tokens).\n",
        "3.  **Limit output length** (don't ask for 1000 words if 100 will do).\n",
        "4.  **Batch processing** (some providers offer 50% off for non-urgent requests)."
      ],
      "id": "h_7b2pkpVZEM"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}