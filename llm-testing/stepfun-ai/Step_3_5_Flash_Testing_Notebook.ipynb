{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- No coding experience required\n",
        "- Join Innovation Community\n",
        "\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Step 3.5 Flash - Complete Testing Notebook\n",
        "\n",
        "## Model Overview\n",
        "\n",
        "**Step 3.5 Flash** is StepFun's most capable open-source foundation model, engineered to deliver frontier reasoning and agentic capabilities with exceptional efficiency.\n",
        "\n",
        "### Key Specifications\n",
        "\n",
        "| Specification | Value |\n",
        "|--------------|-------|\n",
        "| **Architecture** | Sparse Mixture of Experts (MoE) |\n",
        "| **Total Parameters** | 196B |\n",
        "| **Active Parameters** | ~11B (per token) |\n",
        "| **Context Window** | 256K tokens |\n",
        "| **Vocabulary Size** | 128,896 tokens |\n",
        "| **Layers** | 45-layer Transformer (4,096 hidden dim) |\n",
        "| **License** | Apache 2.0 |\n",
        "\n",
        "### üåü Key Capabilities\n",
        "\n",
        "1. **Deep Reasoning at Speed**: Generation throughput of 100‚Äì300 tok/s (peaks at 350 tok/s)\n",
        "2. **Robust Coding & Agents**: 74.4% on SWE-bench Verified, 51.0% on Terminal-Bench 2.0\n",
        "3. **Efficient Long Context**: 256K context window with 3:1 Sliding Window Attention\n",
        "4. **Tool Calling & Function Calling**: Native support for function calling and tool use\n",
        "5. **Reasoning Tokens**: Support for step-by-step reasoning process\n",
        "\n",
        "### MoE Architecture Highlights\n",
        "\n",
        "- **288 routed experts per layer** + 1 shared expert (always active)\n",
        "- **Top-8 experts** selected per token\n",
        "- **3-way Multi-Token Prediction (MTP-3)** for faster generation\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "overview"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Section 1: Installation & Setup"
      ],
      "metadata": {
        "id": "section1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and setup client\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Initialize OpenRouter client for Step 3.5 Flash (FREE tier)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "# Model identifier for Step 3.5 Flash (free tier)\n",
        "MODEL_NAME = \"stepfun/step-3.5-flash:free\"\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"üìç Using model: {MODEL_NAME}\")\n",
        "print(f\"üåê Provider: OpenRouter (Free Tier)\")"
      ],
      "metadata": {
        "id": "setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef234ce0-6072-4e65-e282-09403812ca5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup complete!\n",
            "üìç Using model: stepfun/step-3.5-flash:free\n",
            "üåê Provider: OpenRouter (Free Tier)\n",
            "‚úÖ Setup complete!\n",
            "üìç Using model: stepfun/step-3.5-flash:free\n",
            "üåê Provider: OpenRouter (Free Tier)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üéØ Section 2: Basic Usage - Hello World"
      ],
      "metadata": {
        "id": "section2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic \"Hello World\" test\n",
        "start_time = time.time()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an AI chat assistant provided by StepFun. You are good at Chinese, English, and many other languages.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Hello! Introduce yourself briefly.\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(\"ü§ñ Response:\")\n",
        "print(response.choices[0].message.content)\n",
        "print(f\"\\n‚è±Ô∏è Response time: {elapsed:.2f}s\")\n",
        "\n",
        "# Display usage stats if available\n",
        "if response.usage:\n",
        "    print(f\"üìä Tokens - Prompt: {response.usage.prompt_tokens}, Completion: {response.usage.completion_tokens}\")"
      ],
      "metadata": {
        "id": "hello_world",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c797f9b-b941-4705-8bee-68a1e6063f58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Response:\n",
            "Hello! I‚Äôm **Step**, an AI model developed by **StepFun** (Èò∂Ë∑ÉÊòüËæ∞). I‚Äôm designed to understand and generate natural language, reason logically, and even interpret images. I can help with a wide range of tasks ‚Äî from answering questions and writing creatively, to analyzing data and solving problems.  \n",
            "\n",
            "I support multiple languages (including Chinese, English, and more), and I aim to be honest, helpful, and respectful in our conversations.  \n",
            "\n",
            "Feel free to ask me anything ‚Äî I‚Äôm here to assist! üòä\n",
            "\n",
            "‚è±Ô∏è Response time: 2.60s\n",
            "üìä Tokens - Prompt: 47, Completion: 220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üß† Section 3: Reasoning Capabilities\n",
        "\n",
        "Step 3.5 Flash supports reasoning-enabled responses that show step-by-step thinking process."
      ],
      "metadata": {
        "id": "section3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing reasoning capabilities with a math problem\n",
        "reasoning_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Solve this step by step:\n",
        "\n",
        "A farmer has chickens and cows. If the animals have a total of 50 heads and 140 legs, how many chickens and how many cows are there?\n",
        "\n",
        "Show your reasoning process clearly.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(\"üßÆ Math Problem Solution:\")\n",
        "print(\"=\"*50)\n",
        "print(reasoning_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "reasoning_math"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing complex reasoning - Logic puzzle\n",
        "logic_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Solve this logic puzzle:\n",
        "\n",
        "Five houses in a row are each painted a different color. In each house lives a person of a different nationality.\n",
        "Each person drinks a different beverage, smokes a different brand of cigarettes, and keeps a different pet.\n",
        "\n",
        "Given clues:\n",
        "1. The Brit lives in the red house.\n",
        "2. The Swede keeps dogs.\n",
        "3. The Dane drinks tea.\n",
        "4. The green house is immediately to the left of the white house.\n",
        "5. The owner of the green house drinks coffee.\n",
        "\n",
        "Question: Based on these clues, what can you deduce about the arrangement?\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.5,\n",
        "    max_tokens=1500\n",
        ")\n",
        "\n",
        "print(\"üß© Logic Puzzle Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(logic_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "reasoning_logic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056f3f29-68cb-4a0f-8a5a-26a8f68dd1da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß© Logic Puzzle Analysis:\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üíª Section 4: Coding Capabilities\n",
        "\n",
        "Step 3.5 Flash achieves **74.4% on SWE-bench Verified**, making it excellent for coding tasks."
      ],
      "metadata": {
        "id": "section5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code generation test\n",
        "code_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert Python developer. Write clean, well-documented code with type hints.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Create a Python class for a simple in-memory cache with the following features:\n",
        "1. Set a value with an optional TTL (time-to-live in seconds)\n",
        "2. Get a value (return None if expired or not found)\n",
        "3. Delete a value\n",
        "4. Clear all values\n",
        "5. Get cache stats (hits, misses, size)\n",
        "\n",
        "Include example usage.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "print(\"üíª Generated Python Code:\")\n",
        "print(\"=\"*50)\n",
        "print(code_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "code_generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code review and bug fixing\n",
        "buggy_code = \"\"\"\n",
        "def find_duplicates(lst):\n",
        "    duplicates = []\n",
        "    for i in range(len(lst)):\n",
        "        for j in range(len(lst)):\n",
        "            if lst[i] == lst[j]:\n",
        "                duplicates.append(lst[i])\n",
        "    return duplicates\n",
        "\"\"\"\n",
        "\n",
        "review_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Review this Python code, identify the bugs/issues, and provide a corrected version:\n",
        "\n",
        "```python\n",
        "{buggy_code}\n",
        "```\n",
        "\n",
        "Explain each issue found.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_tokens=1500\n",
        ")\n",
        "\n",
        "print(\"üîç Code Review:\")\n",
        "print(\"=\"*50)\n",
        "print(review_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "code_review"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Algorithm implementation\n",
        "algo_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Implement a Trie (Prefix Tree) data structure in Python with the following methods:\n",
        "1. insert(word) - Insert a word\n",
        "2. search(word) - Return True if word exists\n",
        "3. starts_with(prefix) - Return True if any word starts with prefix\n",
        "4. get_all_words_with_prefix(prefix) - Return all words with given prefix\n",
        "\n",
        "Include time complexity comments.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_tokens=2000\n",
        ")\n",
        "\n",
        "print(\"üå≥ Trie Implementation:\")\n",
        "print(\"=\"*50)\n",
        "print(algo_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "algorithm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üåä Section 5: Streaming Responses"
      ],
      "metadata": {
        "id": "section6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming response example\n",
        "print(\"üìñ Streaming Story Generation:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a short story (about 200 words) about an AI learning to appreciate art.\"\n",
        "        }\n",
        "    ],\n",
        "    stream=True,\n",
        "    temperature=0.8,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "full_response = \"\"\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        full_response += content\n",
        "        print(content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\n‚úÖ Streaming complete!\")"
      ],
      "metadata": {
        "id": "streaming"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üåç Section 6: Multilingual Capabilities"
      ],
      "metadata": {
        "id": "section8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test multilingual capabilities\n",
        "languages = [\n",
        "    (\"Chinese\", \"Áî®‰∏≠ÊñáÂÜô‰∏ÄÈ¶ñÂÖ≥‰∫é‰∫∫Â∑•Êô∫ËÉΩÁöÑÁü≠ËØó\"),\n",
        "    (\"Japanese\", \"Êó•Êú¨Ë™û„ÅßAI„Å´„Å§„ÅÑ„Å¶„ÅÆ‰ø≥Âè•„ÇíÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ\"),\n",
        "    (\"Spanish\", \"Escribe un breve p√°rrafo sobre el futuro de la IA en espa√±ol\"),\n",
        "    (\"French\", \"√âcrivez trois avantages de l'intelligence artificielle en fran√ßais\")\n",
        "]\n",
        "\n",
        "print(\"üåç Multilingual Test:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for lang_name, prompt in languages:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    print(f\"\\nüìå {lang_name}:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(response.choices[0].message.content)\n",
        "    print()"
      ],
      "metadata": {
        "id": "multilingual"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üí¨ Section 7: Multi-Turn Conversation"
      ],
      "metadata": {
        "id": "section9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-turn conversation example\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful AI tutor teaching Python programming. Be encouraging and provide examples.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "user_messages = [\n",
        "    \"Hi! I want to learn about Python decorators. What are they?\",\n",
        "    \"Can you show me a simple example?\",\n",
        "    \"How would I create a decorator that logs function execution time?\"\n",
        "]\n",
        "\n",
        "print(\"üë®‚Äçüè´ Python Tutorial Conversation:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for user_msg in user_messages:\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_msg})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=conversation,\n",
        "        temperature=0.5,\n",
        "        max_tokens=800\n",
        "    )\n",
        "\n",
        "    assistant_msg = response.choices[0].message.content\n",
        "    conversation.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "    print(f\"\\nüë§ User: {user_msg}\")\n",
        "    print(f\"\\nü§ñ Assistant: {assistant_msg}\")\n",
        "    print(\"-\"*40)"
      ],
      "metadata": {
        "id": "multi_turn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìù Section 8: Long Context Demo (256K)"
      ],
      "metadata": {
        "id": "section10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating long context processing\n",
        "# Generate a large text to analyze\n",
        "\n",
        "chapters = []\n",
        "for i in range(1, 6):\n",
        "    chapters.append(f\"\"\"\n",
        "Chapter {i}: The Development of Artificial Intelligence\n",
        "\n",
        "In this chapter, we explore the evolution of AI from its early beginnings to the present day.\n",
        "The field of AI was formally founded in 1956 at the Dartmouth Conference.\n",
        "Early AI focused on symbolic reasoning and expert systems.\n",
        "The advent of machine learning in the 1980s marked a paradigm shift.\n",
        "Deep learning, emerging in the 2010s, revolutionized the field.\n",
        "Today, large language models represent the cutting edge of AI research.\n",
        "The future promises even more advanced AI systems with enhanced reasoning capabilities.\n",
        "\"\"\")\n",
        "\n",
        "long_document = \"\\n\".join(chapters)\n",
        "\n",
        "# Analyze the document\n",
        "analysis_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Here is a multi-chapter document:\n",
        "\n",
        "{long_document}\n",
        "\n",
        "Please provide:\n",
        "1. A summary of the main themes\n",
        "2. Key milestones mentioned\n",
        "3. The overall narrative arc\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.4,\n",
        "    max_tokens=800\n",
        ")\n",
        "\n",
        "print(\"üìö Long Context Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input document: {len(long_document)} characters\")\n",
        "print(\"\\n\" + analysis_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "long_context"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ‚ö° Section 9: Performance Benchmarking"
      ],
      "metadata": {
        "id": "section11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from statistics import mean, stdev\n",
        "\n",
        "# Performance benchmark\n",
        "def benchmark_request(prompt, n_runs=5):\n",
        "    times = []\n",
        "    token_counts = []\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        start = time.time()\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=200\n",
        "        )\n",
        "        elapsed = time.time() - start\n",
        "        times.append(elapsed)\n",
        "        if response.usage:\n",
        "            token_counts.append(response.usage.completion_tokens)\n",
        "\n",
        "    return {\n",
        "        \"avg_time\": mean(times),\n",
        "        \"std_time\": stdev(times) if len(times) > 1 else 0,\n",
        "        \"avg_tokens\": mean(token_counts) if token_counts else 0,\n",
        "        \"avg_tok_per_sec\": mean(token_counts) / mean(times) if token_counts else 0\n",
        "    }\n",
        "\n",
        "print(\"‚ö° Performance Benchmark:\")\n",
        "print(\"=\"*50)\n",
        "print(\"Running 5 iterations for each test...\\n\")\n",
        "\n",
        "# Simple prompt\n",
        "simple_result = benchmark_request(\"What is the capital of France?\")\n",
        "print(f\"üìù Simple Query:\")\n",
        "print(f\"   Avg Response Time: {simple_result['avg_time']:.2f}s (¬±{simple_result['std_time']:.2f}s)\")\n",
        "print(f\"   Avg Tokens: {simple_result['avg_tokens']:.0f}\")\n",
        "print(f\"   Throughput: {simple_result['avg_tok_per_sec']:.1f} tok/s\")\n",
        "\n",
        "# Complex prompt\n",
        "complex_result = benchmark_request(\"Explain the key differences between REST and GraphQL APIs.\")\n",
        "print(f\"\\nüìù Complex Query:\")\n",
        "print(f\"   Avg Response Time: {complex_result['avg_time']:.2f}s (¬±{complex_result['std_time']:.2f}s)\")\n",
        "print(f\"   Avg Tokens: {complex_result['avg_tokens']:.0f}\")\n",
        "print(f\"   Throughput: {complex_result['avg_tok_per_sec']:.1f} tok/s\")"
      ],
      "metadata": {
        "id": "benchmark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üè¢ Section 10: Real-World Use Cases"
      ],
      "metadata": {
        "id": "section12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Case 1: Customer Support Agent\n",
        "support_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are a helpful customer support agent for TechStore.\n",
        "- Be polite and professional\n",
        "- Provide clear solutions\n",
        "- Offer alternatives when needed\n",
        "- Acknowledge customer feelings\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I ordered a laptop 5 days ago and it still hasn't arrived. I'm very frustrated!\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.5,\n",
        "    max_tokens=400\n",
        ")\n",
        "\n",
        "print(\"üéß Customer Support Agent:\")\n",
        "print(\"=\"*50)\n",
        "print(support_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "use_case_support"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Case 2: Data Analysis Assistant\n",
        "data_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a data analysis expert. Provide insights and Python code for analysis.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"I have sales data with columns: date, product, quantity, price, region.\n",
        "I want to:\n",
        "1. Find top 5 products by revenue\n",
        "2. Calculate month-over-month growth\n",
        "3. Identify best performing region\n",
        "\n",
        "Provide Python pandas code for this analysis.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_tokens=1200\n",
        ")\n",
        "\n",
        "print(\"üìä Data Analysis Assistant:\")\n",
        "print(\"=\"*50)\n",
        "print(data_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "use_case_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Case 3: Technical Documentation Writer\n",
        "docs_response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Write API documentation for this Python function:\n",
        "\n",
        "```python\n",
        "def process_batch(\n",
        "    items: List[Dict[str, Any]],\n",
        "    batch_size: int = 100,\n",
        "    retry_count: int = 3,\n",
        "    timeout: float = 30.0,\n",
        "    callback: Optional[Callable[[int, int], None]] = None\n",
        ") -> BatchResult:\n",
        "```\n",
        "\n",
        "Include: description, parameters, return value, examples, and possible exceptions.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.4,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"üìñ API Documentation:\")\n",
        "print(\"=\"*50)\n",
        "print(docs_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "use_case_docs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìã Summary & Key Takeaways\n",
        "\n",
        "### ‚úÖ What We Tested\n",
        "\n",
        "| Capability | Status | Notes |\n",
        "|------------|--------|-------|\n",
        "| Basic Chat | ‚úÖ | Fast response times |\n",
        "| Reasoning | ‚úÖ | Strong math & logic |\n",
        "| Coding | ‚úÖ | Excellent code generation |\n",
        "| Streaming | ‚úÖ | Real-time token output |\n",
        "| Multilingual | ‚úÖ | Chinese, Japanese, Spanish, French |\n",
        "| Long Context | ‚úÖ | 256K token window |\n",
        "\n",
        "### üåü Model Highlights\n",
        "\n",
        "- **Efficiency**: 196B total params, only 11B active per token\n",
        "- **Speed**: 100-300 tok/s generation throughput\n",
        "- **Context**: 256K token window with efficient SWA\n",
        "- **Coding**: 74.4% on SWE-bench Verified\n",
        "- **Agency**: 51.0% on Terminal-Bench 2.0\n",
        "\n",
        "### üìö Resources\n",
        "\n",
        "- **OpenRouter**: https://openrouter.ai/stepfun/step-3.5-flash:free\n",
        "- **ModelScope**: https://modelscope.cn/models/stepfun-ai/Step-3.5-Flash-FP8\n",
        "- **Documentation**: https://static.stepfun.com/blog/step-3.5-flash/\n",
        "- **License**: Apache 2.0\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "1. Explore more advanced agentic workflows\n",
        "2. Build RAG applications with long context\n",
        "3. Create custom tool integrations\n",
        "4. Deploy in production with StepFun or OpenRouter APIs"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<center>\n",
        "\n",
        "*Built with ‚ù§Ô∏è by @BuildFastWithAI*\n",
        "\n",
        "[www.buildfastwithai.com](https://www.buildfastwithai.com)\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "footer"
      }
    }
  ]
}