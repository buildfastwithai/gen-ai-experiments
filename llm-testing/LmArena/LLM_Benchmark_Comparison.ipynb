{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1LMzSS5jIQ6P_2ieU7Cw9XpxYV3A_9UbR?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn by building. Get expert mentorship and work on real AI projects.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n"
      ],
      "metadata": {
        "id": "header-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèÜ LLM Benchmark Comparison\n",
        "\n",
        "**Head-to-Head Comparison of Top AI Models**\n",
        "\n",
        "This notebook benchmarks **4 frontier models** across multiple tasks:\n",
        "\n",
        "| Model | Provider | Access |\n",
        "|-------|----------|--------|\n",
        "| **Claude Opus 4.6** | Anthropic | OpenRouter |\n",
        "| **Minimax m2.1** | minmax | OpenRouter |\n",
        "| **Kimi K2.5** | Moonshot AI | OpenRouter |\n",
        "| **Gemini 3 Flash** | Google | OpenRouter |\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Benchmarks\n",
        "\n",
        "1. **Speed Test** - Response latency\n",
        "2. **Reasoning** - Logic & problem-solving\n",
        "3. **Code Generation** - Python function quality\n",
        "4. **Math** - Multi-step calculations\n",
        "5. **Creative Writing** - Story generation\n",
        "6. **Structured Output** - JSON accuracy"
      ],
      "metadata": {
        "id": "intro-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üì¶ Setup"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# @title Install Dependencies\n",
        "!pip install -q openai pandas tabulate"
      ],
      "outputs": [],
      "metadata": {
        "id": "install-cell"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# @title Configure Models\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# OpenRouter client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "# Models to compare\n",
        "MODELS = {\n",
        "    \"Claude Opus 4.6\": \"anthropic/claude-opus-4.6\",\n",
        "    \"minimax-m2.1\": \"minimax/minimax-m2.1\",\n",
        "    \"Kimi K2.5\": \"moonshotai/kimi-k2.5\",\n",
        "    \"Gemini 3 Flash\": \"google/gemini-3-flash-preview\"\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Models configured:\")\n",
        "for name, model_id in MODELS.items():\n",
        "    print(f\"   ‚Ä¢ {name}: {model_id}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Models configured:\n",
            "   ‚Ä¢ Claude Opus 4.6: anthropic/claude-opus-4.6\n",
            "   ‚Ä¢ minimax-m2.1: minimax/minimax-m2.1\n",
            "   ‚Ä¢ Kimi K2.5: moonshotai/kimi-k2.5\n",
            "   ‚Ä¢ Gemini 3 Flash: google/gemini-3-flash-preview\n"
          ]
        }
      ],
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8371e7d5-83d1-43a6-c054-02f4617edab8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# @title Helper Functions\n",
        "def run_benchmark(model_id, messages, max_tokens=1024):\n",
        "    \"\"\"Run a single benchmark and return response + timing.\"\"\"\n",
        "    start = time.time()\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_id,\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        elapsed = time.time() - start\n",
        "        content = response.choices[0].message.content\n",
        "        tokens = response.usage.completion_tokens if response.usage else len(content.split())\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"content\": content,\n",
        "            \"time\": elapsed,\n",
        "            \"tokens\": tokens,\n",
        "            \"tokens_per_sec\": tokens / elapsed if elapsed > 0 else 0\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"content\": str(e),\n",
        "            \"time\": time.time() - start,\n",
        "            \"tokens\": 0,\n",
        "            \"tokens_per_sec\": 0\n",
        "        }\n",
        "\n",
        "def compare_models(prompt, system_prompt=None, max_tokens=1024):\n",
        "    \"\"\"Run prompt across all models and collect results.\"\"\"\n",
        "    results = {}\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    if system_prompt:\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    for name, model_id in MODELS.items():\n",
        "        print(f\"‚è≥ Running {name}...\", end=\" \")\n",
        "        result = run_benchmark(model_id, messages, max_tokens)\n",
        "        results[name] = result\n",
        "        status = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
        "        print(f\"{status} ({result['time']:.2f}s)\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def display_results(results, show_content=True):\n",
        "    \"\"\"Display benchmark results as table.\"\"\"\n",
        "    data = []\n",
        "    for name, r in results.items():\n",
        "        data.append({\n",
        "            \"Model\": name,\n",
        "            \"Time (s)\": f\"{r['time']:.2f}\",\n",
        "            \"Tokens\": r['tokens'],\n",
        "            \"Tok/s\": f\"{r['tokens_per_sec']:.1f}\",\n",
        "            \"Status\": \"‚úÖ\" if r['success'] else \"‚ùå\"\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"\\nüìä Performance Summary:\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    if show_content:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        for name, r in results.items():\n",
        "            print(f\"\\nü§ñ {name}:\")\n",
        "            print(\"-\" * 40)\n",
        "            print(r['content'][:1000] + \"...\" if len(r['content']) > 1000 else r['content'])\n",
        "            print()\n",
        "\n",
        "print(\"‚úÖ Helper functions ready!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Helper functions ready!\n"
          ]
        }
      ],
      "metadata": {
        "id": "helpers-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80c66bc-cc6c-4bea-948e-46a7ed487015"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üß™ Benchmark 1: Speed Test\n",
        "\n",
        "Simple prompt to measure raw response latency."
      ],
      "metadata": {
        "id": "bench1-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Speed Test\n",
        "print(\"üöÄ BENCHMARK 1: Speed Test\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "speed_prompt = \"Count from 1 to 20, putting each number on a new line.\"\n",
        "\n",
        "speed_results = compare_models(speed_prompt, max_tokens=200)\n",
        "display_results(speed_results, show_content=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bench1-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üß† Benchmark 2: Reasoning\n",
        "\n",
        "Complex logic puzzle to test reasoning capabilities."
      ],
      "metadata": {
        "id": "bench2-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Reasoning Test\n",
        "print(\"üß† BENCHMARK 2: Reasoning Test\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "reasoning_prompt = \"\"\"\n",
        "Solve this logic puzzle step by step:\n",
        "\n",
        "There are 5 houses in a row, each painted a different color.\n",
        "- The green house is immediately to the left of the white house.\n",
        "- The owner of the yellow house drinks coffee.\n",
        "- The person in the middle house drinks milk.\n",
        "- The Norwegian lives in the first house.\n",
        "- The green house owner drinks tea.\n",
        "\n",
        "Question: What does the Norwegian drink?\n",
        "\n",
        "Show your reasoning step by step, then give the final answer.\n",
        "\"\"\"\n",
        "\n",
        "reasoning_results = compare_models(reasoning_prompt, max_tokens=1500)\n",
        "display_results(reasoning_results)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bench2-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üíª Benchmark 3: Code Generation\n",
        "\n",
        "Generate a working Python function with edge cases."
      ],
      "metadata": {
        "id": "bench3-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Code Generation Test\n",
        "print(\"üíª BENCHMARK 3: Code Generation\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "code_prompt = \"\"\"\n",
        "Write a Python function `merge_sorted_lists(list1, list2)` that:\n",
        "1. Merges two sorted lists into one sorted list\n",
        "2. Handles empty lists gracefully\n",
        "3. Has O(n+m) time complexity\n",
        "4. Includes docstring and type hints\n",
        "5. Add 3 test cases with assertions\n",
        "\n",
        "Only return the code, no explanations.\n",
        "\"\"\"\n",
        "\n",
        "code_results = compare_models(\n",
        "    code_prompt,\n",
        "    system_prompt=\"You are an expert Python developer. Write clean, production-quality code.\",\n",
        "    max_tokens=2000\n",
        ")\n",
        "display_results(code_results)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bench3-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üßÆ Benchmark 4: Math\n",
        "\n",
        "Multi-step mathematical problem solving."
      ],
      "metadata": {
        "id": "bench4-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Math Test\n",
        "print(\"üßÆ BENCHMARK 4: Math Test\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "math_prompt = \"\"\"\n",
        "Solve this problem step by step:\n",
        "\n",
        "A store offers a 20% discount on all items. After the discount,\n",
        "a 8% sales tax is applied. If the original price of an item is $150:\n",
        "\n",
        "1. What is the price after the 20% discount?\n",
        "2. What is the final price including 8% sales tax?\n",
        "3. What percentage of the original price is the final price?\n",
        "4. If someone has a $100 budget, can they afford the item? If yes, how much change?\n",
        "\n",
        "Show all calculations.\n",
        "\"\"\"\n",
        "\n",
        "math_results = compare_models(math_prompt, max_tokens=1000)\n",
        "display_results(math_results)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bench4-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ‚úçÔ∏è Benchmark 5: Creative Writing\n",
        "\n",
        "Generate creative short story content."
      ],
      "metadata": {
        "id": "bench5-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Creative Writing Test\n",
        "print(\"‚úçÔ∏è BENCHMARK 5: Creative Writing\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "creative_prompt = \"\"\"\n",
        "Write a 100-word micro-story with these constraints:\n",
        "- Genre: Sci-fi\n",
        "- Must include: an AI, a sunset, and an unexpected twist\n",
        "- Tone: melancholic but hopeful\n",
        "- End with a single impactful sentence\n",
        "\n",
        "Be creative and evocative.\n",
        "\"\"\"\n",
        "\n",
        "creative_results = compare_models(creative_prompt, max_tokens=500)\n",
        "display_results(creative_results)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bench5-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìã Benchmark 6: Structured Output\n",
        "\n",
        "Test JSON generation accuracy."
      ],
      "metadata": {
        "id": "bench6-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Structured Output Test\n",
        "print(\"üìã BENCHMARK 6: Structured Output (JSON)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "json_prompt = \"\"\"\n",
        "Extract information from this text and return ONLY valid JSON:\n",
        "\n",
        "\"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne\n",
        "in Cupertino, California on April 1, 1976. The company is valued at\n",
        "approximately $3 trillion as of 2024 and employs over 160,000 people\n",
        "worldwide. Their main products include iPhone, iPad, Mac, and Apple Watch.\"\n",
        "\n",
        "Schema:\n",
        "{\n",
        "  \"company_name\": string,\n",
        "  \"founders\": [string],\n",
        "  \"founded_date\": string,\n",
        "  \"location\": string,\n",
        "  \"valuation\": string,\n",
        "  \"employees\": number,\n",
        "  \"products\": [string]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "json_results = compare_models(\n",
        "    json_prompt,\n",
        "    system_prompt=\"Respond ONLY with valid JSON. No explanations.\",\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "# Validate JSON output\n",
        "print(\"\\nüîç JSON Validation:\")\n",
        "for name, r in json_results.items():\n",
        "    try:\n",
        "        parsed = json.loads(r['content'])\n",
        "        print(f\"   {name}: ‚úÖ Valid JSON\")\n",
        "    except:\n",
        "        print(f\"   {name}: ‚ùå Invalid JSON\")\n",
        "\n",
        "display_results(json_results)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bench6-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìà Final Summary"
      ],
      "metadata": {
        "id": "summary-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# @title Generate Final Comparison Table\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL BENCHMARK SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Aggregate all results\n",
        "all_benchmarks = {\n",
        "    \"Speed\": speed_results,\n",
        "    \"Reasoning\": reasoning_results,\n",
        "    \"Code\": code_results,\n",
        "    \"Math\": math_results,\n",
        "    \"Creative\": creative_results,\n",
        "    \"JSON\": json_results\n",
        "}\n",
        "\n",
        "# Calculate average times\n",
        "summary = {name: {\"total_time\": 0, \"success_count\": 0} for name in MODELS.keys()}\n",
        "\n",
        "for bench_name, results in all_benchmarks.items():\n",
        "    for model_name, r in results.items():\n",
        "        summary[model_name][\"total_time\"] += r[\"time\"]\n",
        "        if r[\"success\"]:\n",
        "            summary[model_name][\"success_count\"] += 1\n",
        "\n",
        "# Display summary\n",
        "summary_data = []\n",
        "for name, stats in summary.items():\n",
        "    summary_data.append({\n",
        "        \"Model\": name,\n",
        "        \"Total Time (s)\": f\"{stats['total_time']:.2f}\",\n",
        "        \"Avg Time (s)\": f\"{stats['total_time']/6:.2f}\",\n",
        "        \"Success Rate\": f\"{stats['success_count']}/6\"\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\nüèÜ Benchmark Complete!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìä FINAL BENCHMARK SUMMARY\n",
            "============================================================\n",
            "          Model Total Time (s) Avg Time (s) Success Rate\n",
            "Claude Opus 4.6          45.69         7.62          6/6\n",
            "   minimax-m2.1          39.91         6.65          6/6\n",
            "      Kimi K2.5         260.43        43.41          6/6\n",
            " Gemini 3 Flash          17.26         2.88          6/6\n",
            "\n",
            "üèÜ Benchmark Complete!\n"
          ]
        }
      ],
      "metadata": {
        "id": "summary-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d4c184-43bd-45ac-ccef-7c5695494836"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üéØ Model Strengths\n",
        "\n",
        "| Model | Best At | Notes |\n",
        "|-------|---------|-------|\n",
        "| **Claude Opus 4.6** | Reasoning, Coding | 1M context, adaptive thinking |\n",
        "| **MinMax-M2.1** | General tasks | Strong all-rounder |\n",
        "| **Kimi K2.5** | Long context | Cost-effective |\n",
        "| **Gemini 3 Flash** | Speed | Fast inference |\n",
        "\n",
        "---\n",
        "*Notebook by @BuildFastWithAI*"
      ],
      "metadata": {
        "id": "conclusion-cell"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OylWet-wAxg_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}