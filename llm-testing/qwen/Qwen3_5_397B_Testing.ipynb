{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1OchQHwAA4xrNVIVykNwkYeFYbTeHY7jk?usp=sharing)"
      ],
      "metadata": {
        "id": "header-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn by building. Get expert mentorship and work on real AI projects.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n"
      ],
      "metadata": {
        "id": "3we7Aql_PWA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Qwen3.5 397B-A17B ‚Äî Testing Notebook\n",
        "\n",
        "**Alibaba's Latest Hybrid MoE Vision-Language Model via OpenRouter**\n",
        "\n",
        "This notebook tests the capabilities of **Qwen3.5 397B-A17B** ‚Äî a native vision-language model built on a hybrid architecture combining **linear attention** with a **sparse Mixture-of-Experts (MoE)** design for blazing-fast inference.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Model Specifications\n",
        "\n",
        "| Feature | Detail |\n",
        "|---------|--------|\n",
        "| **Provider** | Qwen (Alibaba) via OpenRouter |\n",
        "| **Model ID** | `qwen/qwen3.5-397b-a17b` |\n",
        "| **Total Parameters** | 397 Billion |\n",
        "| **Active Parameters** | 17 Billion (sparse MoE) |\n",
        "| **Architecture** | Hybrid Linear Attention + Sparse MoE |\n",
        "| **Type** | Native Vision-Language Model (VLM) |\n",
        "| **Reasoning** | ‚úÖ Supported |\n",
        "| **Streaming** | ‚úÖ Supported |\n",
        "| **Tool Calling** | ‚úÖ Supported |\n",
        "| **Vision** | ‚úÖ Image & Video Understanding |\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Key Strengths\n",
        "\n",
        "- üß† **Reasoning & Logic** ‚Äî Deep thinking with step-by-step reasoning\n",
        "- üíª **Code Generation** ‚Äî Strong generalization across coding tasks\n",
        "- ü§ñ **Agent Capabilities** ‚Äî Built for agentic workflows & tool use\n",
        "- üëÅÔ∏è **Vision Understanding** ‚Äî Image, video, and GUI interaction\n",
        "- ‚ö° **Efficient Inference** ‚Äî Only 17B active params at any time\n",
        "- üîó [Model Weights on HuggingFace](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)"
      ],
      "metadata": {
        "id": "intro-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üì¶ Setup & Installation"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# @title Install Dependencies\n",
        "!pip install -q openai"
      ],
      "outputs": [],
      "metadata": {
        "id": "install-cell"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# @title Configure OpenRouter Client\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "MODEL = \"qwen/qwen3.5-397b-a17b\"\n",
        "\n",
        "print(f\"‚úÖ Client configured for: {MODEL}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Client configured for: qwen/qwen3.5-397b-a17b\n"
          ]
        }
      ],
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709cb1a2-a107-43c0-f2e9-4baeab08b798"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üí¨ Example 1: Basic Chat & Reasoning\n",
        "\n",
        "Test Qwen3.5's fundamental chat ability and reasoning skills."
      ],
      "metadata": {
        "id": "example1-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Basic Chat\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain the difference between GPT, BERT, and Mixture-of-Experts architectures in simple terms. Use analogies.\"}\n",
        "    ],\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "outputs": [],
      "metadata": {
        "id": "example1-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üß† Example 2: Deep Reasoning with Thinking Mode\n",
        "\n",
        "Qwen3.5 supports **reasoning mode** ‚Äî you can see its step-by-step thinking before the final answer. Perfect for math and logic puzzles!"
      ],
      "metadata": {
        "id": "example2-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Reasoning Mode\n",
        "import json\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"\"\"A bat and a ball cost $1.10 in total.\n",
        "The bat costs $1.00 more than the ball.\n",
        "How much does the ball cost?\n",
        "\n",
        "Think step by step and show your work.\"\"\"}\n",
        "    ],\n",
        "    max_tokens=2048,\n",
        "    extra_body={\n",
        "        \"reasoning\": {\n",
        "            \"effort\": \"high\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# Check for reasoning content\n",
        "msg = response.choices[0].message\n",
        "\n",
        "# Display reasoning if available\n",
        "if hasattr(msg, 'reasoning_content') and msg.reasoning_content:\n",
        "    print(\"üí≠ REASONING:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(msg.reasoning_content)\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(\"‚úÖ ANSWER:\")\n",
        "print(\"=\" * 50)\n",
        "print(msg.content)"
      ],
      "outputs": [],
      "metadata": {
        "id": "example2-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üíª Example 3: Code Generation\n",
        "\n",
        "Qwen3.5 has strong code generation abilities. Let's test it with a real-world coding task."
      ],
      "metadata": {
        "id": "example3-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Code Generation\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert Python developer. Write clean, production-quality code with comments.\"},\n",
        "        {\"role\": \"user\", \"content\": \"\"\"Write a Python class called 'TaskManager' that:\n",
        "1. Can add tasks with a title, priority (high/medium/low), and due date\n",
        "2. Can list all tasks sorted by priority\n",
        "3. Can mark tasks as complete\n",
        "4. Can show only pending tasks\n",
        "5. Include a __str__ method for nice output\n",
        "\n",
        "Show usage examples at the end.\"\"\"}\n",
        "    ],\n",
        "    max_tokens=4096\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "outputs": [],
      "metadata": {
        "id": "example3-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üõ†Ô∏è Example 4: Tool Calling / Function Calling\n",
        "\n",
        "Qwen3.5 excels at agentic tasks. Let's test its ability to understand when and how to call tools."
      ],
      "metadata": {
        "id": "example4-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Define Tools\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_stock_price\",\n",
        "            \"description\": \"Get the current stock price for a given ticker symbol\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"ticker\": {\"type\": \"string\", \"description\": \"Stock ticker symbol (e.g., AAPL, GOOGL)\"},\n",
        "                    \"currency\": {\"type\": \"string\", \"enum\": [\"USD\", \"EUR\", \"GBP\"], \"description\": \"Currency for the price\"}\n",
        "                },\n",
        "                \"required\": [\"ticker\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"send_email\",\n",
        "            \"description\": \"Send an email to a recipient\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"to\": {\"type\": \"string\", \"description\": \"Recipient email address\"},\n",
        "                    \"subject\": {\"type\": \"string\", \"description\": \"Email subject\"},\n",
        "                    \"body\": {\"type\": \"string\", \"description\": \"Email body text\"}\n",
        "                },\n",
        "                \"required\": [\"to\", \"subject\", \"body\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Ask a question that requires tool use\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Check the stock price of Apple and Tesla, then email me a summary at john@example.com\"}\n",
        "    ],\n",
        "    tools=tools,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "message = response.choices[0].message\n",
        "\n",
        "if message.tool_calls:\n",
        "    print(f\"üîß Tool Calls Detected: {len(message.tool_calls)}\\n\")\n",
        "    for i, tc in enumerate(message.tool_calls, 1):\n",
        "        args = json.loads(tc.function.arguments)\n",
        "        print(f\"  Call {i}: {tc.function.name}\")\n",
        "        print(f\"  Args:   {json.dumps(args, indent=2)}\")\n",
        "        print()\n",
        "else:\n",
        "    print(message.content)"
      ],
      "outputs": [],
      "metadata": {
        "id": "example4-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üåä Example 5: Streaming ‚Äî Real-time Output\n",
        "\n",
        "Stream responses token-by-token for a better user experience with long outputs."
      ],
      "metadata": {
        "id": "example5-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Streaming Response\n",
        "print(\"üì° Streaming from Qwen3.5 397B:\\n\")\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a short, compelling story (under 200 words) about a robot that learns to paint.\"}\n",
        "    ],\n",
        "    max_tokens=1024,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "full_response = \"\"\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        full_response += content\n",
        "        print(content, end=\"\", flush=True)\n",
        "\n",
        "print(f\"\\n\\n‚úÖ Stream complete! ({len(full_response)} chars)\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "example5-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ‚öîÔ∏è Example 6: Head-to-Head ‚Äî Qwen3.5 vs Kimi K2.5\n",
        "\n",
        "Let's compare **Qwen3.5 397B** (via OpenRouter) with **Kimi K2.5** (via NVIDIA) on the same prompts to see how they stack up!\n",
        "\n",
        "Both are massive MoE models:\n",
        "- **Qwen3.5**: 397B total / 17B active params\n",
        "- **Kimi K2.5**: 1T total params, 384 experts"
      ],
      "metadata": {
        "id": "example6-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# @title Setup Both Clients\n",
        "!pip install -q langchain-nvidia-ai-endpoints"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "metadata": {
        "id": "example6-install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc99b15c-49d0-4923-c2dd-d1182a58bfa2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Configure Kimi K2.5 Client (NVIDIA)\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "kimi_client = ChatNVIDIA(\n",
        "    model=\"moonshotai/kimi-k2.5\",\n",
        "    api_key=userdata.get(\"NVIDIA_API_KEY\"),\n",
        "    temperature=0.7,\n",
        "    max_tokens=2048,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Kimi K2.5 (NVIDIA) ready!\")\n",
        "print(\"‚úÖ Qwen3.5 (OpenRouter) ready!\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "example6-config"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title üèÜ Comparison Test 1: Logical Reasoning\n",
        "import time\n",
        "\n",
        "REASONING_PROMPT = \"\"\"There are 3 boxes. One has only apples, one has only oranges,\n",
        "and one has both. All boxes are labeled WRONG. You can pick one fruit from\n",
        "one box. Which box do you pick from, and how do you figure out all labels?\n",
        "Explain step by step.\"\"\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üß† LOGICAL REASONING COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- Qwen 3.5 ---\n",
        "print(\"\\nüìò QWEN 3.5 397B:\")\n",
        "print(\"-\" * 40)\n",
        "start = time.time()\n",
        "qwen_response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": REASONING_PROMPT}],\n",
        "    max_tokens=2048\n",
        ")\n",
        "qwen_time = time.time() - start\n",
        "qwen_answer = qwen_response.choices[0].message.content\n",
        "print(qwen_answer)\n",
        "print(f\"\\n‚è±Ô∏è Time: {qwen_time:.2f}s\")\n",
        "\n",
        "# --- Kimi K2.5 ---\n",
        "print(\"\\n\\nüìó KIMI K2.5:\")\n",
        "print(\"-\" * 40)\n",
        "start = time.time()\n",
        "kimi_response = kimi_client.invoke(REASONING_PROMPT)\n",
        "kimi_time = time.time() - start\n",
        "print(kimi_response.content)\n",
        "print(f\"\\n‚è±Ô∏è Time: {kimi_time:.2f}s\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä SPEED COMPARISON:\")\n",
        "print(f\"   Qwen 3.5: {qwen_time:.2f}s\")\n",
        "print(f\"   Kimi K2.5: {kimi_time:.2f}s\")\n",
        "faster = \"Qwen 3.5\" if qwen_time < kimi_time else \"Kimi K2.5\"\n",
        "diff = abs(qwen_time - kimi_time)\n",
        "print(f\"   üèÜ {faster} was {diff:.2f}s faster\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "example6-reasoning"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title üèÜ Comparison Test 2: Code Generation\n",
        "\n",
        "CODE_PROMPT = \"\"\"Write a Python function called 'analyze_text' that takes a string and returns a dictionary with:\n",
        "- word_count: total words\n",
        "- char_count: total characters (no spaces)\n",
        "- sentence_count: number of sentences\n",
        "- most_common_word: the most frequent word\n",
        "- avg_word_length: average word length\n",
        "\n",
        "Include type hints and a docstring.\"\"\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üíª CODE GENERATION COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- Qwen 3.5 ---\n",
        "print(\"\\nüìò QWEN 3.5 397B:\")\n",
        "print(\"-\" * 40)\n",
        "start = time.time()\n",
        "qwen_code = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Write clean Python code only. No extra explanation.\"},\n",
        "        {\"role\": \"user\", \"content\": CODE_PROMPT}\n",
        "    ],\n",
        "    max_tokens=2048\n",
        ")\n",
        "qwen_code_time = time.time() - start\n",
        "print(qwen_code.choices[0].message.content)\n",
        "print(f\"\\n‚è±Ô∏è Time: {qwen_code_time:.2f}s\")\n",
        "\n",
        "# --- Kimi K2.5 ---\n",
        "print(\"\\n\\nüìó KIMI K2.5:\")\n",
        "print(\"-\" * 40)\n",
        "start = time.time()\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "kimi_code = kimi_client.invoke([\n",
        "    SystemMessage(content=\"Write clean Python code only. No extra explanation.\"),\n",
        "    HumanMessage(content=CODE_PROMPT)\n",
        "])\n",
        "kimi_code_time = time.time() - start\n",
        "print(kimi_code.content)\n",
        "print(f\"\\n‚è±Ô∏è Time: {kimi_code_time:.2f}s\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä SPEED COMPARISON:\")\n",
        "print(f\"   Qwen 3.5: {qwen_code_time:.2f}s\")\n",
        "print(f\"   Kimi K2.5: {kimi_code_time:.2f}s\")\n",
        "faster = \"Qwen 3.5\" if qwen_code_time < kimi_code_time else \"Kimi K2.5\"\n",
        "diff = abs(qwen_code_time - kimi_code_time)\n",
        "print(f\"   üèÜ {faster} was {diff:.2f}s faster\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "example6-code-compare"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title üèÜ Comparison Test 3: Creative Writing\n",
        "\n",
        "CREATIVE_PROMPT = \"Write a haiku about artificial intelligence, then explain the meaning behind it in one sentence.\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úçÔ∏è CREATIVE WRITING COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- Qwen 3.5 ---\n",
        "print(\"\\nüìò QWEN 3.5 397B:\")\n",
        "print(\"-\" * 40)\n",
        "start = time.time()\n",
        "qwen_creative = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": CREATIVE_PROMPT}],\n",
        "    max_tokens=512\n",
        ")\n",
        "qwen_creative_time = time.time() - start\n",
        "print(qwen_creative.choices[0].message.content)\n",
        "print(f\"\\n‚è±Ô∏è Time: {qwen_creative_time:.2f}s\")\n",
        "\n",
        "# --- Kimi K2.5 ---\n",
        "print(\"\\n\\nüìó KIMI K2.5:\")\n",
        "print(\"-\" * 40)\n",
        "start = time.time()\n",
        "kimi_creative = kimi_client.invoke(CREATIVE_PROMPT)\n",
        "kimi_creative_time = time.time() - start\n",
        "print(kimi_creative.content)\n",
        "print(f\"\\n‚è±Ô∏è Time: {kimi_creative_time:.2f}s\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä SPEED COMPARISON:\")\n",
        "print(f\"   Qwen 3.5: {qwen_creative_time:.2f}s\")\n",
        "print(f\"   Kimi K2.5: {kimi_creative_time:.2f}s\")\n",
        "faster = \"Qwen 3.5\" if qwen_creative_time < kimi_creative_time else \"Kimi K2.5\"\n",
        "diff = abs(qwen_creative_time - kimi_creative_time)\n",
        "print(f\"   üèÜ {faster} was {diff:.2f}s faster\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "example6-creative"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìà Token Usage & Summary"
      ],
      "metadata": {
        "id": "summary-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Final Summary\n",
        "print(\"üìä Overall Comparison Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n{'Test':<25} {'Qwen 3.5':>12} {'Kimi K2.5':>12} {'Winner':>12}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "tests = [\n",
        "    (\"Logical Reasoning\", qwen_time, kimi_time),\n",
        "    (\"Code Generation\", qwen_code_time, kimi_code_time),\n",
        "    (\"Creative Writing\", qwen_creative_time, kimi_creative_time),\n",
        "]\n",
        "\n",
        "qwen_wins = 0\n",
        "kimi_wins = 0\n",
        "\n",
        "for name, qt, kt in tests:\n",
        "    winner = \"Qwen 3.5\" if qt < kt else \"Kimi K2.5\"\n",
        "    if qt < kt:\n",
        "        qwen_wins += 1\n",
        "    else:\n",
        "        kimi_wins += 1\n",
        "    print(f\"{name:<25} {qt:>10.2f}s {kt:>10.2f}s {winner:>12}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"\\nüèÜ Speed Winner: {'Qwen 3.5' if qwen_wins > kimi_wins else 'Kimi K2.5'} ({max(qwen_wins, kimi_wins)}/{len(tests)} tests faster)\")\n",
        "print(\"\\nüí° Note: Speed ‚â† Quality! Read both responses to judge which model gave better answers.\")\n",
        "print(\"\\nüìù Pricing:\")\n",
        "print(\"   ‚Ä¢ Qwen 3.5: Check https://openrouter.ai/qwen/qwen3.5-397b-a17b\")\n",
        "print(\"   ‚Ä¢ Kimi K2.5: FREE via NVIDIA (build.nvidia.com)\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "summary-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üéØ Key Takeaways\n",
        "\n",
        "### Qwen3.5 397B-A17B Strengths:\n",
        "- ‚úÖ Hybrid architecture = efficient inference\n",
        "- ‚úÖ Strong reasoning with thinking mode\n",
        "- ‚úÖ Excellent code generation\n",
        "- ‚úÖ Native vision-language support\n",
        "- ‚úÖ Agent-ready with tool calling\n",
        "\n",
        "### Comparison Notes:\n",
        "| Feature | Qwen 3.5 | Kimi K2.5 |\n",
        "|---------|----------|----------|\n",
        "| **Total Params** | 397B | 1T |\n",
        "| **Active Params** | 17B | MoE (384 experts) |\n",
        "| **Architecture** | Linear Attn + MoE | MoE + MoonViT |\n",
        "| **Vision** | ‚úÖ Native VLM | ‚úÖ VLM |\n",
        "| **Provider** | OpenRouter | NVIDIA (free) |\n",
        "| **Open Source** | ‚úÖ | ‚úÖ |\n",
        "\n",
        "### üìö Resources:\n",
        "- üîó [Qwen3.5 on OpenRouter](https://openrouter.ai/qwen/qwen3.5-397b-a17b)\n",
        "- üîó [Qwen3.5 on HuggingFace](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)\n",
        "- üîó [Kimi K2.5 on NVIDIA](https://build.nvidia.com/moonshotai/kimi-k2.5)\n",
        "\n",
        "---\n",
        "*Notebook by @BuildFastWithAI*"
      ],
      "metadata": {
        "id": "takeaways-cell"
      }
    }
  ]
}