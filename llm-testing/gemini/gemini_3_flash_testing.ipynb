{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fUStL32GseSXEQyu1TpgEs1Mm1mP8IUx?usp=sharing)\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Flash Testing Notebook\n",
    "\n",
    "This notebook demonstrates the capabilities of Google's **Gemini 3 Flash** model - a fast, efficient model optimized for speed while maintaining high quality outputs.\n",
    "\n",
    "**What we'll cover:**\n",
    "- Basic text generation\n",
    "- Streaming responses\n",
    "- Multimodal capabilities (image understanding)\n",
    "- Structured outputs with Pydantic\n",
    "- Function calling\n",
    "- Google Search integration\n",
    "\n",
    "**Prerequisites:**\n",
    "- Google Cloud Project or AI Studio Account\n",
    "- API Key from [Google AI Studio](https://aistudio.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation\n",
    "Install the Google GenAI SDK and required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q google-genai pydantic pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup & Authentication\n",
    "Configure your API key. It is recommended to use Google Colab's `userdata` for security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except ImportError:\n",
    "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', 'YOUR_API_KEY_HERE')\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
    "print(f\"Using model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic Text Generation\n",
    "Test basic text generation capabilities with Gemini 3 Flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Explain quantum computing in 3 sentences for a 10-year-old.\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Streaming Responses\n",
    "Stream responses for real-time output - ideal for chat applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Write a short poem about artificial intelligence and creativity.\"\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. System Instructions\n",
    "Guide the model's behavior with system instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the best programming language?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a wise programming mentor who always gives balanced, nuanced advice. You never say one language is 'the best' - instead you explain trade-offs.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Multimodal - Image Understanding\n",
    "Test Gemini 3 Flash's vision capabilities with image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Camponotus_flavomarginatus_ant.jpg/800px-Camponotus_flavomarginatus_ant.jpg\"\n",
    "image_response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(image_response.content))\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        \"Describe this image in detail. What species is this? What interesting facts can you share?\",\n",
    "        image\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Structured Outputs with Pydantic\n",
    "Get type-safe, structured JSON responses using Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class CodeReview(BaseModel):\n",
    "    code_quality: int = Field(description=\"Score from 1-10\")\n",
    "    issues: List[str] = Field(description=\"List of identified issues\")\n",
    "    suggestions: List[str] = Field(description=\"Improvement suggestions\")\n",
    "    security_concerns: Optional[List[str]] = Field(description=\"Any security issues found\")\n",
    "\n",
    "code_snippet = \"\"\"\n",
    "def get_user(user_id):\n",
    "    query = f\"SELECT * FROM users WHERE id = {user_id}\"\n",
    "    result = db.execute(query)\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=f\"Review this Python code and identify issues:\\n{code_snippet}\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=CodeReview,\n",
    "    ),\n",
    ")\n",
    "\n",
    "review = CodeReview.model_validate_json(response.text)\n",
    "print(f\"Code Quality: {review.code_quality}/10\")\n",
    "print(f\"\\nIssues Found:\")\n",
    "for issue in review.issues:\n",
    "    print(f\"  - {issue}\")\n",
    "print(f\"\\nSecurity Concerns:\")\n",
    "for concern in review.security_concerns or []:\n",
    "    print(f\"  ⚠️ {concern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Function Calling\n",
    "Define custom functions that the model can call to perform actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str, unit: str = \"celsius\") -> dict:\n",
    "    \"\"\"Get the current weather for a location.\"\"\"\n",
    "    weather_data = {\n",
    "        \"new york\": {\"temp\": 22, \"condition\": \"Sunny\"},\n",
    "        \"london\": {\"temp\": 15, \"condition\": \"Cloudy\"},\n",
    "        \"tokyo\": {\"temp\": 28, \"condition\": \"Humid\"},\n",
    "    }\n",
    "    data = weather_data.get(location.lower(), {\"temp\": 20, \"condition\": \"Unknown\"})\n",
    "    if unit == \"fahrenheit\":\n",
    "        data[\"temp\"] = data[\"temp\"] * 9/5 + 32\n",
    "    return {\"location\": location, \"temperature\": data[\"temp\"], \"unit\": unit, \"condition\": data[\"condition\"]}\n",
    "\n",
    "def calculate(expression: str) -> dict:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return {\"expression\": expression, \"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"expression\": expression, \"error\": str(e)}\n",
    "\n",
    "tools = [get_weather, calculate]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the weather in Tokyo? Also, what's 15% of 340?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=tools,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Google Search Integration\n",
    "Ground responses with real-time information from Google Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What are the latest developments in AI agents as of this week?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Multi-turn Conversation\n",
    "Maintain context across multiple turns in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a helpful AI tutor specializing in machine learning.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "response1 = chat.send_message(\"What is gradient descent?\")\n",
    "print(\"User: What is gradient descent?\")\n",
    "print(f\"AI: {response1.text}\\n\")\n",
    "\n",
    "response2 = chat.send_message(\"Can you give me a simple analogy?\")\n",
    "print(\"User: Can you give me a simple analogy?\")\n",
    "print(f\"AI: {response2.text}\\n\")\n",
    "\n",
    "response3 = chat.send_message(\"What are common variants of it?\")\n",
    "print(\"User: What are common variants of it?\")\n",
    "print(f\"AI: {response3.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Temperature & Generation Config\n",
    "Control creativity and output behavior with generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generate a creative startup idea in one sentence.\"\n",
    "\n",
    "print(\"Low Temperature (0.2) - More focused:\")\n",
    "response_low = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.2,\n",
    "        max_output_tokens=100,\n",
    "    ),\n",
    ")\n",
    "print(f\"  {response_low.text}\\n\")\n",
    "\n",
    "print(\"High Temperature (1.0) - More creative:\")\n",
    "response_high = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1.0,\n",
    "        max_output_tokens=100,\n",
    "    ),\n",
    ")\n",
    "print(f\"  {response_high.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Token Counting\n",
    "Count tokens for input content to manage API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Artificial intelligence is transforming every industry. \" * 50\n",
    "\n",
    "token_count = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=long_text\n",
    ")\n",
    "\n",
    "print(f\"Text length: {len(long_text)} characters\")\n",
    "print(f\"Token count: {token_count.total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Safety Settings\n",
    "Configure content safety filters for your application needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Write a story about a detective solving a mystery.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        safety_settings=[\n",
    "            types.SafetySetting(\n",
    "                category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                threshold=\"BLOCK_ONLY_HIGH\"\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered the key capabilities of **Gemini 3 Flash**:\n",
    "\n",
    "| Feature | Use Case |\n",
    "|---------|----------|\n",
    "| Basic Generation | Simple text completions |\n",
    "| Streaming | Real-time chat applications |\n",
    "| System Instructions | Customizing model behavior |\n",
    "| Multimodal | Image understanding & analysis |\n",
    "| Structured Outputs | Type-safe JSON responses |\n",
    "| Function Calling | Tool use & automation |\n",
    "| Google Search | Real-time information grounding |\n",
    "| Multi-turn Chat | Contextual conversations |\n",
    "| Generation Config | Temperature, tokens control |\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore the [Google AI Studio](https://aistudio.google.com/) for interactive testing\n",
    "- Check out [Gemini API Documentation](https://ai.google.dev/docs) for more features\n",
    "- Join the [Gen AI Experiments](https://github.com/buildfastwithai/gen-ai-experiments) community"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
