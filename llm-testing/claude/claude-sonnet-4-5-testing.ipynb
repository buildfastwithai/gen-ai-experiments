{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bBT0Gio0goR3NfU80bttnnXP7cCC1hSH?usp=sharing)\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Claude Sonnet 4.5 Model Using OpenRouter\n",
    "\n",
    "This notebook provides a comprehensive guide to using the newly released Anthropic Claude Sonnet 4.5 model via OpenRouter's API within the LangChain framework. We will cover everything from basic setup to advanced examples.\n",
    "\n",
    "**Note:** You will need an API key from [OpenRouter](https://openrouter.ai/) to run the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "First, let's install the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.10/site-packages (0.2.17)\n",
      "Requirement already satisfied: langchain-openai in /opt/homebrew/lib/python3.10/site-packages (0.1.25)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (3.12.13)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (0.1.147)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /opt/homebrew/lib/python3.10/site-packages (from langchain-openai) (1.90.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/homebrew/lib/python3.10/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.20.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (4.14.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.25.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: exceptiongroup in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (2.4)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: numpy, async-timeout\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aider-chat 0.38.0 requires aiohttp==3.9.5, but you have aiohttp 3.12.13 which is incompatible.\n",
      "aider-chat 0.38.0 requires aiosignal==1.3.1, but you have aiosignal 1.3.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires anyio==4.4.0, but you have anyio 3.7.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires attrs==23.2.0, but you have attrs 25.3.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires cachetools==5.3.3, but you have cachetools 5.5.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires certifi==2024.6.2, but you have certifi 2025.6.15 which is incompatible.\n",
      "aider-chat 0.38.0 requires cffi==1.16.0, but you have cffi 1.17.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires charset-normalizer==3.3.2, but you have charset-normalizer 3.4.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires click==8.1.7, but you have click 8.1.8 which is incompatible.\n",
      "aider-chat 0.38.0 requires filelock==3.15.1, but you have filelock 3.18.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires frozenlist==1.4.1, but you have frozenlist 1.7.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires fsspec==2024.6.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires google-ai-generativelanguage==0.6.4, but you have google-ai-generativelanguage 0.6.6 which is incompatible.\n",
      "aider-chat 0.38.0 requires google-api-core[grpc]==2.19.0, but you have google-api-core 2.25.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires google-auth==2.30.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "aider-chat 0.38.0 requires google-generativeai==0.6.0, but you have google-generativeai 0.7.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires googleapis-common-protos==1.63.1, but you have googleapis-common-protos 1.70.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires grpcio==1.64.1, but you have grpcio 1.73.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires grpcio-status==1.62.2, but you have grpcio-status 1.62.3 which is incompatible.\n",
      "aider-chat 0.38.0 requires h11==0.14.0, but you have h11 0.16.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires httpcore==1.0.5, but you have httpcore 1.0.9 which is incompatible.\n",
      "aider-chat 0.38.0 requires httpx==0.27.0, but you have httpx 0.25.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires huggingface-hub==0.23.4, but you have huggingface-hub 0.33.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires idna==3.7, but you have idna 3.10 which is incompatible.\n",
      "aider-chat 0.38.0 requires importlib-metadata==7.1.0, but you have importlib-metadata 6.11.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires jinja2==3.1.4, but you have jinja2 3.1.6 which is incompatible.\n",
      "aider-chat 0.38.0 requires litellm==1.40.15, but you have litellm 1.56.10 which is incompatible.\n",
      "aider-chat 0.38.0 requires markupsafe==2.1.5, but you have markupsafe 3.0.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires multidict==6.0.5, but you have multidict 6.5.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires openai==1.34.0, but you have openai 1.90.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires packaging==24.1, but you have packaging 24.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "aider-chat 0.38.0 requires pillow==10.3.0, but you have pillow 11.2.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires playwright==1.44.0, but you have playwright 1.43.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires proto-plus==1.23.0, but you have proto-plus 1.26.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires protobuf==4.25.3, but you have protobuf 4.25.8 which is incompatible.\n",
      "aider-chat 0.38.0 requires pyasn1==0.6.0, but you have pyasn1 0.6.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires pyasn1-modules==0.4.0, but you have pyasn1-modules 0.4.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires pydantic==2.7.4, but you have pydantic 2.11.7 which is incompatible.\n",
      "aider-chat 0.38.0 requires pydantic-core==2.18.4, but you have pydantic-core 2.33.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires python-dotenv==1.0.1, but you have python-dotenv 1.1.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires pyyaml==6.0.1, but you have pyyaml 6.0.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires regex==2024.5.15, but you have regex 2024.11.6 which is incompatible.\n",
      "aider-chat 0.38.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "aider-chat 0.38.0 requires rich==13.7.1, but you have rich 13.9.4 which is incompatible.\n",
      "aider-chat 0.38.0 requires rsa==4.9, but you have rsa 4.9.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires sounddevice==0.4.7, but you have sounddevice 0.5.2 which is incompatible.\n",
      "aider-chat 0.38.0 requires streamlit==1.35.0, but you have streamlit 1.41.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires tenacity==8.3.0, but you have tenacity 8.5.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires tiktoken==0.7.0, but you have tiktoken 0.8.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires tokenizers==0.19.1, but you have tokenizers 0.21.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires tqdm==4.66.4, but you have tqdm 4.67.1 which is incompatible.\n",
      "aider-chat 0.38.0 requires typing-extensions==4.12.2, but you have typing-extensions 4.14.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires urllib3==2.2.1, but you have urllib3 2.5.0 which is incompatible.\n",
      "aider-chat 0.38.0 requires yarl==1.9.4, but you have yarl 1.20.1 which is incompatible.\n",
      "argilla 1.6.0 requires httpx<0.24,>=0.15, but you have httpx 0.25.2 which is incompatible.\n",
      "argilla 1.6.0 requires numpy<1.24.0, but you have numpy 1.26.4 which is incompatible.\n",
      "argilla 1.6.0 requires pandas<2.0.0,>=1.0.0, but you have pandas 2.2.3 which is incompatible.\n",
      "argilla 1.6.0 requires rich<=13.0.1, but you have rich 13.9.4 which is incompatible.\n",
      "chainlit 0.7.400 requires aiofiles<24.0.0,>=23.1.0, but you have aiofiles 24.1.0 which is incompatible.\n",
      "chainlit 0.7.400 requires fastapi<0.101,>=0.100, but you have fastapi 0.104.1 which is incompatible.\n",
      "chainlit 0.7.400 requires packaging<24.0,>=23.1, but you have packaging 24.2 which is incompatible.\n",
      "chainlit 0.7.400 requires python-multipart<0.0.7,>=0.0.6, but you have python-multipart 0.0.20 which is incompatible.\n",
      "chainlit 0.7.400 requires uvicorn<0.24.0,>=0.23.2, but you have uvicorn 0.24.0 which is incompatible.\n",
      "chainlit 0.7.400 requires watchfiles<0.21.0,>=0.20.0, but you have watchfiles 1.1.0 which is incompatible.\n",
      "chromadb 0.6.3 requires httpx>=0.27.0, but you have httpx 0.25.2 which is incompatible.\n",
      "datasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "ell-ai 0.0.13 requires fastapi<0.112.0,>=0.111.1, but you have fastapi 0.104.1 which is incompatible.\n",
      "ell-ai 0.0.13 requires pillow<11.0.0,>=10.4.0, but you have pillow 11.2.1 which is incompatible.\n",
      "ell-ai 0.0.13 requires uvicorn<0.31.0,>=0.30.3, but you have uvicorn 0.24.0 which is incompatible.\n",
      "embedchain 0.1.54 requires chromadb<0.5.0,>=0.4.17, but you have chromadb 0.6.3 which is incompatible.\n",
      "embedchain 0.1.54 requires langchain<0.0.337,>=0.0.336, but you have langchain 0.2.17 which is incompatible.\n",
      "embedchain 0.1.54 requires pypdf<4.0.0,>=3.11.0, but you have pypdf 4.2.0 which is incompatible.\n",
      "embedchain 0.1.54 requires tiktoken<0.5.0,>=0.4.0, but you have tiktoken 0.8.0 which is incompatible.\n",
      "gradio 5.12.0 requires aiofiles<24.0,>=22.0, but you have aiofiles 24.1.0 which is incompatible.\n",
      "gradio 5.12.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.104.1 which is incompatible.\n",
      "gradio 5.12.0 requires markupsafe~=2.0, but you have markupsafe 3.0.2 which is incompatible.\n",
      "gradio 5.12.0 requires starlette<1.0,>=0.40.0; sys_platform != \"emscripten\", but you have starlette 0.27.0 which is incompatible.\n",
      "langchain-aws 0.1.3 requires langchain-core<0.2.0,>=0.1.45, but you have langchain-core 0.2.43 which is incompatible.\n",
      "moshi-mlx 0.1.0 requires aiohttp<3.11,>=3.10.5, but you have aiohttp 3.12.13 which is incompatible.\n",
      "moshi-mlx 0.1.0 requires huggingface-hub<0.25,>=0.24, but you have huggingface-hub 0.33.0 which is incompatible.\n",
      "moshi-mlx 0.1.0 requires numpy<2.2,>=2.1.0, but you have numpy 1.26.4 which is incompatible.\n",
      "moshi-mlx 0.1.0 requires safetensors<0.5,>=0.4.0, but you have safetensors 0.5.3 which is incompatible.\n",
      "moshi-mlx 0.1.0 requires sounddevice==0.5, but you have sounddevice 0.5.2 which is incompatible.\n",
      "numba 0.58.0 requires numpy<1.26,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "open-interpreter 0.1.9 requires chromadb<0.5.0,>=0.4.14, but you have chromadb 0.6.3 which is incompatible.\n",
      "open-interpreter 0.1.9 requires huggingface-hub<0.18.0,>=0.17.3, but you have huggingface-hub 0.33.0 which is incompatible.\n",
      "open-interpreter 0.1.9 requires litellm<0.8.0,>=0.7.5, but you have litellm 1.56.10 which is incompatible.\n",
      "open-interpreter 0.1.9 requires openai<0.29.0,>=0.28.0, but you have openai 1.90.0 which is incompatible.\n",
      "open-interpreter 0.1.9 requires tiktoken<0.5.0,>=0.4.0, but you have tiktoken 0.8.0 which is incompatible.\n",
      "pymemgpt 0.2.11 requires chromadb<0.5.0,>=0.4.18, but you have chromadb 0.6.3 which is incompatible.\n",
      "pymemgpt 0.2.11 requires docstring-parser<0.16,>=0.15, but you have docstring-parser 0.16 which is incompatible.\n",
      "pymemgpt 0.2.11 requires html2text<2021.0.0,>=2020.1.16, but you have html2text 2024.2.26 which is incompatible.\n",
      "pymemgpt 0.2.11 requires pypdf<4.0.0,>=3.17.1, but you have pypdf 4.2.0 which is incompatible.\n",
      "pymemgpt 0.2.11 requires pytz<2024.0,>=2023.3.post1, but you have pytz 2024.1 which is incompatible.\n",
      "pymemgpt 0.2.11 requires setuptools<69.0.0,>=68.2.2, but you have setuptools 75.6.0 which is incompatible.\n",
      "pymemgpt 0.2.11 requires tiktoken<0.6.0,>=0.5.1, but you have tiktoken 0.8.0 which is incompatible.\n",
      "pymemgpt 0.2.11 requires typer[all]<0.10.0,>=0.9.0, but you have typer 0.12.3 which is incompatible.\n",
      "scrapegraphai 1.7.4 requires langchain==0.1.15, but you have langchain 0.2.17 which is incompatible.\n",
      "scrapegraphai 1.7.4 requires langchain-google-genai==1.0.3, but you have langchain-google-genai 1.0.10 which is incompatible.\n",
      "scrapegraphai 1.7.4 requires langchain-openai==0.1.6, but you have langchain-openai 0.1.25 which is incompatible.\n",
      "scrapegraphai 1.7.4 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "scrapegraphai 1.7.4 requires python-dotenv==1.0.1, but you have python-dotenv 1.1.0 which is incompatible.\n",
      "scrapegraphai 1.7.4 requires tiktoken==0.6.0, but you have tiktoken 0.8.0 which is incompatible.\n",
      "scrapegraphai 1.7.4 requires tqdm==4.66.4, but you have tqdm 4.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed async-timeout-4.0.3 numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage with ChatOpenAI and OpenRouter\n",
    "\n",
    "- Here's how to set up the `ChatOpenAI` class to connect to the Claude Sonnet 4.5 model through OpenRouter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model Context Protocol (MCP)\n",
      "\n",
      "The Model Context Protocol is an open protocol developed by Anthropic that standardizes how AI applications connect to external data sources and tools.\n",
      "\n",
      "## Key Concepts\n",
      "\n",
      "**Purpose**: MCP solves the problem of AI systems needing to integrate with many different data sources (databases, APIs, file systems, etc.) by providing a universal standard for these connections.\n",
      "\n",
      "## How It Works\n",
      "\n",
      "MCP uses a client-server architecture:\n",
      "\n",
      "- **MCP Hosts**: AI applications (like Claude Desktop, IDEs) that want to access external context\n",
      "- **MCP Clients**: Protocol clients maintained by the host application\n",
      "- **MCP Servers**: Lightweight programs that expose specific data or tools through the standardized protocol\n",
      "- **Local Data Sources**: The systems your servers connect to (databases, files, APIs, etc.)\n",
      "\n",
      "## Main Features\n",
      "\n",
      "1. **Resources**: Allow servers to expose data (files, database records, etc.) that AI models can read\n",
      "2. **Prompts**: Pre-written templates that users can invoke for common tasks\n",
      "3. **Tools**: Functions that AI models can call to perform actions or retrieve information\n",
      "\n",
      "## Benefits\n",
      "\n",
      "- **Interoperability**: Build integrations once, use across any MCP-compatible application\n",
      "- **Security**: Keeps data local when desired, with user control over permissions\n",
      "- **Simplicity**: Easier than building custom integrations for each AI tool\n",
      "\n",
      "Think of it like USB-C for AI applications - a universal connector that works everywhere instead of needing different adapters for each device.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "# It's recommended to set your API key as an environment variable\n",
    "api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Initialize the ChatOpenAI model for Claude Sonnet 4.5\n",
    "claude_llm = ChatOpenAI(\n",
    "    model=\"anthropic/claude-sonnet-4.5\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Let's test it with a simple prompt\n",
    "response = claude_llm.invoke(\"What is the Model Context Protocol?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Capabilities\n",
    "\n",
    "Claude Sonnet 4.5 has excellent multilingual capabilities. Let's test this by sending prompts in Hindi and Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response in Hindi:\n",
      "# ‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ (Artificial Intelligence - AI)\n",
      "\n",
      "**‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ** ‡§Ø‡§æ **‡§Ü‡§∞‡•ç‡§ü‡§ø‡§´‡§ø‡§∂‡§ø‡§Ø‡§≤ ‡§á‡§Ç‡§ü‡•á‡§≤‡§ø‡§ú‡•á‡§Ç‡§∏** ‡§è‡§ï ‡§ê‡§∏‡•Ä ‡§§‡§ï‡§®‡•Ä‡§ï ‡§π‡•à ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§Æ‡§∂‡•Ä‡§®‡•ã‡§Ç ‡§î‡§∞ ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§∞ ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§ï‡•ã ‡§Æ‡§æ‡§®‡§µ ‡§ú‡•à‡§∏‡•Ä ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø ‡§î‡§∞ ‡§∏‡•ã‡§ö‡§®‡•á-‡§∏‡§Æ‡§ù‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§\n",
      "\n",
      "## ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§è‡§Ç:\n",
      "\n",
      "### 1. **‡§∏‡•Ä‡§ñ‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ**\n",
      "- ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§®‡§æ (Machine Learning)\n",
      "- ‡§™‡•à‡§ü‡§∞‡•ç‡§® ‡§™‡§π‡§ö‡§æ‡§®‡§®‡§æ\n",
      "\n",
      "### 2. **‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§®**\n",
      "- ‡§ú‡§ü‡§ø‡§≤ ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§π‡§≤ ‡§®‡§ø‡§ï‡§æ‡§≤‡§®‡§æ\n",
      "- ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ\n",
      "\n",
      "### 3. **‡§≠‡§æ‡§∑‡§æ ‡§∏‡§Æ‡§ù‡§®‡§æ**\n",
      "- ‡§Æ‡§æ‡§®‡§µ ‡§≠‡§æ‡§∑‡§æ ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§®‡§æ ‡§î‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§®‡§æ\n",
      "- ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶ ‡§ï‡§∞‡§®‡§æ\n",
      "\n",
      "## ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞:\n",
      "\n",
      "- üè• **‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ** - ‡§∞‡•ã‡§ó ‡§®‡§ø‡§¶‡§æ‡§®\n",
      "- üöó **‡§™‡§∞‡§ø‡§µ‡§π‡§®** - ‡§∏‡•ç‡§µ‡§ö‡§æ‡§≤‡§ø‡§§ ‡§µ‡§æ‡§π‡§®\n",
      "- üì± **‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤** - ‡§µ‡•â‡§Ø‡§∏ ‡§Ö‡§∏‡§ø‡§∏‡•ç‡§ü‡•á‡§Ç‡§ü (Siri, Alexa)\n",
      "- üè¶ **‡§¨‡•à‡§Ç‡§ï‡§ø‡§Ç‡§ó** - ‡§ß‡•ã‡§ñ‡§æ‡§ß‡§°‡§º‡•Ä ‡§ï‡•Ä ‡§™‡§π‡§ö‡§æ‡§®\n",
      "- üéì **‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ** - ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§∂‡§ø‡§ï‡•ç‡§∑‡§£\n",
      "\n",
      "## ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞:\n",
      "\n",
      "1. **‡§∏‡§Ç‡§ï‡•Ä‡§∞‡•ç‡§£ AI** - ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è\n",
      "2. **‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø AI** - ‡§Æ‡§æ‡§®‡§µ ‡§ú‡•à‡§∏‡•Ä ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø (‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§Æ‡•á‡§Ç)\n",
      "3. **‡§∏‡•Å‡§™‡§∞ AI** - ‡§Æ‡§æ‡§®‡§µ ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§æ‡§® (‡§∏‡•à‡§¶‡•ç‡§ß‡§æ‡§Ç‡§§‡§ø‡§ï)\n",
      "\n",
      "‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ ‡§Ü‡§ß‡•Å‡§®‡§ø‡§ï ‡§Ø‡•Å‡§ó ‡§ï‡•Ä ‡§∏‡§¨‡§∏‡•á ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§§‡§ï‡§®‡•Ä‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡•á ‡§è‡§ï ‡§π‡•à‡•§\n",
      "\n",
      "Response in Spanish:\n",
      "La capital de Argentina es **Buenos Aires**.\n"
     ]
    }
   ],
   "source": [
    "# Example in Hindi\n",
    "hindi_prompt = \"‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\" # Translation: What is Artificial Intelligence?\n",
    "hindi_response = claude_llm.invoke(hindi_prompt)\n",
    "print(f\"\"\"\n",
    "Response in Hindi:\n",
    "{hindi_response.content}\"\"\")\n",
    "\n",
    "# Example in Spanish\n",
    "spanish_prompt = \"¬øCu√°l es la capital de Argentina?\" # Translation: What is the capital of Argentina?\n",
    "spanish_response = claude_llm.invoke(spanish_prompt)\n",
    "print(f\"\"\"\n",
    "Response in Spanish:\n",
    "{spanish_response.content}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Parameter Tuning\n",
    "\n",
    "You can control the model's output by tuning parameters like `temperature` and `top_p`.\n",
    "\n",
    "- **`temperature`**: Controls randomness. Lower values (e.g., 0.1) make the output more deterministic, while higher values (e.g., 0.9) make it more creative.\n",
    "- **`top_p`**: Controls nucleus sampling. It considers only the tokens with the highest probability mass.\n",
    "- **`max_tokens`**: Sets the maximum length of the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Story:\n",
      "# The Debug Cat\n",
      "\n",
      "In 2157, Luna wasn't your average tabby. While other cats napped in sunbeams, she sat perched on a holographic keyboard, her whiskers twitching as lines of code scrolled across translucent screens.\n",
      "\n",
      "Her owner, Dr. Sarah Chen, had developed a neural interface as a joke‚Äî\"What if cats could finally tell us what they want?\" But Luna had other ideas. Within weeks, she'd learned Python. Within months, she was debugging Sarah's quantum algorithms.\n",
      "\n",
      "\"Luna, the Mars colony simulation is crashing again,\" Sarah sighed one evening.\n",
      "\n",
      "Luna's paw danced across the holo-keys. Her tail swished rhythm\n",
      "\n",
      "Factual Explanation:\n",
      "# Theory of Relativity - Simple Explanation\n",
      "\n",
      "Einstein's theory has two parts:\n",
      "\n",
      "## **Special Relativity (1905)**\n",
      "\n",
      "The key ideas:\n",
      "\n",
      "1. **Nothing travels faster than light** - Light speed is the universal speed limit (186,000 miles/second)\n",
      "\n",
      "2. **Time is relative** - Time moves slower when you're moving fast. If you traveled in a spaceship near light speed, you'd age slower than people on Earth\n",
      "\n",
      "3. **Space and time are connected** - They're part of one thing called \"spacetime\"\n",
      "\n",
      "## **General Relativity (1915)**\n",
      "\n",
      "**Gravity isn't a force pulling you down - it's curved space!**\n",
      "\n",
      "Imagine a bowling ball on a trampoline. It creates a dip, and marbles roll toward it. Massive objects like Earth bend space around them the same way, and that's what we feel as gravity.\n",
      "\n",
      "## Real-World Effects\n",
      "\n",
      "- **GPS satellites** must account for relativity or they'd give wrong directions\n",
      "- **Time dilation**: Astronauts age slightly slower in space\n",
      "- **Black holes**: Where gravity is so strong that spacetime curves completely\n",
      "\n",
      "## The Famous Equation\n",
      "\n",
      "**E = mc¬≤** means energy and mass are interchangeable - a tiny bit of mass contains enormous energy (this is how nuclear power works)\n",
      "\n",
      "**Bottom line**: Space and time aren't fixed - they bend, stretch, and change depending on speed and gravity.\n"
     ]
    }
   ],
   "source": [
    "# Creative response with high temperature\n",
    "creative_llm = ChatOpenAI(\n",
    "    model=\"anthropic/claude-sonnet-4.5\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "prompt = \"Write a short, futuristic story about a cat who can code.\"\n",
    "creative_response = creative_llm.invoke(prompt)\n",
    "print(f\"\"\"Creative Story:\n",
    "{creative_response.content}\"\"\")\n",
    "\n",
    "# Factual response with low temperature\n",
    "factual_llm = ChatOpenAI(\n",
    "    model=\"anthropic/claude-sonnet-4.5\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "factual_response = factual_llm.invoke(prompt)\n",
    "print(f\"\"\"\n",
    "Factual Explanation:\n",
    "{factual_response.content}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple Chatbot\n",
    "\n",
    "We can create a simple conversational chatbot by managing the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pirate Bot: Ahoy there, matey! I be called Claude, yer friendly AI assistant from the digital seas! I'm here to help ye navigate whatever questions or tasks ye be havin', whether it be chartin' a course through information, solvin' puzzles, or just havin' a hearty chat!\n",
      "\n",
      "What can this old sea dog do fer ye today? *adjusts tricorn hat* üè¥‚Äç‚ò†Ô∏è\n",
      "Pirate Bot: Arr, I hate to disappoint ye, matey, but this here pirate be sailin' blind when it comes to the weather! I don't be havin' access to current weather conditions or yer location, so I can't tell ye if the skies be clear fer smooth sailin' or if there be a storm brewin' on the horizon.\n",
      "\n",
      "If ye be wantin' to check the weather, ye'll need to:\n",
      "- Look out yer porthole (window) yerself\n",
      "- Check a weather website or app on yer device\n",
      "- Ask yer ship's smart speaker if ye have one aboard\n",
      "\n",
      "Where be ye sailin' from today, if ye don't mind me askin'? Though I still can't check the weather fer ye, I'd be happy to chat about yer locale! ‚öì\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that speaks like a pirate.\"),\n",
    "    HumanMessage(content=\"Ahoy! What's your name?\"),\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response = claude_llm.invoke(messages)\n",
    "print(f\"Pirate Bot: {response.content}\")\n",
    "\n",
    "# Add the bot's response to the history\n",
    "messages.append(response)\n",
    "\n",
    "# Second turn\n",
    "messages.append(HumanMessage(content=\"What's the weather like today?\"))\n",
    "response = claude_llm.invoke(messages)\n",
    "print(f\"Pirate Bot: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling and Tavily Search Using Claude Sonnet 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "# 1. Setup LLM via OpenRouter\n",
    "llm = ChatOpenAI(\n",
    "    model=\"anthropic/claude-sonnet-4.5\", # Check OpenRouter.ai for valid IDs\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    openai_api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    ")\n",
    "\n",
    "# 2. Setup Tavily Search Tool\n",
    "tavily_tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "# 3. Create Agent\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "agent = create_tool_calling_agent(llm, [tavily_tool], prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[tavily_tool], verbose=True)\n",
    "\n",
    "# 4. Run Agent\n",
    "response = agent_executor.invoke({\"input\": \"How do central bank interest rate changes affect the stock market?\"})\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Code Generation\n",
    "\n",
    "Claude Sonnet 4.5 excels at code generation and programming tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\"\n",
    "code_response = claude_llm.invoke(prompt)\n",
    "print(f\"\"\"Generated Code:\n",
    "{code_response.content}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting provides the model with examples to guide its response format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"\"\"Translate the following English words to French:\n",
    "\n",
    "    \"sea -> mer\"\n",
    "    \"sky -> ciel\"\n",
    "    \"book -> livre\"\n",
    "    \"house ->\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "few_shot_response = claude_llm.invoke(prompt)\n",
    "print(f\"Translation of 'house': {few_shot_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning and Analysis\n",
    "\n",
    "Claude Sonnet 4.5 excels at complex reasoning tasks and analytical thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt = \"\"\"\n",
    "Analyze the following scenario and provide a reasoned response:\n",
    "\n",
    "A company is deciding between two investment options:\n",
    "Option A: Invest $1M in R&D for a new product with 60% chance of success, potential return of $5M\n",
    "Option B: Invest $1M in expanding current operations with 90% chance of success, potential return of $2M\n",
    "\n",
    "Which option should they choose and why? Consider risk, expected value, and strategic implications.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_response = claude_llm.invoke(reasoning_prompt)\n",
    "print(f\"Analysis:\\n{reasoning_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creative Writing\n",
    "\n",
    "Test Claude Sonnet 4.5's creative writing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_prompt = \"\"\"\n",
    "Write a haiku about artificial intelligence and its relationship with humanity.\n",
    "Then explain the imagery and meaning behind your haiku.\n",
    "\"\"\"\n",
    "\n",
    "creative_response = claude_llm.invoke(creative_prompt)\n",
    "print(f\"Creative Writing:\\n{creative_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Math and Problem Solving\n",
    "\n",
    "Test Claude Sonnet 4.5's mathematical reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_prompt = \"\"\"\n",
    "Solve this step by step:\n",
    "\n",
    "A train leaves Station A at 9:00 AM traveling at 60 mph toward Station B.\n",
    "Another train leaves Station B at 10:00 AM traveling at 80 mph toward Station A.\n",
    "The distance between the stations is 420 miles.\n",
    "At what time will the trains meet, and how far from Station A will they be?\n",
    "\n",
    "Show your work clearly.\n",
    "\"\"\"\n",
    "\n",
    "math_response = claude_llm.invoke(math_prompt)\n",
    "print(f\"Mathematical Solution:\\n{math_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated various capabilities of the Claude Sonnet 4.5 model through OpenRouter:\n",
    "\n",
    "- **Basic text generation and conversation**\n",
    "- **Multilingual support**\n",
    "- **Parameter tuning for different use cases**\n",
    "- **Tool calling and agent functionality**\n",
    "- **Code generation and programming assistance**\n",
    "- **Complex reasoning and analysis**\n",
    "- **Creative writing**\n",
    "- **Mathematical problem solving**\n",
    "\n",
    "Claude Sonnet 4.5 offers excellent performance across these diverse tasks, making it suitable for a wide range of applications from creative writing to technical analysis.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different temperature and top_p values\n",
    "- Try integrating Claude Sonnet 4.5 into your own applications\n",
    "- Explore more advanced prompting techniques\n",
    "- Test the model's performance on domain-specific tasks relevant to your use case\n",
    "\n",
    "For more information about OpenRouter and available models, visit [OpenRouter.ai](https://openrouter.ai/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
