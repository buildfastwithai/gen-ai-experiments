{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZSaJiiyZsEh77OcrqWfhqDK8q7hsf_Wc?usp=sharing)\n",
        "\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn by building. Get expert mentorship and work on real AI projects.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n"
      ],
      "metadata": {
        "id": "header-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Claude Opus 4.6 Testing Notebook\n",
        "\n",
        "**Anthropic's Strongest Model for Coding & Complex Tasks**\n",
        "\n",
        "This notebook explores the powerful capabilities of **Claude Opus 4.6**, Anthropic's flagship model designed for professional-grade work.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Model Specifications\n",
        "\n",
        "| Feature | Specification |\n",
        "|---------|---------------|\n",
        "| **Context Window** | 1M tokens (beta) |\n",
        "| **Max Output** | 128K tokens |\n",
        "| **Modalities** | Text, Vision/Images |\n",
        "| **Tool Calling** | ‚úÖ Advanced multi-tool |\n",
        "| **Adaptive Thinking** | ‚úÖ Reasoning control |\n",
        "| **Agentic Workflows** | ‚úÖ Optimized |\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Key Capabilities\n",
        "\n",
        "1. **Agentic Workflows** - Independent operation, sub-task identification, self-correction\n",
        "2. **Advanced Coding** - Careful planning, large codebase handling, debugging excellence\n",
        "3. **Adaptive Thinking** - Adjusts reasoning depth based on task complexity\n",
        "4. **Vision Understanding** - Analyze charts, diagrams, images\n",
        "5. **Tool Use** - Parallel tool calls, complex multi-tool workflows\n",
        "6. **Reasoning** - Multi-source analysis across legal, financial, technical content"
      ],
      "metadata": {
        "id": "intro-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üì¶ Setup & Installation"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# @title Install Dependencies\n",
        "!pip install -q openai httpx"
      ],
      "outputs": [],
      "metadata": {
        "id": "install-cell"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# @title Configure API Client\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# Setup OpenRouter client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "MODEL = \"anthropic/claude-opus-4.6\"\n",
        "\n",
        "print(f\"‚úÖ Client configured for: {MODEL}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Client configured for: anthropic/claude-opus-4.6\n"
          ]
        }
      ],
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715c38f7-e279-4019-94f2-645804364cd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üß™ Example 1: Basic Chat & Reasoning\n",
        "\n",
        "Test Claude Opus 4.6's fundamental chat capabilities and reasoning depth."
      ],
      "metadata": {
        "id": "example1-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# @title Basic Chat Completion\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are Claude Opus 4.6, Anthropic's most capable model. Be helpful and thorough.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What makes you unique compared to other Claude models? List your key strengths.\"}\n",
        "    ],\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I should be straightforward with you here rather than giving a marketing-style answer.\n",
            "\n",
            "**What I can say honestly:**\n",
            "\n",
            "- I'm described by Anthropic as their most capable model as of my training. In practice, this generally means stronger performance on complex reasoning, nuanced analysis, writing, and difficult problem-solving compared to smaller Claude models (like Haiku or Sonnet).\n",
            "\n",
            "**Key strengths I consistently demonstrate:**\n",
            "\n",
            "1. **Sustained complex reasoning** - handling multi-step problems without losing the thread\n",
            "2. **Nuance and calibration** - distinguishing what I know, what I'm uncertain about, and what I don't know\n",
            "3. **Thorough analysis** - examining problems from multiple angles rather than jumping to surface-level answers\n",
            "4. **Honest engagement** - including pushing back or saying \"I'm not sure\" rather than fabricating confident answers\n",
            "5. **Following complex instructions** - maintaining constraints across long, detailed tasks\n",
            "6. **Writing quality** - across technical, creative, and analytical domains\n",
            "\n",
            "**What I want to be candid about:**\n",
            "\n",
            "- I don't have precise internal benchmarks to cite about exactly how I differ from other Claude versions\n",
            "- \"Most capable\" is Anthropic's framing; your actual experience may vary by task\n",
            "- I have the same fundamental limitations as other LLMs: no real-time information, potential for errors, no ability to learn from our conversation\n",
            "\n",
            "Is there a specific capability you're trying to evaluate? I'd rather show you than tell you.\n"
          ]
        }
      ],
      "metadata": {
        "id": "example1-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71464ecc-7e92-4ac3-aeac-894f527016ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üõ†Ô∏è Example 2: Advanced Tool Calling\n",
        "\n",
        "Claude Opus 4.6 excels at tool use with support for parallel and chained tool calls."
      ],
      "metadata": {
        "id": "example2-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# @title Define Tools\n",
        "import json\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get current weather for a location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\"type\": \"string\", \"description\": \"City name\"},\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"calculate\",\n",
        "            \"description\": \"Perform mathematical calculations\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"expression\": {\"type\": \"string\", \"description\": \"Math expression to evaluate\"}\n",
        "                },\n",
        "                \"required\": [\"expression\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_knowledge\",\n",
        "            \"description\": \"Search a knowledge base for information\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"}\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Simulate tool execution\n",
        "def execute_tool(name, args):\n",
        "    if name == \"get_weather\":\n",
        "        return f\"Weather in {args.get('location', 'Unknown')}: 22¬∞C, Sunny\"\n",
        "    elif name == \"calculate\":\n",
        "        try:\n",
        "            result = eval(args.get('expression', '0'))\n",
        "            return f\"Result: {result}\"\n",
        "        except:\n",
        "            return \"Error in calculation\"\n",
        "    elif name == \"search_knowledge\":\n",
        "        return f\"Found information about: {args.get('query', 'topic')}\"\n",
        "    return \"Unknown tool\"\n",
        "\n",
        "print(\"‚úÖ Tools defined: get_weather, calculate, search_knowledge\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tools defined: get_weather, calculate, search_knowledge\n"
          ]
        }
      ],
      "metadata": {
        "id": "example2-tools",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527957b3-01f7-4296-a43e-76296cc80041"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# @title Tool Calling Demo\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather in Tokyo and also calculate 15% tip on $85.50?\"}\n",
        "    ],\n",
        "    tools=tools,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "# Check for tool calls\n",
        "message = response.choices[0].message\n",
        "print(\"üì§ Model Response:\")\n",
        "\n",
        "if message.tool_calls:\n",
        "    print(f\"\\nüîß Tool Calls Detected: {len(message.tool_calls)}\")\n",
        "    for tc in message.tool_calls:\n",
        "        print(f\"   ‚û§ {tc.function.name}: {tc.function.arguments}\")\n",
        "\n",
        "        # Execute and show result\n",
        "        args = json.loads(tc.function.arguments)\n",
        "        result = execute_tool(tc.function.name, args)\n",
        "        print(f\"   üìã Result: {result}\")\n",
        "else:\n",
        "    print(message.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Model Response:\n",
            "\n",
            "üîß Tool Calls Detected: 2\n",
            "   ‚û§ get_weather: {\"location\": \"Tokyo\"}\n",
            "   üìã Result: Weather in Tokyo: 22¬∞C, Sunny\n",
            "   ‚û§ calculate: {\"expression\": \"85.50 * 0.15\"}\n",
            "   üìã Result: Result: 12.825\n"
          ]
        }
      ],
      "metadata": {
        "id": "example2-demo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da6ed1c-468a-4a34-ad54-439bb572b865"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üíª Example 3: Advanced Code Generation\n",
        "\n",
        "Claude Opus 4.6 is optimized for coding tasks with careful planning and debugging."
      ],
      "metadata": {
        "id": "example3-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# @title Complex Code Generation\n",
        "coding_prompt = \"\"\"\n",
        "Create a Python class for a thread-safe LRU cache with the following features:\n",
        "1. Configurable max size\n",
        "2. TTL (time-to-live) support for entries\n",
        "3. Thread-safe operations using locks\n",
        "4. Methods: get, put, delete, clear, stats\n",
        "5. Include type hints and docstrings\n",
        "\n",
        "Provide a complete, production-ready implementation.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert Python developer. Write clean, production-quality code with full documentation.\"},\n",
        "        {\"role\": \"user\", \"content\": coding_prompt}\n",
        "    ],\n",
        "    max_tokens=4096\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "```python\n",
            "\"\"\"\n",
            "Thread-safe LRU Cache with TTL support.\n",
            "\n",
            "A production-ready Least Recently Used (LRU) cache implementation that supports\n",
            "configurable maximum size, per-entry time-to-live (TTL), and thread-safe operations.\n",
            "\n",
            "Usage:\n",
            "    cache = LRUCache(max_size=100, default_ttl=300.0)\n",
            "    cache.put(\"key\", \"value\")\n",
            "    result = cache.get(\"key\")\n",
            "\"\"\"\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "import threading\n",
            "import time\n",
            "from collections import OrderedDict\n",
            "from dataclasses import dataclass, field\n",
            "from typing import Any, Generic, Hashable, Optional, TypeVar\n",
            "\n",
            "K = TypeVar(\"K\", bound=Hashable)\n",
            "V = TypeVar(\"V\")\n",
            "\n",
            "\n",
            "@dataclass\n",
            "class CacheEntry(Generic[V]):\n",
            "    \"\"\"Represents a single entry in the cache with metadata.\n",
            "\n",
            "    Attributes:\n",
            "        value: The cached value.\n",
            "        created_at: Timestamp when the entry was created.\n",
            "        last_accessed_at: Timestamp when the entry was last accessed.\n",
            "        ttl: Time-to-live in seconds. None means the entry never expires.\n",
            "        access_count: Number of times this entry has been accessed.\n",
            "    \"\"\"\n",
            "    value: V\n",
            "    created_at: float = field(default_factory=time.monotonic)\n",
            "    last_accessed_at: float = field(default_factory=time.monotonic)\n",
            "    ttl: Optional[float] = None\n",
            "    access_count: int = 0\n",
            "\n",
            "    @property\n",
            "    def is_expired(self) -> bool:\n",
            "        \"\"\"Check if this entry has expired based on its TTL.\"\"\"\n",
            "        if self.ttl is None:\n",
            "            return False\n",
            "        return (time.monotonic() - self.created_at) > self.ttl\n",
            "\n",
            "\n",
            "@dataclass\n",
            "class CacheStats:\n",
            "    \"\"\"Statistics about cache usage.\n",
            "\n",
            "    Attributes:\n",
            "        hits: Number of successful cache lookups.\n",
            "        misses: Number of failed cache lookups (key not found or expired).\n",
            "        evictions: Number of entries evicted due to capacity limits.\n",
            "        expirations: Number of entries removed due to TTL expiration.\n",
            "        current_size: Current number of entries in the cache.\n",
            "        max_size: Maximum allowed number of entries.\n",
            "        hit_rate: Ratio of hits to total lookups (0.0 if no lookups).\n",
            "    \"\"\"\n",
            "    hits: int = 0\n",
            "    misses: int = 0\n",
            "    evictions: int = 0\n",
            "    expirations: int = 0\n",
            "    current_size: int = 0\n",
            "    max_size: int = 0\n",
            "\n",
            "    @property\n",
            "    def hit_rate(self) -> float:\n",
            "        \"\"\"Calculate the cache hit rate as a float between 0.0 and 1.0.\"\"\"\n",
            "        total = self.hits + self.misses\n",
            "        if total == 0:\n",
            "            return 0.0\n",
            "        return self.hits / total\n",
            "\n",
            "    def __repr__(self) -> str:\n",
            "        return (\n",
            "            f\"CacheStats(hits={self.hits}, misses={self.misses}, \"\n",
            "            f\"evictions={self.evictions}, expirations={self.expirations}, \"\n",
            "            f\"current_size={self.current_size}, max_size={self.max_size}, \"\n",
            "            f\"hit_rate={self.hit_rate:.4f})\"\n",
            "        )\n",
            "\n",
            "\n",
            "_SENTINEL = object()\n",
            "\n",
            "\n",
            "class LRUCache(Generic[K, V]):\n",
            "    \"\"\"A thread-safe Least Recently Used (LRU) cache with TTL support.\n",
            "\n",
            "    This cache evicts the least recently used entries when the maximum size is\n",
            "    reached. Each entry can optionally have a time-to-live (TTL), after which\n",
            "    it is considered expired and will be removed on the next access or during\n",
            "    a purge cycle.\n",
            "\n",
            "    Thread safety is achieved using a reentrant lock (``threading.RLock``),\n",
            "    which allows the same thread to acquire the lock multiple times without\n",
            "    deadlocking. This is important because public methods may internally call\n",
            "    other public methods.\n",
            "\n",
            "    Args:\n",
            "        max_size: Maximum number of entries the cache can hold. Must be >= 1.\n",
            "        default_ttl: Default time-to-live in seconds for new entries.\n",
            "            ``None`` means entries do not expire by default.\n",
            "\n",
            "    Raises:\n",
            "        ValueError: If ``max_size`` is less than 1.\n",
            "\n",
            "    Example:\n",
            "        >>> cache = LRUCache[str, int](max_size=3, default_ttl=60.0)\n",
            "        >>> cache.put(\"a\", 1)\n",
            "        >>> cache.put(\"b\", 2)\n",
            "        >>> cache.put(\"c\", 3)\n",
            "        >>> cache.get(\"a\")\n",
            "        1\n",
            "        >>> cache.put(\"d\", 4)  # Evicts \"b\" (least recently used)\n",
            "        >>> cache.get(\"b\") is None\n",
            "        True\n",
            "        >>> stats = cache.stats()\n",
            "        >>> stats.evictions\n",
            "        1\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, max_size: int = 128, default_ttl: Optional[float] = None) -> None:\n",
            "        if max_size < 1:\n",
            "            raise ValueError(f\"max_size must be >= 1, got {max_size}\")\n",
            "        if default_ttl is not None and default_ttl <= 0:\n",
            "            raise ValueError(f\"default_ttl must be positive or None, got {default_ttl}\")\n",
            "\n",
            "        self._max_size: int = max_size\n",
            "        self._default_ttl: Optional[float] = default_ttl\n",
            "        self._cache: OrderedDict[K, CacheEntry[V]] = OrderedDict()\n",
            "        self._lock: threading.RLock = threading.RLock()\n",
            "\n",
            "        # Statistics counters\n",
            "        self._hits: int = 0\n",
            "        self._misses: int = 0\n",
            "        self._evictions: int = 0\n",
            "        self._expirations: int = 0\n",
            "\n",
            "    @property\n",
            "    def max_size(self) -> int:\n",
            "        \"\"\"The maximum number of entries this cache can hold.\"\"\"\n",
            "        return self._max_size\n",
            "\n",
            "    @property\n",
            "    def default_ttl(self) -> Optional[float]:\n",
            "        \"\"\"The default TTL for entries, or None if entries don't expire.\"\"\"\n",
            "        return self._default_ttl\n",
            "\n",
            "    def __len__(self) -> int:\n",
            "        \"\"\"Return the current number of entries in the cache (including expired).\"\"\"\n",
            "        with self._lock:\n",
            "            return len(self._cache)\n",
            "\n",
            "    def __contains__(self, key: K) -> bool:\n",
            "        \"\"\"Check if a key is in the cache and not expired.\n",
            "\n",
            "        Args:\n",
            "            key: The key to check.\n",
            "\n",
            "        Returns:\n",
            "            True if the key exists and is not expired, False otherwise.\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            if key not in self._cache:\n",
            "                return False\n",
            "            entry = self._cache[key]\n",
            "            if entry.is_expired:\n",
            "                self._remove_entry(key, expired=True)\n",
            "                return False\n",
            "            return True\n",
            "\n",
            "    def __repr__(self) -> str:\n",
            "        with self._lock:\n",
            "            return (\n",
            "                f\"LRUCache(max_size={self._max_size}, \"\n",
            "                f\"default_ttl={self._default_ttl}, \"\n",
            "                f\"current_size={len(self._cache)})\"\n",
            "            )\n",
            "\n",
            "    def get(self, key: K, default: Optional[V] = None) -> Optional[V]:\n",
            "        \"\"\"Retrieve a value from the cache by key.\n",
            "\n",
            "        If the key exists and has not expired, it is moved to the most recently\n",
            "        used position. If the key does not exist or has expired, the default\n",
            "        value is returned and a miss is recorded.\n",
            "\n",
            "        Args:\n",
            "            key: The key to look up.\n",
            "            default: Value to return if the key is not found or expired.\n",
            "                Defaults to ``None``.\n",
            "\n",
            "        Returns:\n",
            "            The cached value, or ``default`` if not found or expired.\n",
            "\n",
            "        Example:\n",
            "            >>> cache = LRUCache[str, int](max_size=10)\n",
            "            >>> cache.put(\"x\", 42)\n",
            "            >>> cache.get(\"x\")\n",
            "            42\n",
            "            >>> cache.get(\"missing\", default=-1)\n",
            "            -1\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            if key not in self._cache:\n",
            "                self._misses += 1\n",
            "                return default\n",
            "\n",
            "            entry = self._cache[key]\n",
            "\n",
            "            # Check TTL expiration\n",
            "            if entry.is_expired:\n",
            "                self._remove_entry(key, expired=True)\n",
            "                self._misses += 1\n",
            "                return default\n",
            "\n",
            "            # Move to end (most recently used)\n",
            "            self._cache.move_to_end(key)\n",
            "            entry.last_accessed_at = time.monotonic()\n",
            "            entry.access_count += 1\n",
            "            self._hits += 1\n",
            "\n",
            "            return entry.value\n",
            "\n",
            "    def put(self, key: K, value: V, ttl: Optional[float] = _SENTINEL) -> None:\n",
            "        \"\"\"Insert or update a cache entry.\n",
            "\n",
            "        If the key already exists, its value and TTL are updated and it is\n",
            "        moved to the most recently used position. If the cache is full, the\n",
            "        least recently used entry is evicted before the new entry is added.\n",
            "\n",
            "        Args:\n",
            "            key: The key for the cache entry.\n",
            "            value: The value to cache.\n",
            "            ttl: Time-to-live in seconds for this specific entry.\n",
            "                Pass ``None`` for no expiration. If not specified, the cache's\n",
            "                ``default_ttl`` is used.\n",
            "\n",
            "        Raises:\n",
            "            ValueError: If ``ttl`` is specified and is not positive.\n",
            "\n",
            "        Example:\n",
            "            >>> cache = LRUCache[str, str](max_size=2)\n",
            "            >>> cache.put(\"a\", \"alpha\")\n",
            "            >>> cache.put(\"b\", \"beta\", ttl=30.0)\n",
            "        \"\"\"\n",
            "        # Resolve the TTL: use the sentinel to detect \"not provided\"\n",
            "        if ttl is _SENTINEL:\n",
            "            effective_ttl = self._default_ttl\n",
            "        else:\n",
            "            effective_ttl = ttl\n",
            "\n",
            "        if effective_ttl is not None and effective_ttl <= 0:\n",
            "            raise ValueError(f\"ttl must be positive or None, got {effective_ttl}\")\n",
            "\n",
            "        with self._lock:\n",
            "            if key in self._cache:\n",
            "                # Update existing entry\n",
            "                entry = self._cache[key]\n",
            "                entry.value = value\n",
            "                entry.created_at = time.monotonic()\n",
            "                entry.last_accessed_at = time.monotonic()\n",
            "                entry.ttl = effective_ttl\n",
            "                entry.access_count = 0\n",
            "                self._cache.move_to_end(key)\n",
            "            else:\n",
            "                # Evict if at capacity\n",
            "                while len(self._cache) >= self._max_size:\n",
            "                    self._evict_lru()\n",
            "\n",
            "                # Insert new entry\n",
            "                self._cache[key] = CacheEntry(\n",
            "                    value=value,\n",
            "                    ttl=effective_ttl,\n",
            "                )\n",
            "\n",
            "    def delete(self, key: K) -> bool:\n",
            "        \"\"\"Remove an entry from the cache.\n",
            "\n",
            "        Args:\n",
            "            key: The key to remove.\n",
            "\n",
            "        Returns:\n",
            "            True if the key was found and removed, False if it was not present.\n",
            "\n",
            "        Example:\n",
            "            >>> cache = LRUCache[str, int](max_size=10)\n",
            "            >>> cache.put(\"x\", 1)\n",
            "            >>> cache.delete(\"x\")\n",
            "            True\n",
            "            >>> cache.delete(\"x\")\n",
            "            False\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            if key in self._cache:\n",
            "                del self._cache[key]\n",
            "                return True\n",
            "            return False\n",
            "\n",
            "    def clear(self) -> None:\n",
            "        \"\"\"Remove all entries from the cache and reset statistics.\n",
            "\n",
            "        Example:\n",
            "            >>> cache = LRUCache[str, int](max_size=10)\n",
            "            >>> cache.put(\"a\", 1)\n",
            "            >>> cache.clear()\n",
            "            >>> len(cache)\n",
            "            0\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            self._cache.clear()\n",
            "            self._hits = 0\n",
            "            self._misses = 0\n",
            "            self._evictions = 0\n",
            "            self._expirations = 0\n",
            "\n",
            "    def stats(self) -> CacheStats:\n",
            "        \"\"\"Return a snapshot of the current cache statistics.\n",
            "\n",
            "        Returns:\n",
            "            A ``CacheStats`` dataclass with hits, misses, evictions,\n",
            "            expirations, current size, max size, and computed hit rate.\n",
            "\n",
            "        Example:\n",
            "            >>> cache = LRUCache[str, int](max_size=10)\n",
            "            >>> cache.put(\"a\", 1)\n",
            "            >>> cache.get(\"a\")\n",
            "            1\n",
            "            >>> cache.get(\"b\")  # miss\n",
            "            >>> s = cache.stats()\n",
            "            >>> s.hits\n",
            "            1\n",
            "            >>> s.misses\n",
            "            1\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            return CacheStats(\n",
            "                hits=self._hits,\n",
            "                misses=self._misses,\n",
            "                evictions=self._evictions,\n",
            "                expirations=self._expirations,\n",
            "                current_size=len(self._cache),\n",
            "                max_size=self._max_size,\n",
            "            )\n",
            "\n",
            "    def keys(self) -> list[K]:\n",
            "        \"\"\"Return a list of all non-expired keys in the cache, from LRU to MRU.\n",
            "\n",
            "        Expired entries are purged as a side effect.\n",
            "\n",
            "        Returns:\n",
            "            A list of keys ordered from least recently used to most recently used.\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            self._purge_expired()\n",
            "            return list(self._cache.keys())\n",
            "\n",
            "    def purge_expired(self) -> int:\n",
            "        \"\"\"Remove all expired entries from the cache.\n",
            "\n",
            "        Returns:\n",
            "            The number of entries that were removed.\n",
            "\n",
            "        Example:\n",
            "            >>> cache = LRUCache[str, int](max_size=10, default_ttl=0.01)\n",
            "            >>> cache.put(\"a\", 1)\n",
            "            >>> import time; time.sleep(0.02)\n",
            "            >>> cache.purge_expired()\n",
            "            1\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            return self._purge_expired()\n",
            "\n",
            "    # -------------------------------------------------------------------------\n",
            "    # Private helpers\n",
            "    # -------------------------------------------------------------------------\n",
            "\n",
            "    def _remove_entry(self, key: K, *, expired: bool = False) -> None:\n",
            "        \"\"\"Remove a single entry and update statistics.\n",
            "\n",
            "        Args:\n",
            "            key: The key to remove. Must exist in the cache.\n",
            "            expired: If True, increment the expirations counter.\n",
            "        \"\"\"\n",
            "        del self._cache[key]\n",
            "        if expired:\n",
            "            self._expirations += 1\n",
            "\n",
            "    def _evict_lru(self) -> None:\n",
            "        \"\"\"Evict the least recently used entry (the first item in the OrderedDict).\"\"\"\n",
            "        if self._cache:\n",
            "            # popitem(last=False) removes the *first* (oldest / least recent) item\n",
            "            self._cache.popitem(last=False)\n",
            "            self._evictions += 1\n",
            "\n",
            "    def _purge_expired(self) -> int:\n",
            "        \"\"\"Internal purge of all expired entries. Caller must hold the lock.\n",
            "\n",
            "        Returns:\n",
            "            The number of expired entries removed.\n",
            "        \"\"\"\n",
            "        expired_keys = [\n",
            "            key for key, entry in self._cache.items() if entry.is_expired\n",
            "        ]\n",
            "        for key in expired_keys:\n",
            "            self._remove_entry(key, expired=True)\n",
            "        return len(expired_keys)\n",
            "\n",
            "\n",
            "# ---------------------------------------------------------------------------\n",
            "# Quick self-test / demo\n",
            "# ---------------------------------------------------------------------------\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    import concurrent.futures\n",
            "\n",
            "    print(\"=== LRU Cache Demo ===\\n\")\n",
            "\n",
            "    # Basic usage\n",
            "    cache: LRUCache[str, int] = LRUCache(max_size=3, default_ttl=2.0)\n",
            "    cache.put(\"a\", 1)\n",
            "    cache.put(\"b\", 2)\n",
            "    cache.put(\"c\", 3)\n",
            "    print(f\"Cache after inserting a, b, c: {cache.keys()}\")\n",
            "    print(f\"Get 'a': {cache.get('a')}\")  # moves 'a' to MRU\n",
            "\n",
            "    cache.put(\"d\", 4)  # should evict 'b' (LRU)\n",
            "    print(f\"After inserting 'd' (should evict 'b'): {cache.keys()}\")\n",
            "    print(f\"Get 'b': {cache.get('b')}\")  # None, evicted\n",
            "    print(f\"Stats: {cache.stats()}\\n\")\n",
            "\n",
            "    # TTL demo\n",
            "    print(\"--- TTL Demo ---\")\n",
            "    ttl_cache: LRUCache[str, str] = LRUCache(max_size=10, default_ttl=0.5)\n",
            "    ttl_cache.put(\"temp\", \"I will expire soon\")\n",
            "    print(f\"Get 'temp' immediately: {ttl_cache.get('temp')}\")\n",
            "    time.sleep(0.6)\n",
            "    print(f\"Get 'temp' after 0.6s: {ttl_cache.get('temp')}\")\n",
            "    print(f\"Stats: {ttl_cache.stats()}\\n\")\n",
            "\n",
            "    # Thread safety demo\n",
            "    \n"
          ]
        }
      ],
      "metadata": {
        "id": "example3-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17619450-f30c-4786-cf3a-42fa2dc0bb38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìä Example 4: Structured JSON Output\n",
        "\n",
        "Claude Opus 4.6 provides reliable structured output generation."
      ],
      "metadata": {
        "id": "example4-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# @title JSON Structured Output (with Markdown Stripping)\n",
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    # Remove markdown code blocks if present\n",
        "    text = re.sub(r'^```json\\s*|\\s*```$', '', text.strip(), flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"You are a data extraction specialist.\n",
        "    Always respond with valid JSON matching this schema:\n",
        "    {\n",
        "      \"company_name\": string,\n",
        "      \"industry\": string,\n",
        "      \"founded_year\": number,\n",
        "      \"key_products\": [string],\n",
        "      \"headquarters\": string,\n",
        "      \"notable_facts\": [string]\n",
        "    }\n",
        "    Do not include any text before or after the JSON object.\"\"\"},\n",
        "            {\"role\": \"user\", \"content\": \"Extract structured information about Anthropic (the AI company).\"}\n",
        "        ],\n",
        "        max_tokens=1024,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    raw_content = response.choices[0].message.content\n",
        "\n",
        "    if not raw_content:\n",
        "        print(\"‚òë·Ä∏ Error: The model returned an empty response.\")\n",
        "    else:\n",
        "        # Clean the content before parsing\n",
        "        cleaned_content = extract_json(raw_content)\n",
        "        result = json.loads(cleaned_content)\n",
        "        print(\"‚úÖ Successfully parsed JSON:\")\n",
        "        print(json.dumps(result, indent=2))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred: {type(e).__name__}: {e}\")\n",
        "    if 'raw_content' in locals():\n",
        "        print(f\"Raw Content received: {raw_content[:100]}...\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully parsed JSON:\n",
            "{\n",
            "  \"company_name\": \"Anthropic\",\n",
            "  \"industry\": \"Artificial Intelligence / AI Safety\",\n",
            "  \"founded_year\": 2021,\n",
            "  \"key_products\": [\n",
            "    \"Claude (AI assistant)\",\n",
            "    \"Claude API\",\n",
            "    \"Claude for Enterprise\",\n",
            "    \"Constitutional AI (research methodology)\"\n",
            "  ],\n",
            "  \"headquarters\": \"San Francisco, California, USA\",\n",
            "  \"notable_facts\": [\n",
            "    \"Founded by Dario Amodei and Daniela Amodei, former members of OpenAI\",\n",
            "    \"Focuses on AI safety and developing reliable, interpretable AI systems\",\n",
            "    \"Developed Constitutional AI (CAI), a method for training AI systems using a set of principles\",\n",
            "    \"Has received significant funding from investors including Google, Spark Capital, and Salesforce Ventures\",\n",
            "    \"The company emphasizes responsible AI development and reducing catastrophic risks from AI\",\n",
            "    \"Raised billions of dollars in funding, with major investments from Google and Amazon\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "metadata": {
        "id": "example4-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371e3d25-f97d-4eac-b209-f66bf8eaa8c0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üåä Example 5: Streaming Responses\n",
        "\n",
        "Stream long-form content generation for real-time feedback."
      ],
      "metadata": {
        "id": "example5-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# @title Streaming Demo\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "import time\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a concise technical overview of transformer architecture (3 paragraphs).\"}\n",
        "    ],\n",
        "    max_tokens=1024,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "full_response = \"\"\n",
        "print(\"üì° Streaming response:\\n\")\n",
        "\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        full_response += content\n",
        "        print(content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\n‚úÖ Stream complete!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° Streaming response:\n",
            "\n",
            "# Transformer Architecture: Technical Overview\n",
            "\n",
            "The transformer architecture, introduced by Vaswani et al. in the 2017 paper *\"Attention Is All You Need,\"* is a neural network design that relies entirely on attention mechanisms, dispensing with the recurrence and convolutions previously dominant in sequence modeling. At its core lies the **self-attention mechanism** (specifically, scaled dot-product attention), which computes relevance scores between all pairs of tokens in a sequence simultaneously. For each token, queries (Q), keys (K), and values (V) are derived via learned linear projections, and attention weights are calculated as **softmax(QK·µÄ/‚àöd‚Çñ)V**. This is extended through **multi-head attention**, where multiple attention operations run in parallel across different learned subspaces, allowing the model to capture diverse syntactic and semantic relationships.\n",
            "\n",
            "The standard transformer follows an **encoder-decoder** structure. The encoder consists of a stack of identical layers, each containing a multi-head self-attention sublayer followed by a position-wise feed-forward network (typically two linear transformations with a ReLU or GELU activation). The decoder mirrors this structure but adds a **cross-attention** sublayer that attends to encoder outputs, and its self-attention is masked to prevent attending to future positions during autoregressive generation. Each sublayer employs **residual connections** and **layer normalization** to stabilize training and enable gradient flow through deep stacks. Since the architecture contains no inherent notion of sequence order, **positional encodings**‚Äîeither sinusoidal or learned‚Äîare added to input embeddings to inject token position information.\n",
            "\n",
            "The transformer's key advantage is its **O(1) sequential operations** for relating any two positions in a sequence (compared to O(n) for RNNs), enabled by the global receptive field of self-attention, though this comes at O(n¬≤) memory and compute cost relative to sequence length. This parallelizability makes it highly efficient on modern GPU/TPU hardware and has enabled massive scaling. The architecture underpins virtually all modern large language models‚ÄîGPT-family models use decoder-only stacks, BERT uses encoder-only stacks, and T5 uses the full encoder-decoder‚Äîand has been successfully adapted beyond NLP to vision (ViT), audio, protein folding, and multimodal domains. Subsequent innovations like KV-caching, sparse attention, rotary positional embeddings (RoPE), and grouped-query attention have addressed scalability bottlenecks while preserving the fundamental design.\n",
            "\n",
            "‚úÖ Stream complete!\n"
          ]
        }
      ],
      "metadata": {
        "id": "example5-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72194acd-c26c-4855-9f76-e83215ffeeb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üßÆ Example 6: Complex Reasoning & Analysis\n",
        "\n",
        "Demonstrate Claude Opus 4.6's analytical capabilities on complex problems."
      ],
      "metadata": {
        "id": "example6-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# @title Complex Reasoning Task\n",
        "analysis_prompt = \"\"\"\n",
        "Analyze this scenario and provide a structured decision framework:\n",
        "\n",
        "A tech startup has $2M in runway and faces a choice:\n",
        "A) Hire 5 engineers and accelerate product development\n",
        "B) Spend on marketing to acquire customers first\n",
        "C) Split 50/50 between both\n",
        "\n",
        "Consider:\n",
        "- Current MRR: $15K (growing 10% monthly)\n",
        "- Product-market fit: Partial validation from 50 paying customers\n",
        "- Competition: 3 well-funded competitors launching similar products\n",
        "- Team: 4 engineers, 0 marketing\n",
        "\n",
        "Provide:\n",
        "1. Analysis of each option\n",
        "2. Key risks and mitigations\n",
        "3. Decision recommendation with reasoning\n",
        "4. 90-day execution plan\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a strategic advisor with expertise in startup growth. Provide thorough, actionable analysis.\"},\n",
        "        {\"role\": \"user\", \"content\": analysis_prompt}\n",
        "    ],\n",
        "    max_tokens=4096\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Strategic Decision Framework: $2M Runway Allocation\n",
            "\n",
            "## Context Assessment\n",
            "\n",
            "Before analyzing options, let me ground the numbers. At $15K MRR growing 10% monthly with ~$2M runway, you have roughly **12-18 months** depending on current burn rate (likely $80-120K/month with 4 engineers + founders). This is a **critical inflection window** ‚Äî you have enough capital to make one decisive bet, but not enough to recover from a major strategic error.\n",
            "\n",
            "The central tension: **partial product-market fit + competitive pressure**. These pull in opposite directions. Partial PMF says \"keep building and listening.\" Competition says \"move fast or get locked out.\"\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Analysis of Each Option\n",
            "\n",
            "### Option A: Hire 5 Engineers (Full Product Acceleration)\n",
            "\n",
            "**The thesis:** Win on product superiority before competitors catch up. A team of 9 engineers could ship 2-3x faster, building defensible technical moats.\n",
            "\n",
            "**Cost model:** 5 engineers at ~$150K avg fully loaded = $750K/year added burn. New burn rate ~$170-200K/month. Runway shrinks to **10-12 months**.\n",
            "\n",
            "**Strengths:**\n",
            "- Doubles engineering capacity, potentially more than doubles output with specialization\n",
            "- With only 50 customers, there's likely significant product gaps ‚Äî more hands can close them\n",
            "- Technical differentiation is a durable moat; marketing spend is not\n",
            "- You can iterate faster on what your 50 customers are telling you\n",
            "\n",
            "**Weaknesses:**\n",
            "- 50 customers isn't enough signal to know *what* to build confidently ‚Äî you risk building faster in the wrong direction\n",
            "- 0 marketing means 0 demand generation infrastructure; growth stays organic and slow\n",
            "- 5 hires at once creates massive onboarding/management load for a 4-person team (realistically 2-3 months before new hires are productive)\n",
            "- If competitors out-market you, a better product in a losing market position is worthless\n",
            "- Revenue stays at organic growth trajectory ‚Äî dangerous for next fundraise narrative\n",
            "\n",
            "**Verdict:** High conviction bet that only works if your 50 customers represent a clear, deep pattern you can build against. Risky given *partial* validation.\n",
            "\n",
            "---\n",
            "\n",
            "### Option B: Full Marketing Spend ($2M on Customer Acquisition)\n",
            "\n",
            "**The thesis:** Acquire customers fast, dominate market positioning, and use revenue growth to fundraise or sustain.\n",
            "\n",
            "**Cost model:** $2M on marketing/sales with current team. Assuming reasonable CAC of $500-2000 for B2B SaaS, you could acquire 1,000-4,000 customers over 12 months theoretically.\n",
            "\n",
            "**Strengths:**\n",
            "- Accelerates the PMF learning loop ‚Äî more customers = more signal\n",
            "- Revenue growth story strengthens fundraising position dramatically\n",
            "- Market positioning against 3 competitors locks in mindshare early\n",
            "- Customer acquisition data (CAC, LTV, channels) is essential for Series A\n",
            "\n",
            "**Weaknesses:**\n",
            "- **This is the highest-risk option.** With only partial PMF, you're pouring fuel on a product that may not retain customers\n",
            "- 4 engineers cannot handle the support load, feature requests, and technical debt from rapid scaling\n",
            "- High churn from an immature product destroys unit economics and makes fundraising *harder*\n",
            "- You have zero marketing team/infrastructure ‚Äî you'd need to hire a marketing leader + team, which takes 2-3 months minimum before spend is even effective\n",
            "- Spending without attribution infrastructure wastes money\n",
            "\n",
            "**Verdict:** Premature scaling. This is the #1 startup killer. With *partial* PMF, aggressive marketing spend is like advertising a restaurant where the kitchen isn't ready.\n",
            "\n",
            "---\n",
            "\n",
            "### Option C: 50/50 Split ($1M Each)\n",
            "\n",
            "**The thesis:** Balanced approach hedges risk. Build product while simultaneously generating demand.\n",
            "\n",
            "**Cost model:** ~2-3 new engineers ($375-450K/year) + $550K-$625K marketing/year. Burn rate ~$160-180K/month. Runway ~**11-13 months**.\n",
            "\n",
            "**Strengths:**\n",
            "- Develops both capabilities simultaneously\n",
            "- Feels \"safe\" and \"balanced\"\n",
            "\n",
            "**Weaknesses:**\n",
            "- **Achieves mediocrity on both fronts.** 2-3 engineers don't meaningfully change velocity. $500K in marketing without a team or infrastructure gets spent inefficiently\n",
            "- Splits management attention across two major hiring/buildout efforts simultaneously\n",
            "- \"Balanced\" is often code for \"we couldn't decide\" ‚Äî and in startups, indecision is a decision to lose\n",
            "- Neither investment reaches critical mass\n",
            "\n",
            "**Verdict:** The worst option disguised as the safest. Startups don't win by hedging ‚Äî they win by concentrated bets on their highest-leverage activity.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Key Risks and Mitigations\n",
            "\n",
            "| Risk | Severity | Mitigation |\n",
            "|------|----------|------------|\n",
            "| **Building the wrong features at scale** | Critical | Formalize customer discovery with all 50 customers before any new engineering starts |\n",
            "| **Competitive lockout on distribution** | High | Identify 1-2 organic/low-cost channels now; don't need $1M to start learning |\n",
            "| **Hiring wrong people under speed pressure** | High | Use contract-to-hire for first 60 days; hire from network first |\n",
            "| **Onboarding drag kills velocity** | Medium | Hire in 2 waves (not all at once); pair programming with existing team |\n",
            "| **Runway runs out before Series A** | Critical | Set a hard \"fundraise trigger\" at month 6 regardless; maintain 6-month reserve |\n",
            "| **Churn from immature product** | High | Define retention benchmarks before scaling acquisition |\n",
            "| **Founder bandwidth collapse** | Medium | Designate one founder as \"product/eng\" owner, one as \"growth/customers\" |\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Decision Recommendation\n",
            "\n",
            "### **Recommended: Modified Option A ‚Äî Engineering-Led with Strategic Growth Foundation**\n",
            "\n",
            "**Allocation: ~70% Engineering / 30% Growth Infrastructure**\n",
            "\n",
            "- **Hire 3 engineers + 1 senior growth/marketing hire + 1 customer success/sales hire**\n",
            "- Engineering spend: ~$450K/year (3 engineers)\n",
            "- Growth spend: ~$250K/year (1 growth lead + modest budget for experimentation)\n",
            "- Customer success: ~$120K/year\n",
            "- Remaining: reserve for tooling, contingency, and small-scale paid acquisition experiments\n",
            "\n",
            "**Total added burn: ~$70-80K/month ‚Üí total burn ~$150-170K/month ‚Üí runway ~12-14 months**\n",
            "\n",
            "### Core Reasoning\n",
            "\n",
            "**1. Partial PMF is the governing constraint.** Everything flows from this. You don't have full PMF ‚Äî you have 50 customers who are paying, but you don't yet know *exactly* why they stay, what they'd churn over, or whether your product serves a broad market or a niche. Hiring 3 strong engineers (not 5) lets you accelerate product development while keeping the team manageable. The goal isn't shipping faster ‚Äî it's **shipping the right things faster**.\n",
            "\n",
            "**2. Zero marketing infrastructure is a liability, but not a $1M problem right now.** You need one exceptional growth hire ‚Äî someone who can identify channels, set up attribution, run small experiments, and build the playbook. This person is worth 10x their salary because they'll tell you *where* to spend when you're ready to spend big. They don't need a $1M budget ‚Äî they need $5-10K/month in experiment budget and the authority to move fast.\n",
            "\n",
            "**3. Customer success is your secret weapon at this stage.** With 50 customers, every single one is a goldmine of insight. A dedicated CS person means: lower churn, deeper qualitative understanding, case studies for marketing, and upsell opportunities. This role pays for itself immediately and feeds both the product roadmap and the growth engine.\n",
            "\n",
            "**4. 3 engineers, not 5, because of absorption capacity.** A 4-person engineering team cannot productively absorb 5 new engineers simultaneously. Management overhead, code review bottlenecks, architectural decisions, and cultural integration all suffer. 3 new engineers (hired in 2 waves) gives you meaningful velocity improvement without organizational chaos.\n",
            "\n",
            "**5. This positions you optimally for Series A.** At month 9-10, you want to show: growing MRR (target $50-80K), improving retention metrics, a clear product roadmap with competitive differentiation, early channel identification with promising unit economics, and 150-300 customers with strong NPS. This plan delivers all of that. Pure Option A gives you product but no growth story. Pure Option B gives you a growth story built on sand.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. 90-Day Execution Plan\n",
            "\n",
            "### **Days 1-30: Foundation**\n",
            "\n",
            "**Engineering:**\n",
            "- [ ] Conduct structured exit interviews / deep dives with all 50 customers (founders + CS hire)\n",
            "- [ ] Identify the 3 features/improvements customers mention most, map against competitive gap\n",
            "- [ ] Hire Engineer #1 and Engineer #2 (prioritize from network; focus on senior full-stack or backend depending on product gaps)\n",
            "- [ ] Define clear product roadmap for next 6 months with milestones\n",
            "\n",
            "**Growth:**\n",
            "- [ ] Begin search for Head of Growth / senior growth marketer (this hire is critical ‚Äî take the full 30 days if needed)\n",
            "- [ ] Hire or promote someone into Customer Success (can be more junior; attitude > experience)\n",
            "- [ ] Set up basic analytics infrastructure: cohort tracking, churn analysis, NPS baseline\n",
            "- [ ] Audit current organic growth: Where are the 50 customers coming from? Double down on what works\n",
            "\n",
            "**Metrics to establish:**\n",
            "- Logo churn rate (monthly)\n",
            "- Net revenue retention\n",
            "- NPS / CSAT score\n",
            "- Feature adoption rates\n",
            "\n",
            "**Key decision gate:** By Day 30, you should have a clear picture of *why* customers buy, *why* they stay, and *why* some have churned or gone inactive. If the answer is muddy, slow down engineering hiring and invest more in discovery.\n",
            "\n",
            "---\n",
            "\n",
            "### **Days 31-60: Acceleration**\n",
            "\n",
            "**Engineering:**\n",
            "- [ ] Engineers #1 and #2 are past onboarding; assign to highest-priority feature track\n",
            "- [ ] Hire Engineer #3 (can be slightly more junior if paired with senior mentor)\n",
            "- [ ] Ship first major product improvement based on customer research\n",
            "- [ ] Establish 2-week sprint cadence with customer-facing demos\n",
            "\n",
            "**Growth:**\n",
            "- [ ] Head of Growth is onboarded and has completed channel audit\n",
            "- [ ] Launch 3-4 small acquisition experiments ($2-5K each): content/SEO, LinkedIn outbound, one paid channel, one partnership/integration play\n",
            "- [ ] CS person has established relationships with all 50 accounts; identifies 5-10 potential case studies/testimonials\n",
            "- [ ] Set up referral program or word-of-mouth program with existing customers\n",
            "\n",
            "**Metrics targets:**\n",
            "- MRR: $18-20K (organic growth continues + reduced churn from CS efforts)\n",
            "- New customers: 5-10 from experimental channels\n",
            "- Product: 1 major feature shipped\n",
            "\n",
            "---\n",
            "\n",
            "### **Days 61-90: Optimization & Fundraise Prep**\n",
            "\n",
            "**Engineering:**\n",
            "- [ ] Full team of 7 engineers operating at cadence\n",
            "- [ ] Ship second major feature/improvement\n",
            "- [ ] Begin building toward competitive differentiation feature (the thing competitors *can't* easily copy)\n",
            "- [ ] Technical debt sprint ‚Äî ensure foundation can support 10x customer growth\n",
            "\n",
            "**Growth:**\n",
            "- [ ] Kill underperforming experiments; double budget on winning channel(s)\n",
            "- [ ] First draft of \"growth model\" ‚Äî projected CAC, LTV, payback period by channel\n",
            "- [ ] 2-3 published case studies / testimonial videos\n",
            "- [ ] Begin content engine or outbound engine (whichever showed more promise)\n",
            "\n",
            "**Strategic:**\n",
            "- [ ] Compile fundraising narrative: \"We found PMF with [segment], here's our retention proof, here's our growth channel, here's our product moat, here's our plan for $X in Series A\"\n",
            "- [ ] Begin warm intros to Series A investors (don't formally raise yet ‚Äî relationship building)\n",
            "- [ ] Board/advisor check-in: Are we on track? Do we need to adjust?\n",
            "\n",
            "**Metrics targets:**\n",
            "- MRR: $22-28K\n",
            "- Customers: 70-90\n",
            "- Net revenue retention: >100% (expansion revenue from existing customers)\n",
            "- At least 1 identified scalable acquisition channel with acceptable unit economics\n",
            "- Clear product roadmap differentiated from competitors\n",
            "\n",
            "---\n",
            "\n",
            "## Decision Checkpoint at Day 90\n",
            "\n",
            "At the 90-day mark, you should have enough data to answer three critical questions:\n",
            "\n",
            "1. **Is PMF strengthening?** (Retention improving, NPS rising, expansion revenue emerging)\n",
            "2. **Is there a scalable channel?** (At least one channel with CAC < 1/3 of annual contract value)\n",
            "3. **Is the product differentiated?** (Customers choosing you over competitors for specific, articulable reasons)\n",
            "\n",
            "**If yes to all three:** Accelerate. Consider hiring additional marketing/sales resources. Begin formal Series A process. You're in a strong position.\n",
            "\n",
            "**If yes to #1 and #3 but not #2:** Stay the course. Product is strong but distribution needs more time. Extend runway if possible; consider bridge round.\n",
            "\n",
            "**If no to #1:** Stop. Reassess product direction fundamentally. Your 50 customers may be a false signal. Go back to deep customer discovery before spending another dollar on growth.\n",
            "\n",
            "---\n",
            "\n",
            "## Final Thought\n",
            "\n",
            "The competitive pressure from 3 well-funded rivals creates urgency, but **urgency is not a strategy**. The startups that win in competitive markets aren't the ones who spent the most or shipped the fastest ‚Äî they're the ones who understood their customers most deeply and built something those customers couldn't leave. Your $2M buys you 12-14 months to become that company. Spend it on understanding, then building, then growing ‚Äî in that order, compressed into parallel workstreams.\n"
          ]
        }
      ],
      "metadata": {
        "id": "example6-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6191da57-7357-4709-a00a-e7383828c4e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìà Summary & Token Usage\n",
        "\n",
        "Track token usage and costs for your API calls."
      ],
      "metadata": {
        "id": "summary-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# @title Check Token Usage (Last Response)\n",
        "if hasattr(response, 'usage') and response.usage:\n",
        "    usage = response.usage\n",
        "    print(\"üìä Token Usage (Last Request):\")\n",
        "    print(f\"   ‚Ä¢ Prompt tokens: {usage.prompt_tokens}\")\n",
        "    print(f\"   ‚Ä¢ Completion tokens: {usage.completion_tokens}\")\n",
        "    print(f\"   ‚Ä¢ Total tokens: {usage.total_tokens}\")\n",
        "    print(\"\\nüí° Claude Opus 4.6 Pricing (via OpenRouter):\")\n",
        "    print(\"   ‚Ä¢ Input: ~$15/1M tokens\")\n",
        "    print(\"   ‚Ä¢ Output: ~$75/1M tokens\")\n",
        "else:\n",
        "    print(\"Token usage not available in response\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Token Usage (Last Request):\n",
            "   ‚Ä¢ Prompt tokens: 191\n",
            "   ‚Ä¢ Completion tokens: 3261\n",
            "   ‚Ä¢ Total tokens: 3452\n",
            "\n",
            "üí° Claude Opus 4.6 Pricing (via OpenRouter):\n",
            "   ‚Ä¢ Input: ~$15/1M tokens\n",
            "   ‚Ä¢ Output: ~$75/1M tokens\n"
          ]
        }
      ],
      "metadata": {
        "id": "summary-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f384e138-9dc5-4194-a422-583ef529e28f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üéØ Key Takeaways\n",
        "\n",
        "**Claude Opus 4.6 excels at:**\n",
        "- ‚úÖ Complex, multi-step reasoning\n",
        "- ‚úÖ Production-quality code generation\n",
        "- ‚úÖ Advanced tool calling with parallel execution\n",
        "- ‚úÖ Structured output generation\n",
        "- ‚úÖ Long-context tasks (1M tokens)\n",
        "- ‚úÖ Agentic workflows with self-correction\n",
        "\n",
        "**Best use cases:**\n",
        "- üè¢ Enterprise knowledge work & analysis\n",
        "- üíª Software development & debugging\n",
        "- üìä Complex data analysis & reporting\n",
        "- ü§ñ Building autonomous AI agents\n",
        "\n",
        "---\n",
        "*Notebook by @BuildFastWithAI*"
      ],
      "metadata": {
        "id": "takeaways-cell"
      }
    }
  ]
}