{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-logo"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-tbvLOFetmNkYmev7Ie55TSe035_Pmdr?usp=sharing)\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn more about Our Bootcamp: https://www.buildfastwithai.com/genai-course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# ğŸŒ™ Kimi K2.5 Cookbook: Native Multimodal Agentic AI\n",
        "\n",
        "This cookbook provides comprehensive examples for using **Moonshot AI's Kimi K2.5** model via OpenRouter. Kimi K2.5 is an open-source, native multimodal agentic model built through continual pretraining on approximately **15 trillion mixed visual and text tokens**.\n",
        "\n",
        "## ğŸ”‘ Key Features\n",
        "\n",
        "- **Native Multimodality**: Pre-trained on vision-language tokens, excels in visual knowledge, cross-modal reasoning, and agentic tool use\n",
        "- **Coding with Vision**: Generates code from visual specifications (UI designs, video workflows) and orchestrates tools for visual data processing\n",
        "- **Agent Swarm**: Self-directed, coordinated swarm-like execution scheme for complex tasks\n",
        "- **Thinking Mode**: Extended reasoning capabilities with step-by-step thinking process\n",
        "- **1T Parameters (32B Active)**: Massive Mixture-of-Experts architecture with 384 experts\n",
        "\n",
        "## ğŸ“‹ What You'll Learn\n",
        "\n",
        "1. **Setup and Configuration** - Installing dependencies and API setup\n",
        "2. **Basic Text Generation** - Simple chat completions\n",
        "3. **Thinking Mode vs Instant Mode** - Extended reasoning capabilities\n",
        "4. **Image Understanding** - Visual analysis and reasoning\n",
        "5. **Code Generation from Visual Specs** - UI to code conversion\n",
        "6. **Complex Reasoning Tasks** - Multi-step problem solving\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## Step 1: Installation and Setup\n",
        "\n",
        "First, let's install the required packages and configure our environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai requests pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "jTwv9aRejXJq",
        "outputId": "e5cdc777-b1d8-4597-c826-1116c624c42c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting requests\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting pillow\n",
            "  Downloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: requests, pillow\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pillow-12.1.0 requests-2.32.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "1d5c53f7559340c9a3adac4d16b07cc8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imports",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5646e83-0c68-474e-a0c5-600fba7448fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"ğŸ“¦ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api-setup-header"
      },
      "source": [
        "## Step 2: API Key Configuration\n",
        "\n",
        "Set up your OpenRouter API key. Get your API key from: https://openrouter.ai/keys\n",
        "\n",
        "**âš ï¸ Never hardcode your API key in production code!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "client-init",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3cc355-46d9-40e6-f169-0343f90ec90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ OpenRouter client initialized successfully!\n",
            "ğŸ“Œ Using model: moonshotai/kimi-k2.5\n"
          ]
        }
      ],
      "source": [
        "# Initialize the OpenRouter client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get(\"OPENROUTER_API_KEY\"),\n",
        ")\n",
        "\n",
        "# Model identifier for Kimi K2.5\n",
        "MODEL = \"moonshotai/kimi-k2.5\"\n",
        "\n",
        "print(\"ğŸ¯ OpenRouter client initialized successfully!\")\n",
        "print(f\"ğŸ“Œ Using model: {MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example1-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Example 1: Basic Chat Completion\n",
        "\n",
        "Let's start with a simple text generation example to test the model's basic capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "example1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c6c22a-003d-4af0-888a-0eb661d985c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ EXAMPLE 1: Basic Chat Completion\n",
            "============================================================\n",
            "ğŸ¤– Model Response:\n",
            "----------------------------------------\n",
            "Mixture-of-Experts (MoE) enables efficient scaling by activating only a subset of model parameters for each input token rather than the entire network. A gating mechanism intelligently routes each token to the most relevant expert subnetworks (typically just 1-2 out of hundreds), allowing the model to massively expand its total parameter count while keeping computational costs proportional only to the active experts. This conditional computation achieves performance comparable to much larger dense models but with significantly lower training costs and faster inference, as only a fraction of the total weights are processed for any given token.\n",
            "----------------------------------------\n",
            "ğŸ“Š Tokens used: 496\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“ EXAMPLE 1: Basic Chat Completion\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful AI assistant specialized in explaining complex topics clearly and concisely.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain what makes Mixture-of-Experts (MoE) architecture efficient for large language models in 3-4 sentences.\"\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=500,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "print(\"ğŸ¤– Model Response:\")\n",
        "print(\"-\" * 40)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"-\" * 40)\n",
        "print(f\"ğŸ“Š Tokens used: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example2-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Example 2: Thinking Mode vs Instant Mode\n",
        "\n",
        "Kimi K2.5 supports two inference modes:\n",
        "- **Thinking Mode**: Extended reasoning with step-by-step thinking (temperature 1.0 recommended)\n",
        "- **Instant Mode**: Quick responses without explicit reasoning traces (temperature 0.6 recommended)\n",
        "\n",
        "Let's compare both modes on a complex reasoning task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "example2-thinking",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2c774d-afec-47d7-89b9-77b1146af9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  EXAMPLE 2: Thinking Mode vs Instant Mode\n",
            "============================================================\n",
            "\n",
            "ğŸ” THINKING MODE (with reasoning):\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ§  EXAMPLE 2: Thinking Mode vs Instant Mode\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "reasoning_problem = \"\"\"A farmer has a fox, a chicken, and a bag of grain. He needs to cross a river in a boat that can only carry him and one item at a time. If left alone together:\n",
        "- The fox will eat the chicken\n",
        "- The chicken will eat the grain\n",
        "\n",
        "How can the farmer get all three across safely? Explain your reasoning.\"\"\"\n",
        "\n",
        "# Thinking Mode (with reasoning)\n",
        "print(\"\\nğŸ” THINKING MODE (with reasoning):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response_thinking = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": reasoning_problem}\n",
        "    ],\n",
        "    max_tokens=1000,\n",
        "    temperature=1.0,\n",
        "    extra_body={\n",
        "        \"reasoning\": {\n",
        "            \"effort\": \"high\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response_thinking.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "example2-instant",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64a3f81-931c-49be-98d7-bb6fba538df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš¡ INSTANT MODE (quick response):\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Instant Mode (quick response)\n",
        "print(\"\\nâš¡ INSTANT MODE (quick response):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response_instant = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": reasoning_problem}\n",
        "    ],\n",
        "    max_tokens=500,\n",
        "    temperature=0.6,\n",
        ")\n",
        "\n",
        "print(response_instant.choices[0].message.content)\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example5-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Example 3: Complex Multi-Step Problem Solving\n",
        "\n",
        "Kimi K2.5's agent swarm architecture allows it to decompose complex tasks into parallel sub-tasks. Let's test its ability to solve multi-step problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "example5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a7fa8b-e4d6-4111-d69f-2cfe55fcf1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§© EXAMPLE 5: Complex Multi-Step Problem Solving\n",
            "============================================================\n",
            "ğŸ—ï¸ Architecture Design:\n",
            "----------------------------------------\n",
            "Here is a comprehensive architecture design for a scalable real-time chat platform:\n",
            "\n",
            "## 1. System Architecture (Text Description)\n",
            "\n",
            "```\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚                        CLIENT LAYER                              â”‚\n",
            "â”‚  (iOS/Android Apps, Web App, Desktop - React Native/Electron)   â”‚\n",
            "â”‚  [E2E Encryption: Signal Protocol Implementation]               â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "               â”‚ HTTPS/WSS                      â”‚ Direct Upload\n",
            "               â–¼                                â–¼\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚     CDN / Edge Layer     â”‚          â”‚   Object Storage     â”‚\n",
            "â”‚  (CloudFlare/AWS CF)     â”‚          â”‚  (S3/MinIO + CDN)    â”‚\n",
            "â”‚  - Static assets         â”‚          â”‚  - Presigned URLs    â”‚\n",
            "â”‚  - TLS termination       â”‚          â”‚  - Virus scanning    â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "           â”‚\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚              LOAD BALANCER LAYER (L4/L7)                    â”‚\n",
            "â”‚         (HAProxy/AWS ALB - Sticky Sessions for WS)          â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "           â”‚\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚                   API GATEWAY CLUSTER                       â”‚\n",
            "â”‚  - Authentication (JWT/OAuth2)                              â”‚\n",
            "â”‚  - Rate Limiting (Token Bucket)                             â”‚\n",
            "â”‚  - WebSocket upgrade routing                                â”‚\n",
            "â”‚  - Protocol Buffer validation                               â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "           â”‚\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚              CHAT SERVER CLUSTER (Stateful)                 â”‚\n",
            "â”‚  (Go/Elixir - 10K-50K conns/server)                         â”‚\n",
            "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
            "â”‚  â”‚  WS Handler â”‚  â”‚  WS Handler â”‚  â”‚  WS Handler â”‚         â”‚\n",
            "â”‚  â”‚ Connection  â”‚  â”‚ Connection  â”‚  â”‚ Connection  â”‚         â”‚\n",
            "â”‚  â”‚    Pool     â”‚  â”‚    Pool     â”‚  â”‚    Pool     â”‚         â”‚\n",
            "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
            "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
            "â”‚                           â”‚                                 â”‚\n",
            "â”‚              Consistent Hashing Ring                        â”‚\n",
            "â”‚         (UserID â†’ Server mapping via etcd/Consul)           â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "           â”‚\n",
            "    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "    â–¼             â–¼              â–¼              â–¼\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ REDIS   â”‚ â”‚  KAFKA   â”‚ â”‚  CASSANDRA  â”‚ â”‚  POSTGRESQL  â”‚\n",
            "â”‚ CLUSTER â”‚ â”‚  CLUSTER â”‚ â”‚   CLUSTER   â”‚ â”‚    CLUSTER   â”‚\n",
            "â”‚         â”‚ â”‚          â”‚ â”‚             â”‚ â”‚              â”‚\n",
            "â”‚Presence â”‚ â”‚ Message  â”‚ â”‚  Messages   â”‚ â”‚  Users/Groupsâ”‚\n",
            "â”‚Sessions â”‚ â”‚  Bus     â”‚ â”‚  (Time-seriesâ”‚ â”‚  Metadata    â”‚\n",
            "â”‚Typing   â”‚ â”‚          â”‚ â”‚   by convo) â”‚ â”‚  (Relational)â”‚\n",
            "â”‚Hot Cacheâ”‚ â”‚          â”‚ â”‚             â”‚ â”‚              â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "                 â”‚              â”‚\n",
            "                 â–¼              â–¼\n",
            "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "        â”‚Push Notif.   â”‚ â”‚ Elasticsearchâ”‚\n",
            "        â”‚Service (FCM/ â”‚ â”‚   (Search)   â”‚\n",
            "        â”‚APNs)         â”‚ â”‚              â”‚\n",
            "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "```\n",
            "\n",
            "## 2. Technology Stack Recommendations\n",
            "\n",
            "### Core Infrastructure\n",
            "- **Language**: **Go** (high concurrency, low latency) or **Elixir/OTP** (WhatsApp-style fault tolerance)\n",
            "- **Protocol**: WebSocket (primary) with HTTP/2 Server-Sent Events fallback\n",
            "- **Serialization**: Protocol Buffers (25-50% smaller than JSON, faster parsing)\n",
            "\n",
            "### Data Layer\n",
            "- **Hot Storage**: Redis Cluster (6+ nodes, 3 masters/3 replicas)\n",
            "  - Presence status, active sessions, typing indicators\n",
            "  - TTL-based expiration for transient data\n",
            "- **Message Persistence**: Apache Cassandra or ScyllaDB\n",
            "  - Time-series partitioning\n",
            "----------------------------------------\n",
            "ğŸ“Š Tokens used: 2135\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ§© EXAMPLE 5: Complex Multi-Step Problem Solving\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "complex_task = \"\"\"You are a senior software architect. Design a scalable real-time chat application with the following requirements:\n",
        "\n",
        "1. Support 100,000+ concurrent users\n",
        "2. Real-time message delivery (<100ms latency)\n",
        "3. Message persistence and history\n",
        "4. End-to-end encryption\n",
        "5. File sharing capabilities\n",
        "6. Presence indicators (online/offline/typing)\n",
        "\n",
        "Provide:\n",
        "- System architecture diagram (describe in text)\n",
        "- Technology stack recommendations\n",
        "- Key design patterns to use\n",
        "- Potential bottlenecks and solutions\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert software architect with deep knowledge of distributed systems, real-time communication, and scalable architectures.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": complex_task\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=2000,\n",
        "    temperature=0.7,\n",
        "    extra_body={\n",
        "        \"reasoning\": {\n",
        "            \"effort\": \"high\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"ğŸ—ï¸ Architecture Design:\")\n",
        "print(\"-\" * 40)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"-\" * 40)\n",
        "print(f\"ğŸ“Š Tokens used: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example6-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Example 4: Streaming Responses for Real-Time Applications\n",
        "\n",
        "For better user experience in production applications, streaming allows you to display responses as they're generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "example6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4fb6eda-fdaf-4819-ca20-7efa180ff883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒŠ EXAMPLE 6: Streaming Responses\n",
            "============================================================\n",
            "ğŸ”„ Starting streaming response...\n",
            "ğŸ“ Response (streaming):\n",
            "----------------------------------------\n",
            "```python\n",
            "from typing import TypeVar, Generic, Optional, Iterator\n",
            "\n",
            "T = TypeVar('T')\n",
            "\n",
            "\n",
            "class Node(Generic[T]):\n",
            "    \"\"\"A node in the binary search tree.\"\"\"\n",
            "    \n",
            "    def __init__(self, value: T) -> None:\n",
            "        \"\"\"\n",
            "        Initialize a node with a value.\n",
            "        \n",
            "        Args:\n",
            "            value: The value to store in this node.\n",
            "        \"\"\"\n",
            "        self.value: T = value\n",
            "        self.left: Optional[Node[T]] = None\n",
            "        self.right: Optional[Node[T]] = None\n",
            "\n",
            "\n",
            "class BinarySearchTree(Generic[T]):\n",
            "    \"\"\"\n",
            "    A Binary Search Tree (BST) implementation.\n",
            "    \n",
            "    BST property: For any given node, all values in the left subtree\n",
            "    are less than the node's value, and all values in the right subtree\n",
            "    are greater than the node's value.\n",
            "    \n",
            "    Type parameter T should support comparison operators (<, >, ==).\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self) -> None:\n",
            "        \"\"\"Initialize an empty binary search tree.\"\"\"\n",
            "        self.root: Optional[Node[T]] = None\n",
            "        self._size: int = 0\n",
            "    \n",
            "    def insert(self, value: T) -> bool:\n",
            "        \"\"\"\n",
            "        Insert a value into the tree.\n",
            "        \n",
            "        Args:\n",
            "            value: The value to insert.\n",
            "            \n",
            "        Returns:\n",
            "            True if insertion was successful, False if value already exists.\n",
            "            \n",
            "        Time Complexity: O(h) where h is the height of the tree.\n",
            "        \"\"\"\n",
            "        if self.root is None:\n",
            "            self.root = Node(value)\n",
            "            self._size += 1\n",
            "            return True\n",
            "        \n",
            "        return self._insert_recursive(self.root, value)\n",
            "    \n",
            "    def _insert_recursive(self, node: Node[T], value\n",
            "----------------------------------------\n",
            "ğŸ“Š Total characters streamed: 1577\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸŒŠ EXAMPLE 6: Streaming Responses\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"ğŸ”„ Starting streaming response...\")\n",
        "print(\"ğŸ“ Response (streaming):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a Python function that implements a binary search tree with insert, search, and delete operations. Include docstrings and type hints.\"\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=1500,\n",
        "    temperature=0.5,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "full_response = \"\"\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        print(content, end=\"\", flush=True)\n",
        "        full_response += content\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(f\"ğŸ“Š Total characters streamed: {len(full_response)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper-header"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ› ï¸ Helper Function for Easy Usage\n",
        "\n",
        "Here's a reusable helper function to simplify interactions with Kimi K2.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "helper-function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ff7fc5-8c46-42c2-b6fc-95ce1f7ed41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Helper function defined!\n"
          ]
        }
      ],
      "source": [
        "def ask_kimi(\n",
        "    prompt: str,\n",
        "    system_prompt: str = \"You are a helpful AI assistant.\",\n",
        "    image_url: str = None,\n",
        "    thinking_mode: bool = False,\n",
        "    max_tokens: int = 1000,\n",
        "    stream: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Helper function to interact with Kimi K2.5 via OpenRouter.\n",
        "\n",
        "    Args:\n",
        "        prompt: The user's question or instruction\n",
        "        system_prompt: System message to set the AI's behavior\n",
        "        image_url: Optional image URL for multimodal queries\n",
        "        thinking_mode: Enable extended reasoning (default: False)\n",
        "        max_tokens: Maximum tokens in response (default: 1000)\n",
        "        stream: Enable streaming response (default: False)\n",
        "\n",
        "    Returns:\n",
        "        The model's response as a string\n",
        "    \"\"\"\n",
        "    # Build message content\n",
        "    if image_url:\n",
        "        content = [\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
        "        ]\n",
        "    else:\n",
        "        content = prompt\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": content}\n",
        "    ]\n",
        "\n",
        "    # Set temperature based on mode\n",
        "    temperature = 1.0 if thinking_mode else 0.6\n",
        "\n",
        "    # Build extra body for thinking mode\n",
        "    extra_body = {}\n",
        "    if thinking_mode:\n",
        "        extra_body[\"reasoning\"] = {\"effort\": \"high\"}\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        stream=stream,\n",
        "        extra_body=extra_body if extra_body else None\n",
        "    )\n",
        "\n",
        "    if stream:\n",
        "        full_response = \"\"\n",
        "        for chunk in response:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                content = chunk.choices[0].delta.content\n",
        "                print(content, end=\"\", flush=True)\n",
        "                full_response += content\n",
        "        print()\n",
        "        return full_response\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "print(\"âœ… Helper function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper-demo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebc04fc-f3c4-4aa0-f308-56ac35280267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Testing helper function:\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Quick demo of the helper function\n",
        "print(\"ğŸ¯ Testing helper function:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "result = ask_kimi(\n",
        "    prompt=\"What are the top 3 benefits of using Mixture-of-Experts architecture?\",\n",
        "    thinking_mode=False\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ“š Summary\n",
        "\n",
        "In this cookbook, we explored **Kimi K2.5**'s powerful capabilities:\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| **Basic Chat** | Standard text generation with customizable parameters |\n",
        "| **Thinking Mode** | Extended reasoning for complex problems |\n",
        "| **Image Understanding** | Native multimodal visual analysis |\n",
        "| **UI to Code** | Generate code from visual specifications |\n",
        "| **Complex Problem Solving** | Multi-step reasoning and architecture design |\n",
        "| **Streaming** | Real-time response generation |\n",
        "\n",
        "## ğŸ”— Resources\n",
        "\n",
        "- **OpenRouter**: https://openrouter.ai/moonshotai/kimi-k2.5\n",
        "- **Hugging Face**: https://huggingface.co/moonshotai/Kimi-K2.5\n",
        "- **Moonshot AI**: https://www.moonshot.ai/\n",
        "- **Model Documentation**: https://platform.moonshot.ai/docs\n",
        "\n",
        "## ğŸ’¡ Tips for Best Results\n",
        "\n",
        "- Use **temperature 1.0** for Thinking Mode\n",
        "- Use **temperature 0.6** for Instant Mode\n",
        "- Set **top_p to 0.95** for optimal sampling\n",
        "- Kimi K2.5 supports up to **256K tokens** context length\n",
        "- For coding tasks, provide clear specifications and examples\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Building! ğŸš€**\n",
        "\n",
        "*For more tutorials and bootcamp information, visit [Build Fast with AI](https://www.buildfastwithai.com/genai-course)*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}