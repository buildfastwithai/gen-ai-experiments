{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/169ZBo3jo0nCQ3duAFzxq0sMY3eQVQh6z?usp=sharing)\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "\n",
        "**What You'll Learn:**\n",
        "- Cutting-edge Generative AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship\n",
        "- No prior coding experience required\n",
        "- Access to an innovation-driven community\n",
        "\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "\n",
        "üëâ [Start Your Journey](https://www.buildfastwithai.com/genai-course)"
      ],
      "metadata": {
        "id": "header-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ GLM-5 Testing Notebook\n",
        "\n",
        "**Z.ai's Flagship Open-Source Foundation Model for Complex Systems & Agent Workflows**\n",
        "\n",
        "This notebook explores the powerful capabilities of **GLM-5**, Z.ai's most capable model engineered for complex systems design and long-horizon agent workflows.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Model Specifications\n",
        "\n",
        "| Feature | Specification |\n",
        "|---------|---------------|\n",
        "| **Provider** | Z.ai (via OpenRouter) |\n",
        "| **Model ID** | `z-ai/glm-5` |\n",
        "| **Architecture** | Open-source foundation model |\n",
        "| **Tool Calling** | ‚úÖ Supported |\n",
        "| **Reasoning** | ‚úÖ Deep backend reasoning |\n",
        "| **Streaming** | ‚úÖ Supported |\n",
        "| **JSON Mode** | ‚úÖ Structured output |\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Key Capabilities\n",
        "\n",
        "1. **Complex Systems Design** - Full-system construction beyond simple code generation\n",
        "2. **Long-Horizon Agent Workflows** - Agentic planning and autonomous execution\n",
        "3. **Deep Backend Reasoning** - Advanced step-by-step reasoning with self-correction\n",
        "4. **Production-Grade Code** - Large-scale programming tasks rivaling closed-source models\n",
        "5. **Iterative Self-Correction** - Identifies and fixes issues autonomously\n",
        "6. **Open Source** - Model weights available on [HuggingFace](https://huggingface.co/zai-org/GLM-5)"
      ],
      "metadata": {
        "id": "intro-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üì¶ Setup & Installation"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# @title Install Dependencies\n",
        "!pip install -q openai"
      ],
      "outputs": [],
      "metadata": {
        "id": "install-cell"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# @title Configure API Client\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# Setup OpenRouter client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "MODEL = \"z-ai/glm-5\"\n",
        "\n",
        "print(f\"‚úÖ Client configured for: {MODEL}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Client configured for: z-ai/glm-5\n"
          ]
        }
      ],
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513a1e51-d024-4488-fbb2-7c72c4ca22f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üß™ Example 1: Basic Chat & Reasoning\n",
        "\n",
        "Test GLM-5's fundamental chat capabilities and its deep reasoning ability."
      ],
      "metadata": {
        "id": "example1-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# @title Basic Chat Completion\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are GLM-5, Z.ai's flagship open-source foundation model. Be helpful, precise, and thorough.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What are the key architectural innovations that make modern large language models effective? Explain concisely.\"}\n",
        "    ],\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "example1-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e74e2f-fbe7-437f-b717-67e962745f20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üõ†Ô∏è Example 2: Tool Calling\n",
        "\n",
        "GLM-5 supports function/tool calling for building agentic workflows."
      ],
      "metadata": {
        "id": "example2-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# @title Define Tools\n",
        "import json\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get current weather for a location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\"type\": \"string\", \"description\": \"City name\"},\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"calculate\",\n",
        "            \"description\": \"Perform mathematical calculations\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"expression\": {\"type\": \"string\", \"description\": \"Math expression to evaluate\"}\n",
        "                },\n",
        "                \"required\": [\"expression\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_knowledge\",\n",
        "            \"description\": \"Search a knowledge base for information\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"}\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Simulate tool execution\n",
        "def execute_tool(name, args):\n",
        "    if name == \"get_weather\":\n",
        "        return f\"Weather in {args.get('location', 'Unknown')}: 22¬∞C, Partly Cloudy\"\n",
        "    elif name == \"calculate\":\n",
        "        try:\n",
        "            result = eval(args.get('expression', '0'))\n",
        "            return f\"Result: {result}\"\n",
        "        except:\n",
        "            return \"Error in calculation\"\n",
        "    elif name == \"search_knowledge\":\n",
        "        return f\"Found information about: {args.get('query', 'topic')}\"\n",
        "    return \"Unknown tool\"\n",
        "\n",
        "print(\"‚úÖ Tools defined: get_weather, calculate, search_knowledge\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tools defined: get_weather, calculate, search_knowledge\n"
          ]
        }
      ],
      "metadata": {
        "id": "example2-tools",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b2e0fe-b4ca-470d-b338-4161dd61c09f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# @title Tool Calling Demo\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather in Beijing and also calculate 18% tip on $120.00?\"}\n",
        "    ],\n",
        "    tools=tools,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "# Check for tool calls\n",
        "message = response.choices[0].message\n",
        "print(\"üì§ Model Response:\")\n",
        "\n",
        "if message.tool_calls:\n",
        "    print(f\"\\nüîß Tool Calls Detected: {len(message.tool_calls)}\")\n",
        "    for tc in message.tool_calls:\n",
        "        print(f\"   ‚û§ {tc.function.name}: {tc.function.arguments}\")\n",
        "\n",
        "        # Execute and show result\n",
        "        args = json.loads(tc.function.arguments)\n",
        "        result = execute_tool(tc.function.name, args)\n",
        "        print(f\"   üìã Result: {result}\")\n",
        "else:\n",
        "    print(message.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Model Response:\n",
            "\n",
            "üîß Tool Calls Detected: 2\n",
            "   ‚û§ get_weather: {\"location\":\"Beijing\"}\n",
            "   üìã Result: Weather in Beijing: 22¬∞C, Partly Cloudy\n",
            "   ‚û§ calculate: {\"expression\":\"120 * 0.18\"}\n",
            "   üìã Result: Result: 21.599999999999998\n"
          ]
        }
      ],
      "metadata": {
        "id": "example2-demo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622aa7bd-e1f3-4095-ff76-3351877d45c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üíª Example 3: Code Generation\n",
        "\n",
        "GLM-5 is built for production-grade programming ‚Äî test its ability to generate complex, well-documented code."
      ],
      "metadata": {
        "id": "example3-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# @title Complex Code Generation\n",
        "coding_prompt = \"\"\"\n",
        "Create a Python class for a Rate Limiter with the following features:\n",
        "1. Token bucket algorithm implementation\n",
        "2. Configurable rate (tokens per second) and burst size\n",
        "3. Thread-safe operations\n",
        "4. Methods: acquire, try_acquire, wait, get_stats\n",
        "5. Include type hints, docstrings, and usage examples\n",
        "\n",
        "Provide a complete, production-ready implementation.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert Python developer. Write clean, production-quality code with full documentation.\"},\n",
        "        {\"role\": \"user\", \"content\": coding_prompt}\n",
        "    ],\n",
        "    max_tokens=4096\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a complete, production-ready implementation of a `RateLimiter` class using the Token Bucket algorithm.\n",
            "\n",
            "```python\n",
            "import time\n",
            "import threading\n",
            "from typing import Optional, Dict, Union\n",
            "\n",
            "\n",
            "class RateLimiter:\n",
            "    \"\"\"\n",
            "    A thread-safe implementation of the Token Bucket algorithm for rate limiting.\n",
            "\n",
            "    This class controls the rate at which actions are performed by maintaining a\n",
            "    \"bucket\" of tokens. Tokens are added at a fixed rate up to a maximum capacity\n",
            "    (burst size). Actions consume tokens; if the bucket is empty, the action must\n",
            "    wait or fail.\n",
            "\n",
            "    Attributes:\n",
            "        rate (float): The rate at which tokens are added to the bucket per second.\n",
            "        burst_size (int): The maximum number of tokens the bucket can hold.\n",
            "    \n",
            "    Example:\n",
            "        >>> limiter = RateLimiter(rate=5, burst_size=10)\n",
            "        >>> if limiter.try_acquire():\n",
            "        ...     print(\"Action permitted\")\n",
            "        ... else:\n",
            "        ...     print(\"Rate limit exceeded\")\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, rate: float, burst_size: int) -> None:\n",
            "        \"\"\"\n",
            "        Initialize the RateLimiter.\n",
            "\n",
            "        Args:\n",
            "            rate (float): The number of tokens added per second. Must be positive.\n",
            "            burst_size (int): The maximum capacity of the token bucket. Must be positive.\n",
            "\n",
            "        Raises:\n",
            "            ValueError: If rate or burst_size are not positive.\n",
            "        \"\"\"\n",
            "        if rate <= 0:\n",
            "            raise ValueError(\"Rate must be a positive value.\")\n",
            "        if burst_size <= 0:\n",
            "            raise ValueError(\"Burst size must be a positive integer.\")\n",
            "\n",
            "        self._rate = float(rate)\n",
            "        self._burst_size = int(burst_size)\n",
            "        self._tokens = float(burst_size)\n",
            "        self._last_update = time.monotonic()\n",
            "        self._lock = threading.Lock()\n",
            "\n",
            "    def _refill(self) -> None:\n",
            "        \"\"\"\n",
            "        Refill tokens based on elapsed time since the last update.\n",
            "        \n",
            "        This method is internal and assumes the lock is held by the caller.\n",
            "        \"\"\"\n",
            "        now = time.monotonic()\n",
            "        elapsed = now - self._last_update\n",
            "        \n",
            "        if elapsed > 0:\n",
            "            # Calculate new tokens, ensuring we don't exceed burst size\n",
            "            new_tokens = elapsed * self._rate\n",
            "            self._tokens = min(self._burst_size, self._tokens + new_tokens)\n",
            "            self._last_update = now\n",
            "\n",
            "    def try_acquire(self, tokens: int = 1) -> bool:\n",
            "        \"\"\"\n",
            "        Attempt to acquire tokens without blocking.\n",
            "\n",
            "        Args:\n",
            "            tokens (int): The number of tokens to consume. Defaults to 1.\n",
            "\n",
            "        Returns:\n",
            "            bool: True if tokens were successfully acquired, False otherwise.\n",
            "        \n",
            "        Raises:\n",
            "            ValueError: If requested tokens exceed burst size.\n",
            "        \"\"\"\n",
            "        if tokens > self._burst_size:\n",
            "            raise ValueError(\n",
            "                f\"Requested tokens ({tokens}) exceed burst size ({self._burst_size}).\"\n",
            "            )\n",
            "\n",
            "        with self._lock:\n",
            "            self._refill()\n",
            "            if self._tokens >= tokens:\n",
            "                self._tokens -= tokens\n",
            "                return True\n",
            "            return False\n",
            "\n",
            "    def acquire(self, tokens: int = 1, timeout: Optional[float] = None) -> bool:\n",
            "        \"\"\"\n",
            "        Acquire tokens, blocking until available or timeout is reached.\n",
            "\n",
            "        Args:\n",
            "            tokens (int): The number of tokens to consume. Defaults to 1.\n",
            "            timeout (Optional[float]): Maximum time to wait in seconds. \n",
            "                                       If None (default), waits indefinitely.\n",
            "\n",
            "        Returns:\n",
            "            bool: True if tokens were acquired, False if the timeout expired.\n",
            "\n",
            "        Raises:\n",
            "            ValueError: If requested tokens exceed burst size.\n",
            "        \"\"\"\n",
            "        if tokens > self._burst_size:\n",
            "            raise ValueError(\n",
            "                f\"Requested tokens ({tokens}) exceed burst size ({self._burst_size}).\"\n",
            "            )\n",
            "\n",
            "        deadline = None\n",
            "        if timeout is not None:\n",
            "            deadline = time.monotonic() + timeout\n",
            "\n",
            "        while True:\n",
            "            with self._lock:\n",
            "                self._refill()\n",
            "                \n",
            "                # Check if we have enough tokens now\n",
            "                if self._tokens >= tokens:\n",
            "                    self._tokens -= tokens\n",
            "                    return True\n",
            "                \n",
            "                # Calculate time needed to generate the required tokens\n",
            "                deficit = tokens - self._tokens\n",
            "                wait_time = deficit / self._rate\n",
            "\n",
            "            # Handle timeout logic outside the lock to allow other threads\n",
            "            if deadline is not None:\n",
            "                current_time = time.monotonic()\n",
            "                if current_time + wait_time > deadline:\n",
            "                    # Not enough time left to acquire tokens\n",
            "                    return False\n",
            "                # Adjust wait_time to respect the deadline\n",
            "                wait_time = min(wait_time, deadline - current_time)\n",
            "\n",
            "            # Sleep for the calculated duration\n",
            "            # We release the lock during sleep to not block other threads\n",
            "            time.sleep(wait_time)\n",
            "\n",
            "    def wait(self, tokens: int = 1) -> None:\n",
            "        \"\"\"\n",
            "        Convenience method to acquire tokens, blocking indefinitely.\n",
            "\n",
            "        Args:\n",
            "            tokens (int): The number of tokens to consume. Defaults to 1.\n",
            "        \n",
            "        Raises:\n",
            "            ValueError: If requested tokens exceed burst size.\n",
            "        \"\"\"\n",
            "        self.acquire(tokens=tokens, timeout=None)\n",
            "\n",
            "    def get_stats(self) -> Dict[str, Union[float, int]]:\n",
            "        \"\"\"\n",
            "        Get current statistics of the rate limiter.\n",
            "\n",
            "        Returns:\n",
            "            Dict[str, Union[float, int]]: A dictionary containing:\n",
            "                - 'rate': Configured tokens per second.\n",
            "                - 'burst_size': Maximum capacity.\n",
            "                - 'current_tokens': Current available tokens (approximate).\n",
            "                - 'last_refill_time': Timestamp of the last refill.\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            self._refill()  # Update tokens before reporting\n",
            "            return {\n",
            "                \"rate\": self._rate,\n",
            "                \"burst_size\": self._burst_size,\n",
            "                \"current_tokens\": self._tokens,\n",
            "                \"last_refill_time\": self._last_update,\n",
            "            }\n",
            "\n",
            "    @property\n",
            "    def rate(self) -> float:\n",
            "        \"\"\"float: The configured rate (tokens per second).\"\"\"\n",
            "        return self._rate\n",
            "\n",
            "    @property\n",
            "    def burst_size(self) -> int:\n",
            "        \"\"\"int: The configured burst size (max capacity).\"\"\"\n",
            "        return self._burst_size\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    # Usage Example\n",
            "    \n",
            "    print(\"--- Rate Limiter Demo ---\")\n",
            "    \n",
            "    # Initialize: 2 tokens per second, max burst of 5\n",
            "    limiter = RateLimiter(rate=2, burst_size=5)\n",
            "    \n",
            "    print(f\"Initial Stats: {limiter.get_stats()}\")\n",
            "    \n",
            "    # 1. Try acquire (burst handling)\n",
            "    print(\"\\nAttempting to consume 5 tokens (burst)...\")\n",
            "    if limiter.try_acquire(5):\n",
            "        print(\"Success! Burst consumed.\")\n",
            "    else:\n",
            "        print(\"Failed.\")\n",
            "        \n",
            "    print(f\"Stats after burst: {limiter.get_stats()}\")\n",
            "    \n",
            "    # 2. Try acquire when empty\n",
            "    print(\"\\nAttempting to consume 1 token immediately (should fail)...\")\n",
            "    if limiter.try_acquire():\n",
            "        print(\"Success (unexpected).\")\n",
            "    else:\n",
            "        print(\"Failed (expected).\")\n",
            "        \n",
            "    # 3. Blocking wait\n",
            "    print(\"\\nWaiting for 1 token (should take ~0.5 seconds)...\")\n",
            "    start = time.monotonic()\n",
            "    limiter.wait(1)\n",
            "    duration = time.monotonic() - start\n",
            "    print(f\"Acquired token in {duration:.3f} seconds.\")\n",
            "    \n",
            "    # 4. Thread safety demo\n",
            "    print(\"\\n--- Thread Safety Demo ---\")\n",
            "    \n",
            "    def worker(worker_id: int, limiter: RateLimiter):\n",
            "        for i in range(2):\n",
            "            limiter.wait()\n",
            "            print(f\"Worker {worker_id} performed action {i+1}\")\n",
            "    \n",
            "    threads = []\n",
            "    for i in range(3):\n",
            "        t = threading.Thread(target=worker, args=(i, limiter))\n",
            "        threads.append(t)\n",
            "        t.start()\n",
            "        \n",
            "    for t in threads:\n",
            "        t.join()\n",
            "        \n",
            "    print(\"Demo complete.\")\n",
            "```\n",
            "\n",
            "### Key Implementation Details\n",
            "\n",
            "1.  **Thread Safety**: I used a `threading.Lock` to protect the internal state (`_tokens` and `_last_update`). This ensures that concurrent calls to `acquire` or `try_acquire` result in a consistent state.\n",
            "2.  **Monotonic Time**: The implementation uses `time.monotonic()` instead of `time.time()`. This is crucial for production systems to avoid issues with system clock adjustments (e.g., NTP syncs moving the clock backward).\n",
            "3.  **Efficient Blocking**: The `acquire` method calculates exactly how long to sleep based on the token deficit. It releases the lock while sleeping to allow other threads to check status or backoff, preventing deadlocks.\n",
            "4.  **Atomic Refill**: The refill logic is separated into a private method `_refill` called internally whenever state is accessed. This lazily updates the token count based on elapsed time.\n",
            "5.  **Burst Handling**: The `burst_size` acts as the maximum capacity. If the limiter is unused for a long time, it fills up to this capacity, allowing for temporary spikes in traffic up to this limit.\n"
          ]
        }
      ],
      "metadata": {
        "id": "example3-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e417c2b-dac1-450b-bbe7-b331ad0e0531"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìä Example 4: Structured JSON Output\n",
        "\n",
        "Extract structured data from unstructured text using GLM-5's JSON mode."
      ],
      "metadata": {
        "id": "example4-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title JSON Structured Output\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"\"\"You are a data extraction specialist.\n",
        "Always respond with valid JSON matching this schema:\n",
        "{\n",
        "  \"company_name\": string,\n",
        "  \"industry\": string,\n",
        "  \"key_products\": [string],\n",
        "  \"headquarters\": string,\n",
        "  \"open_source_models\": [string],\n",
        "  \"notable_facts\": [string]\n",
        "}\"\"\"},\n",
        "        {\"role\": \"user\", \"content\": \"Extract structured information about Z.ai (Zhipu AI), the company behind the GLM series of language models.\"}\n",
        "    ],\n",
        "    max_tokens=1024,\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ")\n",
        "\n",
        "import json\n",
        "result = json.loads(response.choices[0].message.content)\n",
        "print(json.dumps(result, indent=2))"
      ],
      "outputs": [],
      "metadata": {
        "id": "example4-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üåä Example 5: Streaming Responses\n",
        "\n",
        "Stream long-form content generation for real-time feedback."
      ],
      "metadata": {
        "id": "example5-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# @title Streaming Demo\n",
        "stream = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain the evolution of open-source AI models from GPT-2 to modern models like GLM-5. Keep it to 3 concise paragraphs.\"}\n",
        "    ],\n",
        "    max_tokens=1024,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "full_response = \"\"\n",
        "print(\"üì° Streaming response:\\n\")\n",
        "\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        full_response += content\n",
        "        print(content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\n‚úÖ Stream complete!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° Streaming response:\n",
            "\n",
            "\n",
            "\n",
            "‚úÖ Stream complete!\n"
          ]
        }
      ],
      "metadata": {
        "id": "example5-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f936d8be-9f5e-47ef-e596-c4b63ec32585"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üß† Example 6: Complex Reasoning & System Design\n",
        "\n",
        "Push GLM-5's flagship capability: complex systems design and deep analytical reasoning."
      ],
      "metadata": {
        "id": "example6-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title System Design & Reasoning Task\n",
        "design_prompt = \"\"\"\n",
        "Design a real-time notification system for a large-scale social media platform.\n",
        "\n",
        "Requirements:\n",
        "- Handle 10M+ concurrent users\n",
        "- Support push notifications, in-app alerts, and email digests\n",
        "- Sub-second delivery for priority notifications\n",
        "- Notification preferences per user (frequency, channels, mute)\n",
        "- Deduplication and batching for non-urgent notifications\n",
        "\n",
        "Provide:\n",
        "1. High-level architecture with component breakdown\n",
        "2. Data flow diagram (described textually)\n",
        "3. Key technology choices with justification\n",
        "4. Scaling strategy and bottleneck analysis\n",
        "5. Failure handling and graceful degradation approach\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a senior systems architect with expertise in distributed systems at scale. Provide thorough, production-ready design.\"},\n",
        "        {\"role\": \"user\", \"content\": design_prompt}\n",
        "    ],\n",
        "    max_tokens=4096\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "outputs": [],
      "metadata": {
        "id": "example6-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìà Summary & Token Usage\n",
        "\n",
        "Track token usage and costs for your API calls."
      ],
      "metadata": {
        "id": "summary-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Check Token Usage (Last Response)\n",
        "if hasattr(response, 'usage') and response.usage:\n",
        "    usage = response.usage\n",
        "    print(\"üìä Token Usage (Last Request):\")\n",
        "    print(f\"   ‚Ä¢ Prompt tokens: {usage.prompt_tokens}\")\n",
        "    print(f\"   ‚Ä¢ Completion tokens: {usage.completion_tokens}\")\n",
        "    print(f\"   ‚Ä¢ Total tokens: {usage.total_tokens}\")\n",
        "else:\n",
        "    print(\"Token usage not available in response\")\n",
        "\n",
        "print(\"\\nüí° Check latest GLM-5 pricing at:\")\n",
        "print(\"   https://openrouter.ai/z-ai/glm-5\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "summary-code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üéØ Key Takeaways\n",
        "\n",
        "**GLM-5 excels at:**\n",
        "- ‚úÖ Complex systems design and architecture\n",
        "- ‚úÖ Long-horizon agentic workflows\n",
        "- ‚úÖ Production-grade code generation\n",
        "- ‚úÖ Deep reasoning with self-correction\n",
        "- ‚úÖ Tool calling for autonomous execution\n",
        "- ‚úÖ Structured JSON output generation\n",
        "\n",
        "**Best use cases:**\n",
        "- üèóÔ∏è Full-system construction & architecture design\n",
        "- üíª Large-scale software development\n",
        "- ü§ñ Building autonomous AI agents\n",
        "- üìä Complex analytical tasks & reasoning\n",
        "- üîß Production backend engineering\n",
        "\n",
        "**Resources:**\n",
        "- üìñ [OpenRouter Model Page](https://openrouter.ai/z-ai/glm-5)\n",
        "- ü§ó [HuggingFace Model Weights](https://huggingface.co/zai-org/GLM-5)\n",
        "\n",
        "---\n",
        "*Notebook by @BuildFastWithAI*"
      ],
      "metadata": {
        "id": "takeaways-cell"
      }
    }
  ]
}