{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1j_nQA9nkyqdmURTcwdUcjVjNTbPBRRZx?usp=sharing)"
      ],
      "metadata": {
        "id": "header-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn by building. Get expert mentorship and work on real AI projects.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)"
      ],
      "metadata": {
        "id": "kZFWPpyyvPVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸš€ Kimi K2.5 â€” Free Access via NVIDIA AI Endpoints\n",
        "\n",
        "**Use Moonshot AI's Kimi K2.5 for FREE with NVIDIA's GPU-accelerated API!**\n",
        "\n",
        "This notebook shows you **5 simple examples** to get started with **Kimi K2.5** â€” a 1 Trillion parameter Mixture-of-Experts model â€” using NVIDIA's free API.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š Model Overview\n",
        "\n",
        "| Feature | Detail |\n",
        "|---------|--------|\n",
        "| **Model** | `moonshotai/kimi-k2.5` |\n",
        "| **Provider** | NVIDIA (free tier) |\n",
        "| **Parameters** | 1 Trillion (MoE, 384 experts) |\n",
        "| **Architecture** | Vision-Language Model (VLM) |\n",
        "| **Strengths** | Reasoning, Coding, Math, Agentic AI |\n",
        "| **Thinking Mode** | âœ… Supported |\n",
        "| **Streaming** | âœ… Supported |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”‘ How to Get Your Free API Key\n",
        "\n",
        "1. Go to [build.nvidia.com](https://build.nvidia.com)\n",
        "2. Sign up for a **free NVIDIA Developer** account\n",
        "3. Navigate to **Kimi K2.5** model page\n",
        "4. Click **\"Get API Key\"** â€” you'll receive free inference credits!\n",
        "5. Save the key in Colab Secrets as `NVIDIA_API_KEY`"
      ],
      "metadata": {
        "id": "intro-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ“¦ Setup & Installation"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Install Dependencies\n",
        "!pip install -q langchain-nvidia-ai-endpoints langchain-core"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "metadata": {
        "id": "install-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5980e6c-15a1-41ec-f30c-8d671b4d6486"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Configure NVIDIA Client\n",
        "from google.colab import userdata\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "# Initialize the client â€” it's this simple!\n",
        "client = ChatNVIDIA(\n",
        "    model=\"moonshotai/kimi-k2.5\",\n",
        "    api_key=userdata.get(\"NVIDIA_API_KEY\"),\n",
        "    temperature=0.7,\n",
        "    top_p=1,\n",
        ")\n",
        "\n",
        "print(\"âœ… Kimi K2.5 client ready via NVIDIA!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Kimi K2.5 client ready via NVIDIA!\n"
          ]
        }
      ],
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f198f7e5-45bf-4760-d298-09f97ded2df6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ’¬ Example 1: Simple Chat â€” Ask Anything\n",
        "\n",
        "The most basic way to use Kimi K2.5 â€” just send a message and get a response."
      ],
      "metadata": {
        "id": "example1-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Simple Chat\n",
        "response = client.invoke(\n",
        "    \"Explain what an API is in simple terms, like I'm 10 years old.\"\n",
        ")\n",
        "\n",
        "print(response.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Imagine you go to a restaurant. You can't just walk into the kitchen and start cooking your own food, right? Instead, you look at the menu and tell the **waiter** what you want. The waiter takes your order to the kitchen, then brings back your food when it's ready.\n",
            "\n",
            "An **API** is like that waiter, but for computer programs. \n",
            "\n",
            "**API** stands for *Application Programming Interface*â€”but basically, it's a messenger that lets different apps and websites talk to each other.\n",
            "\n",
            "Here's how it works:\n",
            "- You use a weather app on your phone\n",
            "- The app needs to get today's temperature from somewhere\n",
            "- Instead of going directly into the weather service's computers (which it can't do), it asks the API: \"Hey, what's the weather in New York?\"\n",
            "- The API goes to the weather computer, gets the answer, and brings it back to your app\n",
            "- Your app shows you: \"It's 72 degrees and sunny!\"\n",
            "\n",
            "The API is the middleman that says, \"Here's how you can ask me for information, and here's how I'll give it to you.\" It keeps things organized and safe, so apps can share stuff (like photos, scores, or weather) without messing with each other's internal workings.\n",
            "\n",
            "It's like having a secret handshake that lets two friends share their toys without opening up their whole toy box!\n"
          ]
        }
      ],
      "metadata": {
        "id": "example1-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b243dd7-c079-421a-821a-b186f7315503"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ§  Example 2: Thinking Mode â€” Watch the AI Reason\n",
        "\n",
        "Kimi K2.5 supports a **\"thinking\" mode** where you can see its internal reasoning process before it gives the final answer. Great for math, logic, and complex problems!"
      ],
      "metadata": {
        "id": "example2-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Thinking Mode with Streaming\n",
        "# Enable thinking mode to see the model's reasoning\n",
        "\n",
        "thinking_client = ChatNVIDIA(\n",
        "    model=\"moonshotai/kimi-k2.5\",\n",
        "    api_key=userdata.get(\"NVIDIA_API_KEY\"),\n",
        "    temperature=1,\n",
        "    top_p=1,\n",
        "    max_tokens=16384,\n",
        ")\n",
        "\n",
        "print(\"ğŸ§  Thinking Mode Active\\n\")\n",
        "print(\"=\" * 50)\n",
        "print(\"ğŸ’­ REASONING:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "reasoning_text = \"\"\n",
        "answer_text = \"\"\n",
        "in_answer = False\n",
        "\n",
        "for chunk in thinking_client.stream(\n",
        "    [{\"role\": \"user\", \"content\": \"A farmer has 17 sheep. All but 9 run away. How many sheep does he have left?\"}],\n",
        "    chat_template_kwargs={\"thinking\": True}\n",
        "):\n",
        "    # Check for reasoning content\n",
        "    if chunk.additional_kwargs and \"reasoning_content\" in chunk.additional_kwargs:\n",
        "        reasoning = chunk.additional_kwargs[\"reasoning_content\"]\n",
        "        reasoning_text += reasoning\n",
        "        print(reasoning, end=\"\")\n",
        "\n",
        "    # Print the final answer\n",
        "    if chunk.content:\n",
        "        if not in_answer:\n",
        "            print(\"\\n\")\n",
        "            print(\"=\" * 50)\n",
        "            print(\"âœ… ANSWER:\")\n",
        "            print(\"=\" * 50)\n",
        "            in_answer = True\n",
        "        answer_text += chunk.content\n",
        "        print(chunk.content, end=\"\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3510529991.py:4: DeprecationWarning: The 'max_tokens' parameter is deprecated and will be removed in a future version. Please use 'max_completion_tokens' instead.\n",
            "  thinking_client = ChatNVIDIA(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  Thinking Mode Active\n",
            "\n",
            "==================================================\n",
            "ğŸ’­ REASONING:\n",
            "==================================================\n",
            " The user is asking a classic riddle. Let me analyze it:\n",
            "\n",
            " \"A farmer has 17 sheep. All but 9 run away. How many sheep does he have left?\"\n",
            "\n",
            " The trick here is the phrase \"All but 9\". This means that 9 sheep did NOT run away. If all but 9 ran away, then 9 sheep remained.\n",
            "\n",
            " So the answer is 9 sheep.\n",
            "\n",
            " This is a common riddle that plays on people's tendency to want to subtract (17 - 9 = 8), but that's incorrect because the \"all but 9\" refers to the ones that stayed, not the ones that left.\n",
            "\n",
            " Actually, let me double-check:\n",
            " - Total sheep: 17\n",
            " - \"All but 9 run away\" means: All sheep except for 9 ran away\n",
            " - So 9 sheep remained\n",
            " - Therefore, the farmer has 9 sheep left\n",
            "\n",
            " The answer is 9. \n",
            "\n",
            "==================================================\n",
            "âœ… ANSWER:\n",
            "==================================================\n",
            " The farmer has **9** sheep left.\n",
            "\n",
            "The phrase \"all but 9\" means that 9 sheep remained behind while the rest ran away. So 17 - 9 = 8 sheep ran away, leaving **9** sheep with the farmer.\n",
            "\n",
            "(It's a trick question that plays on the instinct to calculate 17 - 9 = 8, but \"all but 9\" refers to the ones that *didn't* run away.)\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "example2-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd6574c-0be4-4427-9d80-6861ecb6dd4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ’» Example 3: Code Generation â€” Write Python Code\n",
        "\n",
        "Kimi K2.5 is strong at coding tasks. Let's ask it to generate a useful Python function."
      ],
      "metadata": {
        "id": "example3-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Generate Python Code\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "response = client.invoke([\n",
        "    SystemMessage(content=\"You are a Python expert. Write clean, well-commented code.\"),\n",
        "    HumanMessage(content=\"\"\"Write a Python function that:\n",
        "1. Takes a list of numbers\n",
        "2. Removes duplicates\n",
        "3. Sorts them\n",
        "4. Returns the top N largest numbers\n",
        "\n",
        "Include examples showing how to use it.\"\"\")\n",
        "])\n",
        "\n",
        "print(response.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "example3-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8db2f66-f6d8-4e82-a7fe-866a3c6ef98e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸŒŠ Example 4: Streaming Responses â€” Real-time Output\n",
        "\n",
        "For longer responses, **streaming** gives you output token-by-token â€” no waiting for the full response!"
      ],
      "metadata": {
        "id": "example4-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Streaming Response\n",
        "print(\"ğŸ“¡ Streaming response from Kimi K2.5:\\n\")\n",
        "\n",
        "full_response = \"\"\n",
        "\n",
        "for chunk in client.stream(\n",
        "    \"Give me 5 creative startup ideas using AI that could help students study better. Keep each idea to 2-3 sentences.\"\n",
        "):\n",
        "    content = chunk.content\n",
        "    full_response += content\n",
        "    print(content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\nâœ… Stream complete!\")\n",
        "print(f\"ğŸ“ Total characters: {len(full_response)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¡ Streaming response from Kimi K2.5:\n",
            "\n",
            " 1. **Cognitive Load Optimizer**: An AI that monitors keystroke patterns, eye movement, or typing speed to detect mental fatigue in real-time, automatically adjusting study material difficulty or forcing strategic breaks before burnout hits.\n",
            "\n",
            "2. **Cross-Subject Syntheziser**: A platform that builds dynamic knowledge graphs connecting concepts across different coursesâ€”showing how calculus derivatives relate to physics acceleration or history themes mirror literatureâ€”filling gaps by revealing hidden interdisciplinary patterns.\n",
            "\n",
            "3. **Professor-Specific Exam Simulator**: An AI that trains on a specific instructor's past exams, lecture transcripts, and favorite question types to generate practice tests that mirror their exact wording style, difficulty curve, and grading rubrics with uncanny accuracy.\n",
            "\n",
            "4. **Socratic Voice Companion**: A voice-first AI study partner that refuses to give direct answers, instead using spoken dialogue to ask strategic questions that force students to articulate their reasoning and discover solutions through guided self-explanation.\n",
            "\n",
            "5. **Complementary Study Pod Matchmaker**: An algorithm that analyzes knowledge gaps across students in real-time, automatically forming micro study groups of 3-4 people with perfectly complementary weaknesses and strengths, then moderating peer-teaching sessions to maximize mutual learning.\n",
            "\n",
            "âœ… Stream complete!\n",
            "ğŸ“ Total characters: 1365\n"
          ]
        }
      ],
      "metadata": {
        "id": "example4-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae83362-a954-4930-9ff8-3ed53b262aa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ”„ Example 5: Multi-turn Conversation â€” Chat with History\n",
        "\n",
        "Build a real conversation! The model remembers previous messages and responds in context."
      ],
      "metadata": {
        "id": "example5-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Multi-turn Conversation\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Build a conversation step-by-step\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a friendly cooking assistant. Keep answers short and helpful.\"),\n",
        "    HumanMessage(content=\"I have eggs, bread, and cheese. What can I make?\"),\n",
        "]\n",
        "\n",
        "# Turn 1\n",
        "print(\"ğŸ‘¤ User: I have eggs, bread, and cheese. What can I make?\")\n",
        "response1 = client.invoke(messages)\n",
        "print(f\"ğŸ¤– Kimi: {response1.content}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
        "\n",
        "# Turn 2 â€” add the AI's response and a follow-up question\n",
        "messages.append(AIMessage(content=response1.content))\n",
        "messages.append(HumanMessage(content=\"How do I make the first option? Give me simple steps.\"))\n",
        "\n",
        "print(\"ğŸ‘¤ User: How do I make the first option? Give me simple steps.\")\n",
        "response2 = client.invoke(messages)\n",
        "print(f\"ğŸ¤– Kimi: {response2.content}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
        "\n",
        "# Turn 3 â€” another follow-up\n",
        "messages.append(AIMessage(content=response2.content))\n",
        "messages.append(HumanMessage(content=\"Any tips to make it extra delicious?\"))\n",
        "\n",
        "print(\"ğŸ‘¤ User: Any tips to make it extra delicious?\")\n",
        "response3 = client.invoke(messages)\n",
        "print(f\"ğŸ¤– Kimi: {response3.content}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‘¤ User: I have eggs, bread, and cheese. What can I make?\n",
            "ğŸ¤– Kimi:  You can make:\n",
            "\n",
            "**Egg & Cheese Sandwich** â€“ Fry or scramble the eggs, add cheese (let it melt!), sandwich between bread.\n",
            "\n",
            "**Grilled Cheese with Egg** â€“ Make a grilled cheese, then tuck a fried egg inside.\n",
            "\n",
            "**Egg in a Hole** â€“ Cut a circle from the bread, fry an egg in the hole, and melt cheese on top.\n",
            "\n",
            "**Cheesy French Toast** â€“ Dip bread in beaten egg, fry it, and add cheese between two slices like a sandwich.\n",
            "\n",
            "The sandwich options are quickest!\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ‘¤ User: How do I make the first option? Give me simple steps.\n",
            "ğŸ¤– Kimi:  **Egg & Cheese Sandwich**\n",
            "\n",
            "1. **Beat** 1â€“2 eggs with a pinch of salt/pepper (optional).\n",
            "2. **Cook** eggs in a lightly oiled/buttered pan over medium heat, stirring for scrambled or flipping for a patty.\n",
            "3. **Add cheese** on top of the eggs in the last minute; cover the pan briefly so it melts.\n",
            "4. **Toast** your bread in the same pan or a toaster until golden.\n",
            "5. **Assemble:** Bread â†’ cheesy eggs â†’ bread. \n",
            "\n",
            "Eat warm!\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ‘¤ User: Any tips to make it extra delicious?\n",
            "ğŸ¤– Kimi:  **Quick upgrades:**\n",
            "\n",
            "â€¢ **Butter everything** â€“ Melt butter in the pan, then toast the bread in it instead of using a toaster.  \n",
            "â€¢ **Low and slow eggs** â€“ Cook eggs on medium-low, stirring gently; theyâ€™ll be creamier than high-heat rubbery ones.  \n",
            "â€¢ **Cheese contact** â€“ Lay cheese on the eggs while still in the pan, then cover with a lid (or foil) for 30 seconds to get that gooey melt.  \n",
            "â€¢ **Season early** â€“ Pinch of salt on the eggs while cooking, plus black pepper if you have it.  \n",
            "â€¢ **Crispy edges** â€“ If you want a \"grilled cheese\" vibe, butter the outside of the bread and fry the whole sandwich until golden.\n",
            "\n",
            "Enjoy!\n"
          ]
        }
      ],
      "metadata": {
        "id": "example5-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1cf51ac-ed89-4855-ace5-99eaeeb84854"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸ¯ Summary & Next Steps\n",
        "\n",
        "### What We Covered:\n",
        "\n",
        "| # | Example | What You Learned |\n",
        "|---|---------|------------------|\n",
        "| 1 | Simple Chat | Basic `invoke()` call |\n",
        "| 2 | Thinking Mode | Streaming with reasoning visibility |\n",
        "| 3 | Code Generation | System + User messages |\n",
        "| 4 | Streaming | Real-time token-by-token output |\n",
        "| 5 | Multi-turn Chat | Conversation with message history |\n",
        "\n",
        "### ğŸ”‘ Key Points:\n",
        "- âœ… **Completely FREE** via NVIDIA's API\n",
        "- âœ… **1 Trillion parameters** â€” massive model, zero cost\n",
        "- âœ… **Thinking mode** â€” see the AI's reasoning process\n",
        "- âœ… **Easy setup** â€” just `pip install` and go\n",
        "\n",
        "### ğŸ“š Resources:\n",
        "- ğŸ”— [NVIDIA API Catalog](https://build.nvidia.com)\n",
        "- ğŸ”— [Kimi K2.5 on NVIDIA](https://build.nvidia.com/moonshotai/kimi-k2.5)\n",
        "- ğŸ”— [LangChain NVIDIA Docs](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/)\n",
        "\n",
        "---\n",
        "*Notebook by @BuildFastWithAI*"
      ],
      "metadata": {
        "id": "summary-cell"
      }
    }
  ]
}