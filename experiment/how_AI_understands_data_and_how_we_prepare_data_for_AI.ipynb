{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G0yrDgnVdLp"
      },
      "source": [
        "# How AI Understands Data & How We Prepare It üß†\n",
        "\n",
        "To build great AI applications (like RAG systems), you need to understand how LLMs \"see\" data. It's not just text in, text out!\n",
        "\n",
        "In this notebook, we'll explore the **Three Pillars of AI Data**:\n",
        "\n",
        "1.  **Tokens**: The atoms of language for AI.\n",
        "2.  **Embeddings**: Converting text into meaning (numbers).\n",
        "3.  **Chunking**: Breaking data into bite-sized pieces for the model.\n",
        "\n",
        "---"
      ],
      "id": "8G0yrDgnVdLp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3sn9CbZVdLz"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install tiktoken numpy matplotlib scikit-learn\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "except Exception:\n",
        "    pass"
      ],
      "id": "I3sn9CbZVdLz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vprmr7R5VdL2"
      },
      "source": [
        "## 1. Tokens: The Atoms of Language ‚öõÔ∏è\n",
        "\n",
        "We learned in the pricing guide that models read tokens, not words. But how does that actually look?\n",
        "\n",
        "### Why does this matter?\n",
        "- **Context Window**: Models have a limit (e.g., 128k tokens). You fit more if you tokenize efficiently.\n",
        "- **Understanding**: Rare words might be broken into multiple tokens, which can confuse smaller models."
      ],
      "id": "Vprmr7R5VdL2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoIqBR7NVdL3"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def show_tokens(text):\n",
        "    tokens = encoder.encode(text)\n",
        "    decoded = [encoder.decode([t]) for t in tokens]\n",
        "    print(f\"Original: {text}\")\n",
        "    print(f\"Token IDs: {tokens}\")\n",
        "    print(f\"Read as: {decoded}\\n\")\n",
        "\n",
        "show_tokens(\"Apple\")\n",
        "show_tokens(\"ApplePie\") # Compound words usually split\n",
        "show_tokens(\"The quick brown fox jumps over the lazy dog.\")"
      ],
      "id": "OoIqBR7NVdL3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGhmUkL0VdL5"
      },
      "source": [
        "## 2. Embeddings: Meaning as Numbers üî¢\n",
        "\n",
        "Tokens are just integers. To understand *meaning*, AI converts these tokens into **Vectors** (lists of numbers).\n",
        "\n",
        "### The Theory: Vector Space & Cosine Similarity\n",
        "Imagine a huge multi-dimensional space (often 1536 dimensions for OpenAI models).\n",
        "Every piece of text is a point in this space.\n",
        "\n",
        "- **Vector**: A list of numbers representing the coordinates of that point.\n",
        "- **Cosine Similarity**: The math used to measure distance. It checks the *angle* between two vectors.\n",
        "    - **0 degrees (1.0)**: Identical meaning.\n",
        "    - **90 degrees (0.0)**: Completely unrelated.\n",
        "    - **180 degrees (-1.0)**: Opposite meaning.\n",
        "\n",
        "### Semantic Search vs. Keyword Search\n",
        "- **Keyword Search** (Old): Matches exact words. Fails if you search \"canine\" but the text says \"dog\".\n",
        "- **Semantic Search** (AI): Matches *meaning*. \"Canine\" and \"Dog\" have vectors that are very close together, so it works!"
      ],
      "id": "oGhmUkL0VdL5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiFI-8BcVdL6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "words = [\"King\", \"Queen\", \"Man\", \"Woman\", \"Apple\", \"Orange\"]\n",
        "vectors = {\n",
        "    \"King\": [0.9, 0.8],\n",
        "    \"Queen\": [0.8, 0.9],\n",
        "    \"Man\": [0.9, 0.1],\n",
        "    \"Woman\": [0.8, 0.2],\n",
        "    \"Apple\": [0.1, 0.9],\n",
        "    \"Orange\": [0.2, 0.8]\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for word, vec in vectors.items():\n",
        "    plt.scatter(vec[0], vec[1], s=100)\n",
        "    plt.text(vec[0]+0.02, vec[1]+0.02, word, fontsize=12)\n",
        "\n",
        "plt.title(\"Simplified Embedding Space (Semantic Proximity)\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "oiFI-8BcVdL6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvvNDjgJVdL7"
      },
      "source": [
        "## 3. Chunking: Preparing Data for AI üç∞\n",
        "\n",
        "When you have a huge PDF or dataset, you can't feed it all to the LLM at once. You need to **Chunk** it.\n",
        "\n",
        "### Why Overlap matters?\n",
        "Notice in chunking we often use \"overlap\" (e.g., 50 tokens).\n",
        "If we cut a sentence in half, we lose context.\n",
        "\n",
        "**Example:**\n",
        "- Chunk 1: \"The secret code to the vault is...\"\n",
        "- Chunk 2: \"...1234. Don't tell anyone!\"\n",
        "\n",
        "If we search for \"vault code\", we might get Chunk 1, but it doesn't have the answer! Overlapping ensures the important context (the code itself) appears fully in at least one chunk."
      ],
      "id": "AvvNDjgJVdL7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8sK7iR9VdL9"
      },
      "outputs": [],
      "source": [
        "# Example of a Simple Recursive Chunker logic\n",
        "\n",
        "text = \"\"\"\n",
        "GenAI is fascinating. It changes how we work.\n",
        "\n",
        "However, data preparation is key. Bad data = Bad AI.\n",
        "This is why we chunk text.\n",
        "\"\"\"\n",
        "\n",
        "def simple_recursive_chunker(text, chunk_size=50):\n",
        "    chunks = []\n",
        "    # Split by double newline first (paragraphs)\n",
        "    paragraphs = text.split(\"\\n\\n\")\n",
        "\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for para in paragraphs:\n",
        "        if len(current_chunk) + len(para) < chunk_size:\n",
        "            current_chunk += para + \"\\n\\n\"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = para + \"\\n\\n\"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "chunks = simple_recursive_chunker(text, chunk_size=50)\n",
        "\n",
        "print(f\"Original Length: {len(text)}\")\n",
        "print(f\"Number of Chunks: {len(chunks)}\")\n",
        "print(\"----\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}: '{chunk}'\")"
      ],
      "id": "c8sK7iR9VdL9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe041ar0VdL_"
      },
      "source": [
        "## Conclusion üèÅ\n",
        "\n",
        "-   **Tokens** are the cost unit.\n",
        "-   **Embeddings** are the meaning unit.\n",
        "-   **Chunks** are the processing unit.\n",
        "\n",
        "Master these three, and you're ready to build RAG (Retrieval Augmented Generation) apps!"
      ],
      "id": "fe041ar0VdL_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}