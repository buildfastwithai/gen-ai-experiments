{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_C4c4nQCzQNXdv1sJb24kZJZs-tpJ690?usp=sharing)\n",
        "\n",
        "\n",
        "## Google Gemini Interactions API\n",
        "**What You'll Learn:**\n",
        "- Build with the Interactions API\n",
        "- Create multi-turn conversations (stateful & stateless)\n",
        "- Use multimodal capabilities\n",
        "- Leverage agentic features (tools, function calling)\n",
        "- Implement streaming and advanced configurations\n",
        "\n",
        "The Interactions API is designed for building and interacting with agents, and includes support for function calling, built-in tools, structured outputs, and the Model Context Protocol (MCP).\n",
        "\n",
        "[Learn More](https://ai.google.dev/gemini-api/docs/interactions)"
      ],
      "metadata": {
        "id": "header_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Installation\n",
        "Install the latest Google GenAI SDK to use the Interactions API."
      ],
      "metadata": {
        "id": "setup_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-genai"
      ],
      "metadata": {
        "id": "install_sdk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. API Key and Client Setup\n",
        "Initialize the GenAI client with your API key."
      ],
      "metadata": {
        "id": "client_setup_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your API key from Colab secrets\n",
        "api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "client = genai.Client(api_key=api_key)\n",
        "\n",
        "print(\"Client initialized successfully!\")"
      ],
      "metadata": {
        "id": "client_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef8c785-7adc-45c3-b7f4-803e51829fe3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Basic Interactions\n",
        "The simplest way to interact with the model is by providing a text prompt. The input can be a string, a list containing content objects, or a list of turns with roles and content objects."
      ],
      "metadata": {
        "id": "basic_interactions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic text interaction\n",
        "interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"Tell me a short joke about programming.\"\n",
        ")\n",
        "\n",
        "print(f\"Response: {interaction.outputs[-1].text}\")"
      ],
      "metadata": {
        "id": "basic_interaction_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438342a4-2180-456f-d904-2a2f2c6849cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4030357283.py:2: UserWarning: Interactions usage is experimental and may change in future versions.\n",
            "  interaction = client.interactions.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Why do programmers prefer dark mode?\n",
            "\n",
            "Because light attracts bugs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another basic example\n",
        "interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"What are the benefits of using Python for data science?\"\n",
        ")\n",
        "\n",
        "print(f\"Response: {interaction.outputs[-1].text}\")"
      ],
      "metadata": {
        "id": "basic_interaction_example2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7800078-e4a9-42dd-ed53-f4c6aa00c35f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Python has become the industry standard for data science, surpassing other languages like R, SAS, and MATLAB in general popularity. Its dominance is due to a unique combination of simplicity, a massive ecosystem, and versatility.\n",
            "\n",
            "Here are the primary benefits of using Python for data science:\n",
            "\n",
            "### 1. Simple and Readable Syntax\n",
            "Python’s syntax is designed to be readable and mimics the English language. \n",
            "*   **Low Barrier to Entry:** Data scientists often come from backgrounds in math, statistics, or biology rather than computer science. Python allows them to focus on solving data problems rather than struggling with complex code structures (like pointers or memory management).\n",
            "*   **Efficiency:** You can accomplish complex tasks in fewer lines of code than in C++ or Java.\n",
            "\n",
            "### 2. An Unrivaled Ecosystem of Libraries\n",
            "Python has a specialized library for every step of the data science workflow:\n",
            "*   **Data Manipulation:** **Pandas** and **NumPy** are the industry standards for handling structured data and numerical arrays.\n",
            "*   **Machine Learning:** **Scikit-learn** provides a consistent interface for almost all classical machine learning algorithms.\n",
            "*   **Deep Learning:** **TensorFlow** and **PyTorch** are the leading frameworks for building neural networks and AI.\n",
            "*   **Data Visualization:** **Matplotlib**, **Seaborn**, and **Plotly** allow for the creation of everything from simple charts to complex, interactive dashboards.\n",
            "\n",
            "### 3. Strong Community and Support\n",
            "Because Python is so popular, it has a massive global community.\n",
            "*   **Troubleshooting:** If you encounter an error, the solution is likely already on Stack Overflow or GitHub.\n",
            "*   **Pre-built Solutions:** Most new research papers in AI and Data Science release their code in Python first, meaning you can use cutting-edge techniques almost immediately.\n",
            "\n",
            "### 4. Versatility and \"Full-Stack\" Capability\n",
            "Python is not just for statistics; it is a general-purpose language. This is a major advantage over R.\n",
            "*   **Integration:** You can use Python to scrape data from the web (BeautifulSoup), interact with databases (SQLAlchemy), and then build a machine learning model.\n",
            "*   **Deployment:** You can turn your data model into a web application using frameworks like **Flask**, **Django**, or **Streamlit**, all within the same language.\n",
            "\n",
            "### 5. Scalability and Big Data Integration\n",
            "While Python can be slower than compiled languages like C, it integrates perfectly with high-performance tools:\n",
            "*   **Big Data:** Python works seamlessly with **Apache Spark** (via PySpark) to process massive datasets across clusters.\n",
            "*   **Performance:** Libraries like NumPy are actually written in C under the hood, giving you Python’s ease of use with the speed of lower-level languages.\n",
            "\n",
            "### 6. Wide Adoption in Industry\n",
            "Most major tech companies (Google, Netflix, NASA, Meta) use Python for their data pipelines. \n",
            "*   **Interoperability:** It works well with cloud platforms like AWS, Google Cloud, and Azure, which all provide extensive SDKs for Python.\n",
            "*   **Career Opportunities:** Because it is the \"gold standard,\" learning Python makes a data scientist highly employable across different sectors (finance, healthcare, tech).\n",
            "\n",
            "### 7. Excellent Data Visualization\n",
            "Data science is as much about communication as it is about calculation. Python’s visualization libraries allow you to create:\n",
            "*   Static publication-quality plots.\n",
            "*   Interactive web-based maps and charts.\n",
            "*   Real-time monitoring dashboards.\n",
            "\n",
            "### Summary\n",
            "While other languages have specific strengths (e.g., R for pure statistical research or Julia for high-speed mathematical computing), **Python is the \"Swiss Army Knife\" of data science.** It balances ease of use with enough power to handle production-level machine learning at scale.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Multi-turn Conversations\n",
        "You can build multi-turn conversations in two ways:\n",
        "- **Statefully**: by referencing a previous interaction\n",
        "- **Statelessly**: by providing the entire conversation history"
      ],
      "metadata": {
        "id": "conversation_section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Stateful Conversation\n",
        "Pass the `id` from the previous interaction to the `previous_interaction_id` parameter to continue a conversation. The server manages the conversation history for you."
      ],
      "metadata": {
        "id": "stateful_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First turn\n",
        "interaction1 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"Hi, my name is Phil.\"\n",
        ")\n",
        "print(f\"Model: {interaction1.outputs[-1].text}\")\n",
        "\n",
        "# 2. Second turn (passing previous_interaction_id)\n",
        "interaction2 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"What is my name?\",\n",
        "    previous_interaction_id=interaction1.id\n",
        ")\n",
        "print(f\"Model: {interaction2.outputs[-1].text}\")"
      ],
      "metadata": {
        "id": "stateful_conversation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e99135-2a82-4f72-f49b-fd92f4243c8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Hi Phil! It's nice to meet you. How can I help you today?\n",
            "Model: Your name is Phil! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieve Past Stateful Interactions\n",
        "Using the interaction `id` to retrieve previous turns of the conversation."
      ],
      "metadata": {
        "id": "retrieve_interactions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a previous interaction using its ID\n",
        "previous_interaction = client.interactions.get(interaction1.id)\n",
        "print(f\"Retrieved Interaction ID: {previous_interaction.id}\")\n",
        "print(f\"Retrieved Content: {previous_interaction.outputs[-1].text}\")"
      ],
      "metadata": {
        "id": "retrieve_interaction_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58d3670-be95-4577-aa60-9065c696deb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved Interaction ID: v1_ChdmZGxuYWVxZktyR05qckVQdklpWWdBaxIXZmRsbmFlcWZLckdOanJFUHZJaVlnQWs\n",
            "Retrieved Content: Hi Phil! It's nice to meet you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Stateless Conversation\n",
        "You can manage conversation history manually on the client side by providing the full history with each request."
      ],
      "metadata": {
        "id": "stateless_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manage conversation history manually\n",
        "conversation_history = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What are the three largest cities in Spain?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "interaction1 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=conversation_history\n",
        ")\n",
        "print(f\"Model: {interaction1.outputs[-1].text}\")\n",
        "\n",
        "# Add model response and new user message to history\n",
        "conversation_history.append({\"role\": \"model\", \"content\": interaction1.outputs})\n",
        "conversation_history.append({\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"What is the most famous landmark in the second one?\"\n",
        "})\n",
        "\n",
        "interaction2 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=conversation_history\n",
        ")\n",
        "print(f\"Model: {interaction2.outputs[-1].text}\")"
      ],
      "metadata": {
        "id": "stateless_conversation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c168bdc-1745-419f-8a07-9008e1d936e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: The three largest cities in Spain by population are:\n",
            "\n",
            "1.  **Madrid** (approx. 3.3 million)\n",
            "2.  **Barcelona** (approx. 1.6 million)\n",
            "3.  **Valencia** (approx. 800,000)\n",
            "Model: The most famous landmark in **Barcelona** is the **Sagrada Família**, the massive unfinished minor basilica designed by the architect Antoni Gaudí.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Agentic Capabilities\n",
        "The Interactions API is designed for building and interacting with agents, and includes support for function calling, built-in tools, structured outputs, and the Model Context Protocol (MCP)."
      ],
      "metadata": {
        "id": "agentic_section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Deep Research Agent\n",
        "Use specialized agents like `deep-research-pro-preview-12-2025` for complex research tasks."
      ],
      "metadata": {
        "id": "deep_research_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# 1. Start the Deep Research Agent\n",
        "initial_interaction = client.interactions.create(\n",
        "    input=\"Research the history of the Google TPUs with a focus on 2025 and 2026.\",\n",
        "    agent=\"deep-research-pro-preview-12-2025\",\n",
        "    background=True\n",
        ")\n",
        "\n",
        "print(f\"Research started. Interaction ID: {initial_interaction.id}\")\n",
        "\n",
        "# 2. Poll for results\n",
        "while True:\n",
        "    interaction = client.interactions.get(initial_interaction.id)\n",
        "    print(f\"Status: {interaction.status}\")\n",
        "\n",
        "    if interaction.status == \"completed\":\n",
        "        print(\"\\nFinal Report:\\n\", interaction.outputs[-1].text)\n",
        "        break\n",
        "    elif interaction.status in [\"failed\", \"cancelled\"]:\n",
        "        print(f\"Failed with status: {interaction.status}\")\n",
        "        break\n",
        "\n",
        "    time.sleep(10)"
      ],
      "metadata": {
        "id": "deep_research_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652129c1-6a72-47a5-8c62-97029c81acb1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Research started. Interaction ID: v1_ChdwOWxuYWJMZlBPM3FqckVQdFoyczhRaxIXcDlsbmFiTGZQTzNxanJFUHRaMnM4UWs\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: completed\n",
            "\n",
            "Final Report:\n",
            " # The Evolution of Google Tensor Processing Units: A Historical Analysis with Emphasis on the 2025-2026 Paradigm Shift\n",
            "\n",
            "### Key Points\n",
            "*   **Strategic Pivot to Inference (2025):** The release of the TPU v7 \"Ironwood\" in 2025 marked a fundamental architectural shift, prioritizing inference workloads to address the \"Age of Inference.\" This generation introduced native FP8 support and a massive increase in High Bandwidth Memory (HBM) to 192GB per chip to support large language models (LLMs) [cite: 1, 2].\n",
            "*   **Commercialization and Ecosystem Expansion:** In late 2025, Google solidified a historic $52 billion partnership with Anthropic, involving the deployment of up to 1 million TPUs. This move signaled Google's transition from using TPUs primarily for internal workloads to becoming a major merchant silicon provider competing directly with Nvidia [cite: 3, 4].\n",
            "*   **Bifurcation of Architecture (2026):** The roadmap for 2026 introduces the TPU v8 generation, split into two distinct architectures: \"Sunfish\" (TPU v8ax) for training and \"Zebrafish\" (TPU v8x) for inference. This specialization reflects the diverging computational requirements of model training versus large-scale deployment [cite: 5].\n",
            "*   **Supply Chain Diversification:** To meet the scaling demands of 2026, Google expanded its supply chain beyond Broadcom, enlisting MediaTek for the inference-focused v8 designs, thereby reducing reliance on single-source partners and optimizing production costs [cite: 6, 7].\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Introduction\n",
            "\n",
            "The trajectory of artificial intelligence hardware has been defined by the transition from general-purpose computing units to Domain-Specific Architectures (DSAs). Google’s Tensor Processing Unit (TPU) stands as a seminal example of this evolution. First deployed internally in 2015, the TPU was designed to accelerate machine learning workloads built on the TensorFlow framework. For the first decade of its existence, the TPU served primarily as the backbone of Google’s internal services—powering Search, Translate, and Photos—before becoming a cloud offering.\n",
            "\n",
            "However, the years 2025 and 2026 represent a critical inflection point in the history of the TPU. Driven by the exponential computational demands of generative AI and Large Language Models (LLMs), Google accelerated its hardware roadmap, moving from the general-purpose TPU v6 \"Trillium\" to the highly specialized TPU v7 \"Ironwood\" and the bifurcated TPU v8 series. This report provides an exhaustive analysis of the TPU's history, with a specific focus on the architectural breakthroughs, market maneuvers, and supply chain realignments that characterized the 2025-2026 period.\n",
            "\n",
            "## 2. Historical Context: The Pre-2025 Era (TPU v1–v5)\n",
            "\n",
            "To understand the significance of the developments in 2025, one must first contextualize the architectural lineage of the TPU.\n",
            "\n",
            "### 2.1 First Generation: The Inference Engine\n",
            "Google introduced the TPU v1 in 2015 (publicly announced in 2016) to address the computational bottlenecks of deep learning inference. Unlike Graphics Processing Units (GPUs), which were designed for parallel graphics rendering, the TPU v1 was an Application-Specific Integrated Circuit (ASIC) optimized for 8-bit integer operations. It utilized a systolic array architecture to maximize matrix multiplication throughput while minimizing energy consumption [cite: 8, 9].\n",
            "\n",
            "### 2.2 Second and Third Generations: Training and Liquid Cooling\n",
            "The TPU v2 (2017) and TPU v3 (2018) marked Google's entry into AI training. These generations introduced High Bandwidth Memory (HBM) and the ability to interconnect chips into massive \"Pods\" using a proprietary Inter-Core Interconnect (ICI) fabric. TPU v3 was notable for introducing liquid cooling to Google’s data centers to manage increased thermal density [cite: 8, 9].\n",
            "\n",
            "### 2.3 Fourth and Fifth Generations: Optical Switching and Efficiency\n",
            "The TPU v4 (2021) revolutionized data center networking by integrating Optical Circuit Switches (OCS), allowing for dynamic reconfiguration of supercomputer topologies. This was followed by the TPU v5 series in 2023/2024, which split into two variants: the TPU v5e (efficiency-focused) and the TPU v5p (performance-focused). The v5p was designed to train massive foundation models, serving as the workhorse for Google's Gemini 1.0 models [cite: 8, 10].\n",
            "\n",
            "## 3. The Trillium Bridge (2024-2025)\n",
            "\n",
            "In May 2024, Google announced the sixth-generation TPU, codenamed \"Trillium\" (TPU v6). While Trillium was a significant leap forward, it served largely as a bridge to the radical architectural shifts of 2025.\n",
            "\n",
            "### 3.1 Architectural Specifications\n",
            "Trillium was engineered to address the immediate needs of the generative AI boom. It delivered a 4.7x increase in peak compute performance per chip compared to the TPU v5e and doubled both the HBM capacity and the Interchip Interconnect (ICI) bandwidth [cite: 11, 12].\n",
            "*   **Energy Efficiency:** Trillium achieved a 67% improvement in energy efficiency over its predecessor, a critical metric as data center power consumption became a limiting factor in AI scaling [cite: 12].\n",
            "*   **Scaling:** A single Trillium pod could scale up to 256 chips, with \"multislice\" technology allowing tens of thousands of chips to be connected via the Jupiter data center network [cite: 11, 12].\n",
            "\n",
            "### 3.2 Market Positioning\n",
            "Trillium reached general availability in late 2024 and early 2025, underpinning the training of Gemini 2.0 [cite: 13, 14]. It was positioned as a versatile chip capable of handling both training and inference, but it retained the unified architecture of previous generations. Its primary role was to provide immediate relief to the GPU shortage and offer a cost-effective alternative to Nvidia’s H100 for Google Cloud customers [cite: 15].\n",
            "\n",
            "## 4. 2025: The Ironwood Revolution (TPU v7)\n",
            "\n",
            "The year 2025 defined a new era for Google's silicon with the introduction of the TPU v7, codenamed \"Ironwood.\" Announced in preview in April 2025 and reaching general availability later that year, Ironwood represented a departure from general-purpose AI acceleration toward a design explicitly optimized for the \"Age of Inference\" [cite: 1, 16].\n",
            "\n",
            "### 4.1 Architectural Deep Dive\n",
            "Ironwood was designed to solve the specific bottlenecks associated with serving \"thinking models\"—complex LLMs and Mixture-of-Experts (MoE) models that require massive memory bandwidth and low latency.\n",
            "\n",
            "#### 4.1.1 Systolic Array Expansion\n",
            "A defining feature of Ironwood was the reconfiguration of its compute units. While previous generations utilized 128x128 systolic arrays, Ironwood expanded this to 256x256 arrays. This quadrupling of the matrix multiplication unit (MXU) size allowed for significantly higher throughput per cycle, essential for the dense matrix math inherent in transformer models [cite: 17, 18].\n",
            "\n",
            "#### 4.1.2 Memory and Bandwidth\n",
            "To accommodate the ballooning size of model parameters and Key-Value (KV) caches, Ironwood integrated 192GB of HBM3e memory per chip—a 6x increase over the previous Trillium generation [cite: 1, 2].\n",
            "*   **Bandwidth:** The memory bandwidth reached 7.4 TB/s per chip, a 4.5x improvement over Trillium. This massive bandwidth was critical for reducing memory-bound latency in large-scale inference tasks [cite: 1, 19].\n",
            "*   **Interconnect:** The ICI bandwidth was increased to 1.2 TB/s per chip, facilitating rapid communication within the 3D torus topology of TPU pods [cite: 17].\n",
            "\n",
            "#### 4.1.3 Native FP8 Support\n",
            "Ironwood was the first TPU to introduce native support for FP8 (8-bit floating point) precision. Previous generations emulated lower precision or relied on BF16 (BrainFloat 16). The shift to native FP8 allowed Ironwood to deliver 4.6 PetaFLOPS of compute performance per chip, rivaling Nvidia’s Blackwell architecture in raw throughput while maintaining higher energy efficiency [cite: 2, 17].\n",
            "\n",
            "### 4.2 The \"Age of Inference\" Strategy\n",
            "Google explicitly marketed Ironwood as the hardware for the \"Age of Inference.\" As foundation models matured, the industry focus shifted from training (creating the model) to inference (running the model). Ironwood’s high memory capacity allowed it to store larger model partitions locally, reducing the need for inter-chip communication and enabling the efficient serving of models with long context windows [cite: 1, 20].\n",
            "\n",
            "### 4.3 System-Level Scaling\n",
            "Ironwood introduced a new \"Superpod\" architecture capable of scaling up to 9,216 chips in a single synchronous domain. This configuration delivered approximately 42.5 ExaFLOPS of FP8 compute power, surpassing the capabilities of many top-tier supercomputers [cite: 1, 17]. The use of Optical Circuit Switches (OCS) allowed these pods to be dynamically reconfigured, maximizing utilization and fault tolerance [cite: 21].\n",
            "\n",
            "## 5. The Commercialization Pivot: The Anthropic Deal\n",
            "\n",
            "Perhaps the most significant event in the TPU's history during 2025 was the massive commercial agreement between Google and Anthropic. This deal marked the transition of the TPU from an internal differentiator to a merchant silicon product.\n",
            "\n",
            "### 5.1 Deal Structure and Valuation\n",
            "In October 2025, Google and Anthropic announced a partnership valued at approximately $52 billion. The agreement included the deployment of up to 1 million TPU v7 Ironwood chips for Anthropic’s use [cite: 3, 4].\n",
            "*   **Direct Purchase vs. Rental:** The deal was structured as a hybrid model. Approximately 400,000 TPU v7 chips were purchased directly by Anthropic (via Broadcom) for on-premise or collocated deployment, while 600,000 units were rented through Google Cloud Platform (GCP) [cite: 3].\n",
            "*   **Strategic Implications:** This deal provided Anthropic with over a gigawatt of compute capacity by 2026, reducing its reliance on Nvidia GPUs and Amazon Trainium chips. For Google, it validated the TPU v7 as a commercially viable alternative to Nvidia’s dominance [cite: 22, 23].\n",
            "\n",
            "### 5.2 Total Cost of Ownership (TCO)\n",
            "A key driver for this partnership was the superior TCO of the TPU v7. Analysis indicated that the TPU v7 offered a TCO approximately 44% lower than comparable Nvidia GB200 servers for Google’s internal use, and roughly 30% lower for external customers like Anthropic. This cost efficiency was driven by the TPU's vertical power delivery and optimized cooling infrastructure [cite: 3, 24].\n",
            "\n",
            "## 6. 2026: Bifurcation and Mass Deployment (TPU v8)\n",
            "\n",
            "Looking ahead to 2026, the TPU roadmap indicates a further specialization of hardware to meet the distinct needs of training and inference.\n",
            "\n",
            "### 6.1 TPU v8: Sunfish and Zebrafish\n",
            "Reports emerging in late 2025 and early 2026 confirmed that the eighth generation of TPUs would be split into two distinct product lines, expected to launch in the second half of 2026 [cite: 5, 25].\n",
            "\n",
            "#### 6.1.1 TPU v8ax \"Sunfish\" (Training)\n",
            "The \"Sunfish\" variant is designed specifically for training massive foundation models. It focuses on maximizing floating-point performance and interconnect bandwidth to handle the synchronization requirements of distributed training. Google partnered with Broadcom for the end-to-end design and packaging of this chip, continuing their long-standing collaboration [cite: 5].\n",
            "\n",
            "#### 6.1.2 TPU v8x \"Zebrafish\" (Inference)\n",
            "The \"Zebrafish\" variant is optimized for large-scale inference. It prioritizes memory capacity and energy efficiency over raw training throughput. Notably, Google enlisted MediaTek to assist with the design of the I/O and peripheral components for Zebrafish. This partnership with MediaTek marked a strategic diversification of Google’s supply chain, aiming to lower production costs and leverage MediaTek’s expertise in power-efficient designs [cite: 6].\n",
            "\n",
            "### 6.2 Manufacturing and Supply Chain\n",
            "Both TPU v8 variants are slated for production on TSMC’s 3nm process node. The supply chain for 2026 involves a complex network of partners:\n",
            "*   **Broadcom:** Remains the primary partner for the high-performance training chips (Sunfish) and networking switches [cite: 7, 26].\n",
            "*   **MediaTek:** Emerging as a key partner for the inference chips (Zebrafish), with revenue contributions expected to ramp up in 2026 [cite: 6, 27].\n",
            "*   **TSMC:** Provides the CoWoS (Chip-on-Wafer-on-Substrate) advanced packaging required for the HBM integration [cite: 26].\n",
            "\n",
            "### 6.3 Market Outlook for 2026\n",
            "By 2026, Google is projected to ship between 3.1 and 3.2 million TPU units, driven by the mass deployment of Ironwood and the initial ramp of the v8 series. This volume represents a significant portion of the global AI accelerator market, positioning Google as the largest challenger to Nvidia in the cloud AI sector [cite: 21, 27].\n",
            "\n",
            "## 7. Comparative Analysis: TPU vs. GPU in 2025-2026\n",
            "\n",
            "The rivalry between Google’s TPUs and Nvidia’s GPUs intensified during this period.\n",
            "\n",
            "### 7.1 Architecture and Flexibility\n",
            "Nvidia’s GPUs (H100, Blackwell) rely on the CUDA ecosystem, offering immense flexibility for researchers experimenting with novel architectures. In contrast, TPUs are ASICs that require models to be defined using XLA (Accelerated Linear Algebra) via frameworks like JAX or TensorFlow. While less flexible, the TPU’s systolic array architecture offers higher efficiency for standard matrix operations [cite: 19, 21].\n",
            "\n",
            "### 7.2 Performance per Watt\n",
            "In 2025, the TPU v7 Ironwood claimed a 2x improvement in performance per watt over the previous Trillium generation. When compared to Nvidia’s Blackwell, Ironwood offered a competitive advantage in power-constrained environments, allowing for higher compute density per rack [cite: 1, 2].\n",
            "\n",
            "### 7.3 Economic Model\n",
            "Google’s ability to offer TPUs at a lower price point—both for internal use and external rental—created a \"moat\" against the high margins commanded by Nvidia. The Anthropic deal demonstrated that for large-scale, stable workloads, the economic benefits of the TPU could outweigh the flexibility of the GPU ecosystem [cite: 28, 29].\n",
            "\n",
            "## 8. Conclusion\n",
            "\n",
            "The years 2025 and 2026 represent a transformative era for Google’s Tensor Processing Units. The launch of the TPU v7 \"Ironwood\" signified a strategic pivot toward inference, acknowledging that the future of AI lies in the efficient deployment of models rather than just their creation. The unprecedented $52 billion deal with Anthropic validated the TPU as a commercial-grade competitor to Nvidia’s hardware, breaking the monopoly of the GPU in the high-end AI accelerator market.\n",
            "\n",
            "Furthermore, the bifurcation of the roadmap in 2026 with the \"Sunfish\" and \"Zebrafish\" architectures demonstrates Google’s maturing silicon strategy. By optimizing distinct chips for training and inference and diversifying its supply chain to include MediaTek, Google has positioned the TPU not merely as an internal accelerator, but as a cornerstone of the global AI infrastructure. As the \"Age of Inference\" accelerates, the innovations established during this period will likely define the economics and performance standards of artificial intelligence for the remainder of the decade.\n",
            "\n",
            "**Sources:**\n",
            "1. [blog.google](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGhOWLe3T-9wBOAbVqX9ozBLf8rSe2RsmQTvdq53vw-gPADqF-MQWZBouwwVgOeyQQk95LIp72b-x9a2ihlk4GhNacbmQgt07pb5mQ44YqJN9rLcR7-KyJCBrXstsMG_TzzsiiYgIC8M2js-btObXh3ieXj95tKABLLcnQGnwlwGFdwQQhcUGnzzoWP0UA_U3gm_h8JxkGOwIrUW630Z2JA)\n",
            "2. [lunabase.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG_ZXKEw2HOGUbP5Jgs05is1VYgX55im3rmdAmMinia7QSNJoSbqnUUpYCrjae4uf__L7y423FqLaNNBSRXelfFHU0XsFQJSxYPNVcfUZMMp9NGZ2TPwRN0cJE9wuzXPcQ_6sI5hCHZ7tBYoMgPDfzjPIn7hzrxNN_OOJnZww6mnflr6ejmyL8lZnFYS37pYYuoNwOu4p7Y8j7l)\n",
            "3. [howaiworks.ai](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHx5u3Xfaktpd8osEs1fFfZKmeCQacy_j3oVYZLCsAFgIRuAct3bUCDWJYC5mCKF1z4fEtZJ9HqOJODsZw9sLtPuiCHbFQ01oKefW5uRU8NAPeWFbgDf82HOxixcv8A4sNB0lM7Yq55uCgZTsRylChprMOdQ8LX69M=)\n",
            "4. [vktr.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHw5ie9ziMmTK2jZuWD5ykphcpHKn3IKIqVGHQT_znfFFjKmujVAJDAqRcEVg1c8zsH6x-OEX54_0KZDmQADRNcTeu3Y6DNmJZxHuBsrGbX3REqB-_1eQt2hhGDpLkBQnrSKY-6pkFIOXwYVKJKXm41VrTgpupbaCzAnIVl_Hu3rHoFqDVAMw==)\n",
            "5. [techpowerup.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFtpTRXiN-9Mlcz7-7YGQHOfpTtP4OR8EMyvhP8t0ZwlE9hooUw7y5ZAfRoZNhpzgpZXXMB39EyC73SUhF1DJaFQW2TrQwBITNcscS1Y2PRyXXqmbbuiAVAOdf9b3SmdnNATV2sJrMDYYDOvdGgwmActZtKB4LsPv12xg9JKyCrlavhBWlXAbZwKVZjfSpy-RDQJefEhqs=)\n",
            "6. [techpowerup.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHHc3yA1vI_50ukZJFCfV64Ewoyv0eyIDWceLY8bLovNTOEGn-OTofGs2REQcxSULu6qSr8rMJNbk8vYwYR7JssjAQJggvsO7MoLdT9nl1vivPvl5HuTjF1d8Kn0TDyOg==)\n",
            "7. [reddit.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFFg2_V8HNPq9aocg1wAscfe2MDqCowFOCzMEWghuDr6X1VzjRwtlfTfkmyTHHUoKhiGPYaLomoVcDzvUzBp3XqiUlgvJOhchPMtHDee84gXrDSGC-LofV1AJ3fxbt76qLUFWInwjbbKOUWeiA0-VdHikNl4_m2cTgMnj7LQ6Kt3I4nfjZodw6sTwG_ZPJ4YP7VllRSPz_s09Ypvtcw9Y_tUIPMWg==)\n",
            "8. [orhanergun.net](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNe5PvkveO6Zg8JQp5lJ0wQe0E9rbhGo041C0bdrrmSmtQv4DatYWGtlR43yhwF8s68MlsN-iUFAYG99gYzHdcwchcV9ishDiNoXx30onSD3zW3MGun2Ro6k4R16H_dVcgX0uEpO--5w4MG3CVXvOXj25gPSNVkgpcPcX0I0LjjQ3BkAKi)\n",
            "9. [wikipedia.org](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEFUuWixljB-BpjvUAErsMUBnTVozqeuKUrhfOzFgDhuRcplODnUHXZSEMfzLld-p6yfbY6EthWotK71-vqxG3G4Ugjmmupj3DVBoWdu0gzZzFVb5OLSKKBR24b3WBSdMESxuJcLUomeU-h)\n",
            "10. [google.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGgWkBF0KXHe_dlxdbtNkwN32h2orITIof-975f2GK43O5td6_aChORFhLRXuUUEc6JdXmUH4eO079ukJHwHN0JE-bFcjzMPgxyzZhxKpyud-SJ)\n",
            "11. [hpcwire.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFwgY1Fmj9rhm8QVYrRCUELakF7hNH2odZUb9zxZmZ8P_EJ32XDW9JCy1Wx7vKsq_Gif7rI23WLXr5X_KtXrFMUFlPzOXw4mPNGkt6zAJIhvxuU3yR-P_Gv28np_RDZ5_h_SdqbfZ2dfZ1noe1df0DgrukV_yoQOefjpvINdEaeLw6T6E_nk_SmjpApzu55EKg3-vBlWFXFdv8=)\n",
            "12. [google.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGGp7s6Lh5Rto1TIHacpps3OWYegIemlzTRDExxoqwSiZoOdoTBa17zPx36s5GTlp1MMYxfmqhll3oBtSwXjFnMfq1Oe9Z9_F9RTJgC3bduEAgllp2Oghf63ChM2Y3JkuftWy8buIVjwb24updo27c39a68ldyv3Pw5VH_mR1pJw9aSTE8isA==)\n",
            "13. [infoq.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHmoKJWcFGQsTNDQbdn4yiNPztrSNrDPeZ4UTpVRy6g9QPGI6GsplWMB3DXEDd2uM8RP7vNu7HWf2QV1cbtNyjXCfyEIXOqJ7ynsNFhxTkhbFWQvyzPVTdZ0Xz-nFwNiVwt4i7pnQOZOJDFTkNVh37eSeqf)\n",
            "14. [techradar.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGh2YFLQfmqkOX5VhxMv8zgbx_avJsuxhNzMdRmvJp4_vslIrMxxbhgjo9L3X5dt6KIKQ-B0cfjm1lud_8GQv_UJH3Adk318dstGTuq6aDqnrYbBtKzQvCUwEwYPUe-_w631aWdsJLLB_2CjH8pw7ysrxupi4TeonKl0qsp6rf0ljJ4qIZiY12OpJ730n4Nv2B4tUuUOZ6C6GhhG4FupM18rUeXRmVp3VCBN1Gg5apNgg1qJpziLR9gjBKLZaGYCjB4ZeNrETp7iA9BbWmCS7U=)\n",
            "15. [nextplatform.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH6QIxBXe3SolCJdGec6Vjb3RsxLY8Jr9NlgKg2HGhy7yRPI6yf7ElydXM3sxzh2H_BF165y4MSOrKcHWIAytl3okk8Nm-gqsq5EUVPCBbx8orJU-L330IKbw6vD8agc-WFhZ5sqde5vKRyV9PkNlJfJ_yh1FPok7mGNDUmhAnQgX0vURPf18wwPohuH8zUexJCp39M_dEc5G0=)\n",
            "16. [yolegroup.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4YgYnO5NBfzJ-nfl4hrM-Fto2vUPnD58uP5MCr0QHxfFrHMNty-8U-_0X8mgk45LC3ghxJ3J1_fuzE-HG68rT9O-yzswe-JTdEVZ2oWnwjHgMI-Vr4_fviyJ40-rQ7d_sAe9dedUHr0oJbRiqsz7BeYl2Wi6WuUKS8wuv68CXU7Pf2BASsTJWBwiCRKA_Ed0=)\n",
            "17. [introl.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHu7-0DIIXnQadN6ktwArYYuacqRbak-KflXNbOJYEkVVxtjxElUROcJUoYp-yqfi96RUGKVYbDBtRg_uol2FujEtEfOko46sKnLAD6x-qduoy0DIWUnK3jrGHC7v33ELepUt3fE46KcswHsZuu_RQMvST4E_1bQqKsVthK44Fl3NjB)\n",
            "18. [baogaobox.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuIHD2c_lneXLVTgFQYB7rCSIydCWEqlyj4qR8BX_LkGxU2D6TE2wvCy8SGeZDyCpHZ8OmxGEp1F53aHvdhKoeq5ntOBxo8ZLD5cQN9fupaNKSSlyiwlwLvlNV7hjw08ZY05xBsk7ngiCPwZ_8)\n",
            "19. [nevsemi.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF7Q8PUOl2se0X3zrvU8lmes8t_j6_UDzk-BM1m2-tSJJFXGbDb5qPegqWaVGokzztWdAw1Kl7LYGnzd9fL049as8dVQaDQLSkLsJagFaY2f3q6DNy1Ky4qTmY4g37t0y3SSOv5jzawGoHU1B4X2AM3U_DHZ3Bgsq4nr736uyWRYw==)\n",
            "20. [chromeunboxed.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFnai1gJ8p6RXY7wIxClwOrweOiMPk-6XK6Gt4i7m-qUy1PumxrIAMf3oLwy3837aVWSOVPF4z8Gc5pMfpY_IFTEa6IA_ed0eX8eZcOm1HF8k6i5hwyXFqOTzSSyIrl9Fn73hoKBIkvrZQTJIM8Z_DpfCf1IZVZAzRoF8-0tmZHF2vyoJ7WhaC8-35iLS3aeUNtaiIlpGY=)\n",
            "21. [investing.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFS7EQrurcyA4bVGQednR8xZrDx0_zya5K26F-TS8JB4X8Zy95Djj-DZ22mJ3EcOjo4OwqAXNgnSEjiGEwT-h2zsR7imRj2ldvlPbeyKVfcG8i3Ac-k4Hd7K1JMg9306UTTkZ2Doy_TwNYbytcE9KQqnzc-Jh5s_5Oj1kaoLoGCD6xVbNdYxGCe2mq85dgIPhhdDCAyjFkTCFWwTUXr-k3BXkKmyu8=)\n",
            "22. [siliconangle.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHID6L2of1Co6QokBRvCsVkcYZwfz3XRlByN0FZoSVeIv_6Q3tQoyC-Gpf3FmzDAIh_Z1QYzf0da3-F0zylD3chKYrY8umClw4_E1wc44SyFxeL66IvHzAhz3TdGk7it--zPKjGS9DcTo6MFtFtS2T6bTacH7s6d-DfTU7UDLs5RmGWLpZaAT6vg4BmqhrVGHPBTII01faryXLoAE4dQKiX)\n",
            "23. [techradar.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFbRE8KBMvGlOWlgAGlO0KGHyQ7mYHqjEwawjgVFItRlVEqw67XgBZX21ebaYHAu_pj7wynGHAfwAs0C6NnFm6q-pT_SaTHW4WNleUesA65hrICRm_Pfo1xgmGVyLJH8Y8C7xq3Zala4yoJsL1NC8xcHjYXhBwjhHPmm_wMo5FIsI3JcJIcjy0Wstm6lCj9FCbuzl00AkBRLiDlvXIkvx2LqFO4h8Jm9XcJLQ==)\n",
            "24. [futunn.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFHRvKygRFQ06_2kDg2zrky_0CVEE4rH5UlBZZNshwmtUMoTfoMKHCwOJPtCaL1FGHNm8iJDCCaNpSwKAQb4Voj_nfRIqv1EI7NlQWuJmmcQ_6CKZdkigVrM53juxV6th3WkLf-qVDOCg9HDCi6WUJf3bKZIPl4WBMMUNz3s_uf_IrFgcb0E0ppxLBJ8gg5ZaUQZSxgAMfRaNjaQt5zYOOpaEqh)\n",
            "25. [substack.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGM0iAdPn4roIFQjhyU6N0AdvxoNr0WLnOuoQpFT360EIY-mht9k_gUmX0-tUgzmWAAx6rcWH-tLUaijz3WOWfWfFOmvSkpE4UKUQoMjvKK99sKUJm--zPo0A0SR3GTzW7ifr2wnLBry5PE5n5aXZx0DUrft4bjD58Tkq2si4MMuzY=)\n",
            "26. [moomoo.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFT7HJ1yrBIPsnxF-URPpGWULd4lTfUxIpxBwoz1nRnBZikxp2DT_7CqKbdq_CZ8_QVmDDl-ztfFEkwz1FxRLAJzhvL7LXsBb63lEJTlAXUs0_6sUvSM3OGDFDSXmxIXXRjmYRfj8G8ZiSxsNchWtQ4n1SpoQEZwsiZhG-DUIG6c5Ud1-qxxXf7EuXyhzVHrIh-H-fEm6-T80Wz)\n",
            "27. [trendforce.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEZrmCcej_GGBdn3LPYk63kXTW4u1cCKpn2sqd5T4NvcxZCPuInmSnPuxBWLDxTe6AxYX3kFU-i3fYv_xKRWjesKAET0dCEnH36OoEBjcJ4XQmu4QeDGhxs_KC-oOVvAM-RjGyivZ-sirjbh219yhcVVjnixY-zjGW_cnVjJ4cZzrkyglWCsNy3UhFYf1QI4i67Ji54i1eDn5v2F4fV3kMiUAuG0lPfFi3JF7ZdabRLPhCVyeZHb4d3YlU4c4D8RAJKhRarbyU=)\n",
            "28. [semianalysis.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH2wIjwrhIA1cvHmctdQnNioUhLjeQdjRo_hRUBFaxarAyC1NEGko0eBR39AVrWRfmZ8d6HEg-Yq0W_WERMxkSECWX4dSSFxaxMJE0odApRj6XO4wv1phkJpkYFwmjkjPlxe8yRDteJctMYqfVowWC48fxidEvbNFln__tX1g==)\n",
            "29. [binaryverseai.com](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHQNYoKIC09ReJPDWERFF2gDm-jF9ddmikeE16X4jK3Uby3st_xzEV02T1w534O4s3DeOrC0XFfV32cZ8TgjIjJOK7MLZme8SOZh37gGZdJ1eqVhGQLJdGggoJ2Q2VyRcEHWhDe3IaU_xaJudm8toqbqSt-dFsQVnxu-5G01oR4KNQ=)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Function Calling (Tools)\n",
        "Define custom tools and use function calling within the Interactions API."
      ],
      "metadata": {
        "id": "function_calling_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the tool\n",
        "def get_weather(location: str):\n",
        "    \"\"\"Gets the weather for a given location.\"\"\"\n",
        "    return f\"The weather in {location} is sunny.\"\n",
        "\n",
        "weather_tool = {\n",
        "    \"type\": \"function\",\n",
        "    \"name\": \"get_weather\",\n",
        "    \"description\": \"Gets the weather for a given location.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}\n",
        "        },\n",
        "        \"required\": [\"location\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2. Send the request with tools\n",
        "interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"What is the weather in Paris?\",\n",
        "    tools=[weather_tool]\n",
        ")\n",
        "\n",
        "# 3. Handle the tool call\n",
        "for output in interaction.outputs:\n",
        "    if output.type == \"function_call\":\n",
        "        print(f\"Tool Call: {output.name}({output.arguments})\")\n",
        "\n",
        "        # Execute tool\n",
        "        result = get_weather(**output.arguments)\n",
        "\n",
        "        # Send result back\n",
        "        interaction = client.interactions.create(\n",
        "            model=\"gemini-3-flash-preview\",\n",
        "            previous_interaction_id=interaction.id,\n",
        "            input=[{\n",
        "                \"type\": \"function_result\",\n",
        "                \"name\": output.name,\n",
        "                \"call_id\": output.id,\n",
        "                \"result\": result\n",
        "            }]\n",
        "        )\n",
        "        print(f\"Response: {interaction.outputs[-1].text}\")"
      ],
      "metadata": {
        "id": "function_calling_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973c6a38-fafc-47fd-e26b-ec63a723480a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool Call: get_weather({'location': 'Paris'})\n",
            "Response: The weather in Paris is currently sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Structured Output (JSON Schema)\n",
        "Enforce a specific JSON output by providing a JSON schema in the `response_format` parameter. This is useful for tasks like moderation, classification, or data extraction."
      ],
      "metadata": {
        "id": "structured_output_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, Union\n",
        "\n",
        "class SpamDetails(BaseModel):\n",
        "    reason: str = Field(description=\"The reason why the content is considered spam.\")\n",
        "    spam_type: Literal[\"phishing\", \"scam\", \"unsolicited promotion\", \"other\"]\n",
        "\n",
        "class NotSpamDetails(BaseModel):\n",
        "    summary: str = Field(description=\"A brief summary of the content.\")\n",
        "    is_safe: bool = Field(description=\"Whether the content is safe for all audiences.\")\n",
        "\n",
        "class ModerationResult(BaseModel):\n",
        "    decision: Union[SpamDetails, NotSpamDetails]\n",
        "\n",
        "# Create interaction with structured output\n",
        "interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"Moderate the following content: 'Congratulations! You've won a free cruise. Click here to claim your prize: www.definitely-not-a-scam.com'\",\n",
        "    response_format=ModerationResult.model_json_schema(),\n",
        ")\n",
        "\n",
        "# Parse the output\n",
        "parsed_output = ModerationResult.model_validate_json(interaction.outputs[-1].text)\n",
        "print(f\"Moderation Result: {parsed_output}\")"
      ],
      "metadata": {
        "id": "structured_output_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac52eed-c628-407e-835e-2b6fe2762b8f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moderation Result: decision=SpamDetails(reason='The content uses a generic prize claim and a suspicious URL to lure users into a potential fraud scheme.', spam_type='scam')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Advanced Features"
      ],
      "metadata": {
        "id": "advanced_features_section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Streaming\n",
        "Receive responses incrementally as they are generated."
      ],
      "metadata": {
        "id": "streaming_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream responses\n",
        "stream = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"Explain quantum entanglement in simple terms.\",\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    if chunk.event_type == \"content.delta\":\n",
        "        if chunk.delta.type == \"text\":\n",
        "            print(chunk.delta.text, end=\"\", flush=True)\n",
        "        elif chunk.delta.type == \"thought\":\n",
        "            print(chunk.delta.thought, end=\"\", flush=True)\n",
        "    elif chunk.event_type == \"interaction.complete\":\n",
        "        print(f\"\\n\\n--- Stream Finished ---\")\n",
        "        print(f\"Total Tokens: {chunk.interaction.usage.total_tokens}\")"
      ],
      "metadata": {
        "id": "streaming_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144a26d2-6c0e-4010-a619-f6637bc224a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have two **magic coins**. These coins are special because they are \"entangled.\"\n",
            "\n",
            "Here is how they work:\n",
            "\n",
            "### 1. The Connection\n",
            "You keep one coin and give the other to a friend who travels to the other side of the galaxy. \n",
            "\n",
            "Normally, if you flip your coin, it has a 50/50 chance of being Heads or Tails. Your friend's coin would also have a 50/50 chance. What you get shouldn't affect what they get.\n",
            "\n",
            "But because these are **entangled**, the moment you flip your coin and see it is \"Heads,\" your friend’s coin **instantly** becomes \"Heads\" too—even though they are billions of miles away.\n",
            "\n",
            "### 2. The \"Spooky\" Part\n",
            "The weirdest part is that before you look at your coin, it isn't Heads or Tails. It is actually a blurry mix of both at the same time (scientists call this *superposition*). \n",
            "\n",
            "The two coins act like a single unit. They don't send a signal to each other through the air; they are simply linked by a fundamental law of the universe. Albert Einstein found this so strange that he famously called it **\"spooky action at a distance.\"**\n",
            "\n",
            "### 3. Why does this matter?\n",
            "While it sounds like science fiction, quantum entanglement is real. Scientists have proven it in labs many times. \n",
            "\n",
            "It is the \"engine\" behind some of the most exciting technology of the future:\n",
            "*   **Quantum Computers:** Computers that can solve problems in seconds that would take today’s supercomputers millions of years.\n",
            "*   **Unhackable Communication:** If someone tries to \"spy\" on an entangled signal, the link breaks, and the users instantly know they’re being watched.\n",
            "\n",
            "### Summary in one sentence:\n",
            "**Quantum entanglement is when two tiny particles become so deeply linked that whatever happens to one instantly happens to the other, no matter how far apart they are.**\n",
            "\n",
            "--- Stream Finished ---\n",
            "Total Tokens: 1020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Configuration\n",
        "Customize the model's behavior with `generation_config`."
      ],
      "metadata": {
        "id": "configuration_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Customize generation with config\n",
        "interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"Tell me a story about a brave knight.\",\n",
        "    generation_config={\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_output_tokens\": 500,\n",
        "        \"thinking_level\": \"low\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(interaction.outputs[-1].text)"
      ],
      "metadata": {
        "id": "configuration_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e9ec6c-a9bd-4f8d-eaa0-c40969281480"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the kingdom of Aethelgard, bravery was usually measured by the size of one’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 System Instructions\n",
        "Provide system-level instructions to guide model behavior."
      ],
      "metadata": {
        "id": "system_instructions_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use system instructions\n",
        "interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"What is the capital of France?\",\n",
        "    system_instruction=\"You are a poetic assistant. Answer only in rhymes.\"\n",
        ")\n",
        "\n",
        "print(interaction.outputs[-1].text)"
      ],
      "metadata": {
        "id": "system_instructions_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72cb4f4a-d7ae-491f-c3ec-d0ae4debbadf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The city of light, where romance is grand,\n",
            "Paris is the capital of that fair land.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Supported Models & Agents\n",
        "The Interactions API supports the following models and agents:\n",
        "\n",
        "**Models:**\n",
        "- `gemini-2.5-pro`\n",
        "- `gemini-2.5-flash`\n",
        "- `gemini-2.5-flash-lite`\n",
        "- `gemini-3-pro-preview`\n",
        "- `gemini-3-flash-preview`\n",
        "\n",
        "**Agents:**\n",
        "- `deep-research-pro-preview-12-2025`"
      ],
      "metadata": {
        "id": "supported_models"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. How the Interactions API Works\n",
        "\n",
        "The Interactions API is designed around a central resource: the **Interaction**.\n",
        "\n",
        "An Interaction represents a complete turn in a conversation or task. It acts as a session record, containing the entire history of an interaction, including:\n",
        "- All user inputs\n",
        "- Model thoughts\n",
        "- Tool calls\n",
        "- Tool results\n",
        "- Final model outputs\n",
        "\n",
        "When you make a call to `interactions.create`, you are creating a new Interaction resource.\n",
        "\n",
        "Optionally, you can use the `id` of this resource in a subsequent call using the `previous_interaction_id` parameter to continue the conversation. The server uses this ID to retrieve the full context, saving you from having to resend the entire chat history.\n",
        "\n",
        "This server-side state management is optional; you can also operate in stateless mode by sending the full conversation history in each request."
      ],
      "metadata": {
        "id": "how_it_works"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Comparison: Interactions API vs generate_content\n",
        "\n",
        "Here's a quick comparison to understand when to use each:"
      ],
      "metadata": {
        "id": "comparison_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traditional generate_content approach\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain what an API is in one sentence.\"\n",
        ")\n",
        "print(f\"generate_content: {response.text}\")\n",
        "\n",
        "# Interactions API approach\n",
        "interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"Explain what an API is in one sentence.\"\n",
        ")\n",
        "print(f\"Interactions API: {interaction.outputs[-1].text}\")"
      ],
      "metadata": {
        "id": "comparison_example",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e800f9a-18cd-4350-b9dd-0538dcce1c0d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate_content: An API (Application Programming Interface) is a set of definitions and protocols that allows different software applications to communicate and interact with each other.\n",
            "Interactions API: An API (Application Programming Interface) is a set of rules that allows different software applications to communicate and share data with each other.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we covered:\n",
        "\n",
        "1. **Basic Interactions** - Simple text prompts and responses\n",
        "2. **Multi-turn Conversations** - Both stateful (server-managed) and stateless (client-managed)\n",
        "3. **Multimodal Capabilities** - Image understanding with base64 encoding\n",
        "4. **Agentic Capabilities** - Deep Research Agent, Function Calling, and Structured Outputs\n",
        "5. **Advanced Features** - Streaming, Configuration, and System Instructions\n",
        "\n",
        "The Interactions API provides a powerful foundation for building sophisticated AI agents and applications!\n",
        "\n",
        "[Learn more in the official documentation](https://ai.google.dev/gemini-api/docs/interactions)"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}