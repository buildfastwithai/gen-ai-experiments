{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_a0Q3Fws0Ay"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-tbvLOFetmNkYmev7Ie55TSe035_Pmdr?usp=sharing)\n",
        "\n",
        "## Master Generative AI in 4 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "\n",
        "*Empowering the Next Generation of AI Innovators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoVdyYHbs0A0"
      },
      "source": [
        "# Fine-tune Open-Source LLMs on <a href=\"https://tokenfactory.nebius.com/\"><img src=\"https://mintcdn.com/nebius-723e8b65/jsgY7B_gdaTjMC6y/logo/Main-logo-TF-Dark.svg?fit=max&auto=format&n=jsgY7B_gdaTjMC6y&q=85&s=92ebc07d32d93f3918de2f7ec4a0754a\" width=\"200\"></a>\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Nebius Token Factory** is a powerful platform for fine-tuning large language models (LLMs) with LoRA adapters, enabling you to customize models for domain-specific tasks efficiently and cost-effectively.\n",
        "\n",
        "### Why Fine-Tuning?\n",
        "\n",
        "While general-purpose LLMs like Llama, Qwen, and DeepSeek are versatile, they often struggle with:\n",
        "- Domain-specific terminology and context\n",
        "- Highly structured or formatted outputs\n",
        "- Specialized task requirements\n",
        "- Custom function calling patterns\n",
        "\n",
        "**Fine-tuning solves these challenges by:**\n",
        "- Adapting models to your specific use case\n",
        "- Improving accuracy on specialized tasks\n",
        "- Reducing inference costs through smaller, focused models\n",
        "- Maintaining model quality while customizing behavior\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "In this notebook, you'll learn to:\n",
        "1. Prepare datasets for fine-tuning\n",
        "2. Configure LoRA hyperparameters\n",
        "3. Launch fine-tuning jobs on Nebius Token Factory\n",
        "4. Deploy custom LoRA adapters\n",
        "5. Evaluate fine-tuned model performance\n",
        "\n",
        "### About Nebius Token Factory\n",
        "\n",
        "**Nebius Token Factory** provides:\n",
        "- **Cost-Efficient Training** - LoRA adapters reduce training costs significantly\n",
        "- **Fast Deployment** - Deploy serverless LoRA models with per-token billing\n",
        "- **Multiple Models** - Support for Llama, Qwen, Mistral, and more\n",
        "- **W&B Integration** - Built-in Weights & Biases tracking\n",
        "- **OpenAI-Compatible API** - Easy integration with existing code\n",
        "\n",
        "---\n",
        "\n",
        "**Built with ‚ù§Ô∏è by BuildFastWithAI | Powered by Nebius Token Factory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAyTUDpcs0A1"
      },
      "source": [
        "## üìã Table of Contents\n",
        "\n",
        "1. [Setup & Installation](#setup)\n",
        "2. [Dataset Preparation](#dataset)\n",
        "3. [LoRA Fine-Tuning Configuration](#lora-config)\n",
        "4. [Launch Fine-Tuning Job](#fine-tuning)\n",
        "5. [Monitor Training Progress](#monitoring)\n",
        "6. [Deploy Custom Model](#deployment)\n",
        "7. [Inference & Evaluation](#inference)\n",
        "8. [Production Best Practices](#best-practices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOHsYMCWs0A2"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "## 1. Setup & Installation üîß\n",
        "\n",
        "First, let's install the required dependencies and set up authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cxhJxYAZs0A2"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install openai python-dotenv datasets pandas numpy tqdm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt2TOWAas0A3"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "80PT29iqs0A3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from openai import OpenAI\n",
        "from datasets import load_dataset, Dataset\n",
        "from tqdm import tqdm\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M34t97RZs0A3"
      },
      "source": [
        "### Configure API Access\n",
        "\n",
        "To use Nebius Token Factory, you need an API key:\n",
        "\n",
        "1. Sign up at [Nebius Token Factory](https://tokenfactory.nebius.com/)\n",
        "2. Generate an API key from your dashboard\n",
        "3. Set it as an environment variable\n",
        "\n",
        "**For Colab:** Use the Colab secrets manager or set directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcVERJm4s0A4",
        "outputId": "a45eb951-7eb4-4eae-824c-0871454a45f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nebius Token Factory client initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Using Colab userdata (recommended for Colab)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    NEBIUS_API_KEY = userdata.get('NEBIUS_API_KEY')\n",
        "except:\n",
        "    # Option 2: Using environment variables\n",
        "    load_dotenv()\n",
        "    NEBIUS_API_KEY = os.getenv('NEBIUS_API_KEY')\n",
        "\n",
        "    # Option 3: Set directly (not recommended for production)\n",
        "    if not NEBIUS_API_KEY:\n",
        "        NEBIUS_API_KEY = \"your_api_key_here\"  # Replace with your actual API key\n",
        "\n",
        "# Initialize Nebius Token Factory client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.tokenfactory.nebius.com/v1/\",\n",
        "    api_key=NEBIUS_API_KEY\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Nebius Token Factory client initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFEC6MbHs0A4"
      },
      "source": [
        "<a id=\"dataset\"></a>\n",
        "## 2. Dataset Preparation üìä\n",
        "\n",
        "### Dataset Format Requirements\n",
        "\n",
        "Nebius Token Factory expects datasets in **JSONL format** with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"messages\": [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is AI?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"AI stands for Artificial Intelligence...\"}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "- **Training Set:** 80-90% of your data (minimum 10 examples)\n",
        "- **Validation Set:** 10-20% of your data (optional but recommended)\n",
        "- **Quality over Quantity:** Well-crafted examples are better than many poor ones\n",
        "- **Diverse Examples:** Cover edge cases and variations in your domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSmxZ6H3s0A5"
      },
      "source": [
        "### Example 1: Function Calling Dataset\n",
        "\n",
        "Let's create a dataset for function calling - a common use case for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP3QzYhds0A5",
        "outputId": "cde3c598-3274-4482-8f26-ef45b541d66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 5 training examples\n",
            "\n",
            "Sample example:\n",
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"role\": \"system\",\n",
            "      \"content\": \"You are a helpful assistant with access to weather and calculation functions.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"What's the weather like in New York?\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"{\\\"function\\\": \\\"get_weather\\\", \\\"arguments\\\": {\\\"location\\\": \\\"New York\\\", \\\"unit\\\": \\\"celsius\\\"}}\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def create_function_calling_dataset():\n",
        "    \"\"\"\n",
        "    Create a sample dataset for function calling fine-tuning.\n",
        "    This teaches the model to properly format function calls.\n",
        "    \"\"\"\n",
        "\n",
        "    training_data = [\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to weather and calculation functions.\"},\n",
        "                {\"role\": \"user\", \"content\": \"What's the weather like in New York?\"},\n",
        "                {\"role\": \"assistant\", \"content\": '{\"function\": \"get_weather\", \"arguments\": {\"location\": \"New York\", \"unit\": \"celsius\"}}'}\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to weather and calculation functions.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Calculate 25 times 4\"},\n",
        "                {\"role\": \"assistant\", \"content\": '{\"function\": \"calculate\", \"arguments\": {\"operation\": \"multiply\", \"numbers\": [25, 4]}}'}\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to weather and calculation functions.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Is it going to rain in London tomorrow?\"},\n",
        "                {\"role\": \"assistant\", \"content\": '{\"function\": \"get_weather\", \"arguments\": {\"location\": \"London\", \"unit\": \"celsius\", \"forecast\": \"tomorrow\"}}'}\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to weather and calculation functions.\"},\n",
        "                {\"role\": \"user\", \"content\": \"What's 100 divided by 5?\"},\n",
        "                {\"role\": \"assistant\", \"content\": '{\"function\": \"calculate\", \"arguments\": {\"operation\": \"divide\", \"numbers\": [100, 5]}}'}\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to weather and calculation functions.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Tell me the temperature in Tokyo\"},\n",
        "                {\"role\": \"assistant\", \"content\": '{\"function\": \"get_weather\", \"arguments\": {\"location\": \"Tokyo\", \"unit\": \"celsius\"}}'}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return training_data\n",
        "\n",
        "# Create training dataset\n",
        "train_data = create_function_calling_dataset()\n",
        "print(f\"Created {len(train_data)} training examples\")\n",
        "print(\"\\nSample example:\")\n",
        "print(json.dumps(train_data[0], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bXAlr3ts0A5"
      },
      "source": [
        "### Example 2: Domain-Specific Knowledge Dataset\n",
        "\n",
        "Create a dataset for a specific domain (e.g., medical, legal, technical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wwd8C3_5s0A5"
      },
      "outputs": [],
      "source": [
        "def create_medical_qa_dataset():\n",
        "    \"\"\"\n",
        "    Create a sample medical Q&A dataset.\n",
        "    Note: This is for demonstration only. Use verified medical data in production.\n",
        "    \"\"\"\n",
        "\n",
        "    training_data = [\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a medical information assistant. Provide accurate, evidence-based information.\"},\n",
        "                {\"role\": \"user\", \"content\": \"What are the common symptoms of type 2 diabetes?\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"Common symptoms of type 2 diabetes include: increased thirst and frequent urination, increased hunger, unintended weight loss, fatigue, blurred vision, slow-healing sores, and frequent infections. However, many people with type 2 diabetes have no symptoms initially.\"}\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a medical information assistant. Provide accurate, evidence-based information.\"},\n",
        "                {\"role\": \"user\", \"content\": \"How is hypertension diagnosed?\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"Hypertension is diagnosed through blood pressure measurements. A diagnosis typically requires: multiple readings on different occasions, readings consistently at or above 140/90 mmHg, and ruling out white coat hypertension through ambulatory monitoring if necessary.\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return training_data\n",
        "\n",
        "# You can create domain-specific datasets for your use case\n",
        "# medical_data = create_medical_qa_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgBtr1tUs0A5"
      },
      "source": [
        "### Save Dataset to JSONL Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaXu70l_s0A6",
        "outputId": "df2e3228-a60a-4e28-f75d-0f7506353726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 4 examples to fine_tuning_train.jsonl\n",
            "‚úÖ Saved 1 examples to fine_tuning_validation.jsonl\n",
            "\n",
            "üìä Dataset Split:\n",
            "Training examples: 4\n",
            "Validation examples: 1\n"
          ]
        }
      ],
      "source": [
        "def save_to_jsonl(data: List[Dict], filename: str):\n",
        "    \"\"\"\n",
        "    Save dataset to JSONL format required by Nebius Token Factory.\n",
        "\n",
        "    Args:\n",
        "        data: List of conversation examples\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for example in data:\n",
        "            f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
        "    print(f\"‚úÖ Saved {len(data)} examples to {filename}\")\n",
        "\n",
        "# Split data into train and validation (80-20 split)\n",
        "split_idx = int(len(train_data) * 0.8)\n",
        "train_set = train_data[:split_idx]\n",
        "val_set = train_data[split_idx:]\n",
        "\n",
        "# Save datasets\n",
        "save_to_jsonl(train_set, \"fine_tuning_train.jsonl\")\n",
        "save_to_jsonl(val_set, \"fine_tuning_validation.jsonl\")\n",
        "\n",
        "print(f\"\\nüìä Dataset Split:\")\n",
        "print(f\"Training examples: {len(train_set)}\")\n",
        "print(f\"Validation examples: {len(val_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZZQfaVfs0A6"
      },
      "source": [
        "### Upload Datasets to Nebius Token Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFhMSbiWs0A6",
        "outputId": "e92f16dd-7b1c-4578-e4f7-4b67025eb32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading training dataset...\n",
            "‚úÖ Training file uploaded: file-019afdb4-f761-77e6-a357-027ef521dbad\n",
            "\n",
            "Uploading validation dataset...\n",
            "‚úÖ Validation file uploaded: file-019afdb4-f87b-7abf-bc60-543cca61e9d3\n"
          ]
        }
      ],
      "source": [
        "# Upload training dataset\n",
        "print(\"Uploading training dataset...\")\n",
        "training_file = client.files.create(\n",
        "    file=open(\"fine_tuning_train.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(f\"‚úÖ Training file uploaded: {training_file.id}\")\n",
        "\n",
        "# Upload validation dataset (optional but recommended)\n",
        "print(\"\\nUploading validation dataset...\")\n",
        "validation_file = client.files.create(\n",
        "    file=open(\"fine_tuning_validation.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(f\"‚úÖ Validation file uploaded: {validation_file.id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCwxeSKQs0A6"
      },
      "source": [
        "<a id=\"lora-config\"></a>\n",
        "## 3. LoRA Fine-Tuning Configuration ‚öôÔ∏è\n",
        "\n",
        "### Understanding LoRA Hyperparameters\n",
        "\n",
        "**LoRA (Low-Rank Adaptation)** is an efficient fine-tuning method that:\n",
        "- Trains only small adapter matrices instead of the entire model\n",
        "- Reduces training costs by up to 90%\n",
        "- Maintains model quality with proper configuration\n",
        "\n",
        "#### Key Hyperparameters:\n",
        "\n",
        "1. **lora_r (Rank)**: Size of the low-rank matrices\n",
        "   - Higher = More capacity but higher cost\n",
        "   - Recommended: 8-32 for most tasks\n",
        "   - Use 16-64 for complex domain adaptation\n",
        "\n",
        "2. **lora_alpha**: Scaling factor for LoRA weights\n",
        "   - Typically set equal to lora_r\n",
        "   - Controls influence of fine-tuning on base model\n",
        "\n",
        "3. **lora_dropout**: Regularization to prevent overfitting\n",
        "   - Range: 0.0 - 0.1\n",
        "   - Higher for smaller datasets\n",
        "\n",
        "4. **batch_size**: Number of examples per training step\n",
        "   - Larger = More stable but higher memory\n",
        "   - Recommended: 8-32\n",
        "\n",
        "5. **n_epochs**: Number of training passes\n",
        "   - Start with 3-5 epochs\n",
        "   - Monitor validation loss to avoid overfitting\n",
        "\n",
        "6. **learning_rate**: Controls update step size\n",
        "   - Default: 1e-5\n",
        "   - Increase for new domains, decrease for fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLXFLMxps0A6"
      },
      "source": [
        "### Configuration Presets\n",
        "\n",
        "Here are recommended configurations for different scenarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFlaEMTYs0A6",
        "outputId": "e15a9460-906c-49a5-f368-fdb677c6d4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Configuration Presets:\n",
            "\n",
            "üìã QUICK_TEST\n",
            "   Fast training for testing (~$1-2)\n",
            "   Config: {\n",
            "  \"n_epochs\": 1,\n",
            "  \"batch_size\": 16,\n",
            "  \"lora\": true,\n",
            "  \"lora_r\": 8,\n",
            "  \"lora_alpha\": 8,\n",
            "  \"lora_dropout\": 0.05\n",
            "}\n",
            "\n",
            "üìã STANDARD\n",
            "   Balanced quality and cost (~$5-10)\n",
            "   Config: {\n",
            "  \"n_epochs\": 3,\n",
            "  \"batch_size\": 32,\n",
            "  \"lora\": true,\n",
            "  \"lora_r\": 16,\n",
            "  \"lora_alpha\": 16,\n",
            "  \"lora_dropout\": 0.05\n",
            "}\n",
            "\n",
            "üìã HIGH_QUALITY\n",
            "   Best quality for production (~$15-25)\n",
            "   Config: {\n",
            "  \"n_epochs\": 5,\n",
            "  \"batch_size\": 32,\n",
            "  \"lora\": true,\n",
            "  \"lora_r\": 32,\n",
            "  \"lora_alpha\": 32,\n",
            "  \"lora_dropout\": 0.05,\n",
            "  \"learning_rate\": 1e-05\n",
            "}\n",
            "\n",
            "üìã DOMAIN_ADAPTATION\n",
            "   For completely new domains (~$20-30)\n",
            "   Config: {\n",
            "  \"n_epochs\": 5,\n",
            "  \"batch_size\": 32,\n",
            "  \"lora\": true,\n",
            "  \"lora_r\": 64,\n",
            "  \"lora_alpha\": 64,\n",
            "  \"lora_dropout\": 0.1,\n",
            "  \"learning_rate\": 3e-05\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Configuration presets for different use cases\n",
        "\n",
        "PRESETS = {\n",
        "    \"quick_test\": {\n",
        "        \"description\": \"Fast training for testing (~$1-2)\",\n",
        "        \"hyperparameters\": {\n",
        "            \"n_epochs\": 1,\n",
        "            \"batch_size\": 16,\n",
        "            \"lora\": True,\n",
        "            \"lora_r\": 8,\n",
        "            \"lora_alpha\": 8,\n",
        "            \"lora_dropout\": 0.05\n",
        "        }\n",
        "    },\n",
        "    \"standard\": {\n",
        "        \"description\": \"Balanced quality and cost (~$5-10)\",\n",
        "        \"hyperparameters\": {\n",
        "            \"n_epochs\": 3,\n",
        "            \"batch_size\": 32,\n",
        "            \"lora\": True,\n",
        "            \"lora_r\": 16,\n",
        "            \"lora_alpha\": 16,\n",
        "            \"lora_dropout\": 0.05\n",
        "        }\n",
        "    },\n",
        "    \"high_quality\": {\n",
        "        \"description\": \"Best quality for production (~$15-25)\",\n",
        "        \"hyperparameters\": {\n",
        "            \"n_epochs\": 5,\n",
        "            \"batch_size\": 32,\n",
        "            \"lora\": True,\n",
        "            \"lora_r\": 32,\n",
        "            \"lora_alpha\": 32,\n",
        "            \"lora_dropout\": 0.05,\n",
        "            \"learning_rate\": 0.00001\n",
        "        }\n",
        "    },\n",
        "    \"domain_adaptation\": {\n",
        "        \"description\": \"For completely new domains (~$20-30)\",\n",
        "        \"hyperparameters\": {\n",
        "            \"n_epochs\": 5,\n",
        "            \"batch_size\": 32,\n",
        "            \"lora\": True,\n",
        "            \"lora_r\": 64,\n",
        "            \"lora_alpha\": 64,\n",
        "            \"lora_dropout\": 0.1,\n",
        "            \"learning_rate\": 0.00003\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display presets\n",
        "print(\"Available Configuration Presets:\\n\")\n",
        "for name, config in PRESETS.items():\n",
        "    print(f\"üìã {name.upper()}\")\n",
        "    print(f\"   {config['description']}\")\n",
        "    print(f\"   Config: {json.dumps(config['hyperparameters'], indent=2)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84BSGiIOs0A6"
      },
      "source": [
        "<a id=\"fine-tuning\"></a>\n",
        "## 4. Launch Fine-Tuning Job üöÄ\n",
        "\n",
        "### Supported Models\n",
        "\n",
        "Nebius Token Factory supports fine-tuning for:\n",
        "- **Llama 3.1** (8B, 70B) - Best for instruction following\n",
        "- **Qwen 2.5** (7B, 14B, 32B, 72B) - Multilingual, strong reasoning\n",
        "- **Mistral** (7B) - Efficient and fast\n",
        "- **DeepSeek** - Code and reasoning tasks\n",
        "\n",
        "Check [Nebius Models](https://docs.tokenfactory.nebius.com/fine-tuning/models) for the latest supported models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMpAo7r0s0A7",
        "outputId": "ac6ba48b-5e41-4063-e1a3-366d5c4413e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Configuration: STANDARD\n",
            "ü§ñ Model: meta-llama/Llama-3.1-8B-Instruct\n",
            "\n",
            "Balanced quality and cost (~$5-10)\n"
          ]
        }
      ],
      "source": [
        "# Choose your configuration preset\n",
        "SELECTED_PRESET = \"standard\"  # Change to: quick_test, standard, high_quality, or domain_adaptation\n",
        "\n",
        "# Select model\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # Options: meta-llama/Llama-3.1-8B-Instruct, Qwen/Qwen2.5-7B-Instruct, etc.\n",
        "\n",
        "print(f\"üîß Configuration: {SELECTED_PRESET.upper()}\")\n",
        "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
        "print(f\"\\n{PRESETS[SELECTED_PRESET]['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le4vJ9W_s0A7"
      },
      "source": [
        "### Create Fine-Tuning Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVbT8OITs0A7",
        "outputId": "41f222c8-764a-4d21-990d-3f63944bbba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Launching fine-tuning job...\n",
            "\n",
            "‚úÖ Fine-tuning job created successfully!\n",
            "\n",
            "Job ID: ftjob-4b50e7e54fe84bbcb6a45a3b62b9be8a\n",
            "Model: meta-llama/Llama-3.1-8B-Instruct\n",
            "Status: running\n",
            "\n",
            "Hyperparameters:\n",
            "{\n",
            "  \"batch_size\": 32,\n",
            "  \"learning_rate_multiplier\": null,\n",
            "  \"n_epochs\": 3,\n",
            "  \"learning_rate\": 1e-05,\n",
            "  \"warmup_ratio\": 0.0,\n",
            "  \"weight_decay\": 0.0,\n",
            "  \"lora\": true,\n",
            "  \"lora_r\": 16,\n",
            "  \"lora_alpha\": 16,\n",
            "  \"lora_dropout\": 0.05,\n",
            "  \"packing\": true,\n",
            "  \"max_grad_norm\": 1.0,\n",
            "  \"context_length\": 8192\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Launch fine-tuning job\n",
        "print(\"üöÄ Launching fine-tuning job...\\n\")\n",
        "\n",
        "ft_job = client.fine_tuning.jobs.create(\n",
        "    training_file=training_file.id,\n",
        "    validation_file=validation_file.id,\n",
        "    model=MODEL_NAME,\n",
        "    suffix=\"buildfast-experiment\",  # Custom name for your model\n",
        "    hyperparameters=PRESETS[SELECTED_PRESET][\"hyperparameters\"],\n",
        "    seed=42  # For reproducibility\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Fine-tuning job created successfully!\\n\")\n",
        "print(f\"Job ID: {ft_job.id}\")\n",
        "print(f\"Model: {ft_job.model}\")\n",
        "print(f\"Status: {ft_job.status}\")\n",
        "print(f\"\\nHyperparameters:\")\n",
        "print(json.dumps(dict(ft_job.hyperparameters), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX2gVPats0A7"
      },
      "source": [
        "<a id=\"monitoring\"></a>\n",
        "## 5. Monitor Training Progress üìä\n",
        "\n",
        "Training can take 15-60 minutes depending on:\n",
        "- Dataset size\n",
        "- Model size\n",
        "- Number of epochs\n",
        "- LoRA rank\n",
        "\n",
        "The cell below will monitor progress and update every 15 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "976zdENWs0A7",
        "outputId": "5554be21-8ee3-4671-dc9c-690b5db39a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Monitoring fine-tuning job...\n",
            "\n",
            "Status updates:\n",
            "--------------------------------------------------\n",
            "[0s] Status: running\n",
            "[16s] Status: running\n",
            "[31s] Status: running\n",
            "[46s] Status: running\n",
            "[62s] Status: running\n",
            "[77s] Status: running\n",
            "[93s] Status: running\n",
            "[108s] Status: running | Tokens trained: 16,204 | ETA: 1765193281\n",
            "[124s] Status: running | Tokens trained: 48,612 | ETA: 1765193279\n",
            "[139s] Status: running | Tokens trained: 48,612 | ETA: 1765193279\n",
            "[155s] Status: running | Tokens trained: 48,612 | ETA: 1765193279\n",
            "[170s] Status: running | Tokens trained: 48,612 | ETA: 1765193279\n",
            "[186s] Status: succeeded | Tokens trained: 48,612 | ETA: 1765193279\n",
            "--------------------------------------------------\n",
            "\n",
            "‚úÖ Fine-tuning completed successfully!\n",
            "Fine-tuned model: None\n",
            "Total training time: 186s (3.1 minutes)\n"
          ]
        }
      ],
      "source": [
        "def monitor_fine_tuning_job(job_id: str, check_interval: int = 15):\n",
        "    \"\"\"\n",
        "    Monitor the fine-tuning job status.\n",
        "\n",
        "    Args:\n",
        "        job_id: Fine-tuning job ID\n",
        "        check_interval: Seconds between status checks\n",
        "    \"\"\"\n",
        "    active_statuses = [\"validating_files\", \"queued\", \"running\"]\n",
        "\n",
        "    print(\"üìä Monitoring fine-tuning job...\\n\")\n",
        "    print(\"Status updates:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        job = client.fine_tuning.jobs.retrieve(job_id)\n",
        "        elapsed = int(time.time() - start_time)\n",
        "\n",
        "        print(f\"[{elapsed}s] Status: {job.status}\", end=\"\")\n",
        "\n",
        "        if job.trained_tokens:\n",
        "            print(f\" | Tokens trained: {job.trained_tokens:,}\", end=\"\")\n",
        "\n",
        "        if job.estimated_finish:\n",
        "            print(f\" | ETA: {job.estimated_finish}\", end=\"\")\n",
        "\n",
        "        print()  # New line\n",
        "\n",
        "        if job.status not in active_statuses:\n",
        "            print(\"-\" * 50)\n",
        "            if job.status == \"succeeded\":\n",
        "                print(f\"\\n‚úÖ Fine-tuning completed successfully!\")\n",
        "                print(f\"Fine-tuned model: {job.fine_tuned_model}\")\n",
        "                print(f\"Total training time: {elapsed}s ({elapsed/60:.1f} minutes)\")\n",
        "            elif job.status == \"failed\":\n",
        "                print(f\"\\n‚ùå Fine-tuning failed: {job.error}\")\n",
        "            else:\n",
        "                print(f\"\\n‚ö†Ô∏è Fine-tuning ended with status: {job.status}\")\n",
        "            break\n",
        "\n",
        "        time.sleep(check_interval)\n",
        "\n",
        "    return job\n",
        "\n",
        "# Start monitoring\n",
        "completed_job = monitor_fine_tuning_job(ft_job.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWqgAPkQs0A7"
      },
      "source": [
        "### View Training Metrics (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POtY2VM-s0A7",
        "outputId": "9407b2ce-2740-41dd-887a-6e85fa9f2a1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìà Training Summary:\n",
            "\n",
            "Job ID: ftjob-4b50e7e54fe84bbcb6a45a3b62b9be8a\n",
            "Base Model: meta-llama/Llama-3.1-8B-Instruct\n",
            "Fine-tuned Model: None\n",
            "Status: succeeded\n",
            "Total Tokens Trained: 48,612\n",
            "\n",
            "Hyperparameters Used:\n",
            "  batch_size: 32\n",
            "  learning_rate_multiplier: None\n",
            "  n_epochs: 3\n",
            "  learning_rate: 1e-05\n",
            "  warmup_ratio: 0.0\n",
            "  weight_decay: 0.0\n",
            "  lora: True\n",
            "  lora_r: 16\n",
            "  lora_alpha: 16\n",
            "  lora_dropout: 0.05\n",
            "  packing: True\n",
            "  max_grad_norm: 1.0\n",
            "  context_length: 8192\n"
          ]
        }
      ],
      "source": [
        "# Retrieve final job details\n",
        "final_job = client.fine_tuning.jobs.retrieve(ft_job.id)\n",
        "\n",
        "print(\"üìà Training Summary:\\n\")\n",
        "print(f\"Job ID: {final_job.id}\")\n",
        "print(f\"Base Model: {final_job.model}\")\n",
        "print(f\"Fine-tuned Model: {final_job.fine_tuned_model}\")\n",
        "print(f\"Status: {final_job.status}\")\n",
        "print(f\"Total Tokens Trained: {final_job.trained_tokens:,}\")\n",
        "print(f\"\\nHyperparameters Used:\")\n",
        "for key, value in dict(final_job.hyperparameters).items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzYYcsBus0A7"
      },
      "source": [
        "<a id=\"deployment\"></a>\n",
        "## 6. Deploy Custom Model üåê\n",
        "\n",
        "Your fine-tuned LoRA adapter is automatically deployed and ready to use!\n",
        "\n",
        "### Serverless Deployment Benefits:\n",
        "- ‚ö° **No infrastructure management** - Just call the API\n",
        "- üí∞ **Pay per token** - Only pay for what you use\n",
        "- üîÑ **Instant scaling** - Handle variable workloads\n",
        "- üöÄ **Low latency** - Optimized inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mfIZNcns0A7",
        "outputId": "aba17501-d528-4fa9-b621-1f9519a662ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Your fine-tuned model is deployed!\n",
            "\n",
            "Model ID: meta-llama/Meta-Llama-3.1-8B-Instruct-LoRa:buildfast-experiment-IiJa\n",
            "\n",
            "Use this model ID for inference requests.\n"
          ]
        }
      ],
      "source": [
        "# Your fine-tuned model ID\n",
        "fine_tuned_model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct-LoRa:buildfast-experiment-IiJa\"\n",
        "\n",
        "print(f\"üéØ Your fine-tuned model is deployed!\\n\")\n",
        "print(f\"Model ID: {fine_tuned_model_id}\")\n",
        "print(f\"\\nUse this model ID for inference requests.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPsxzfUas0A8"
      },
      "source": [
        "<a id=\"inference\"></a>\n",
        "## 7. Inference & Evaluation üß™\n",
        "\n",
        "Now let's test your fine-tuned model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TKr6wCis0A8"
      },
      "source": [
        "### Compare Base vs Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1BYjW5Us0A8",
        "outputId": "33af5b4c-4477-4bb8-cd99-89523d129b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Prompt: 'What's the weather in Paris?'\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üìò BASE MODEL RESPONSE:\n",
            "\n",
            "Unfortunately, I'm a large language model, I don't have real-time access to current weather conditions. However, I can suggest some ways for you to find out the current weather in Paris.\n",
            "\n",
            "1. **Check online weather websites**: You can check websites like AccuWeather, Weather.com, or BBC Weather for the current weather conditions in Paris.\n",
            "2. **Use a weather app**: Download a weather app like Dark Sky, Weather Underground, or The Weather Channel on your smartphone to get the current weather conditions in Paris.\n",
            "3. **Ask a voice assistant**: If you have a smart speaker or virtual assistant like Siri, Google Assistant, or Alexa, you can ask them to tell you the current weather in Paris.\n",
            "\n",
            "If you want, I can also give you a general idea of the typical weather conditions in Paris at different times of the year. Just let me know!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üéØ FINE-TUNED MODEL RESPONSE:\n",
            "\n",
            "Let me check the current weather conditions in Paris for you.\n",
            "\n",
            "As of my knowledge cutoff (2023), I can provide you with the following information:\n",
            "\n",
            "Paris, France has a temperate oceanic climate. The current weather conditions in Paris might be affected by the season you're inquiring about.\n",
            "\n",
            "If you could specify the month or season you're interested in, I can provide you with more detailed information.\n",
            "\n",
            "However, if you want a general idea, I can give you some typical weather conditions for different times of the year:\n",
            "\n",
            "- **Spring (March to May)**: Cool and rainy, with average highs around 12-17¬∞C (54-63¬∞F).\n",
            "- **Summer (June to August)**: Warm and sunny, with average highs around 23-26¬∞C (73-79¬∞F).\n",
            "- **Autumn (September to November)**: Cool and rainy, with average highs around 12-17¬∞C (54-63¬∞F).\n",
            "- **Winter (December to February)**: Cold\n",
            "\n",
            "================================================================================\n",
            "\n",
            "################################################################################\n",
            "\n",
            "üß™ Testing Prompt: 'Calculate 45 plus 67'\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üìò BASE MODEL RESPONSE:\n",
            "\n",
            "45 + 67 = 112\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üéØ FINE-TUNED MODEL RESPONSE:\n",
            "\n",
            "To calculate 45 plus 67, I will use a simple arithmetic operation.\n",
            "\n",
            "45 + 67 = 112\n",
            "\n",
            "================================================================================\n",
            "\n",
            "################################################################################\n",
            "\n",
            "üß™ Testing Prompt: 'Is it sunny in Miami?'\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üìò BASE MODEL RESPONSE:\n",
            "\n",
            "Unfortunately, I'm a large language model, I don't have real-time access to current weather conditions. However, I can suggest a few ways to find out if it's sunny in Miami:\n",
            "\n",
            "1. Check a weather website or app: You can check websites like weather.com, accuweather.com, or wunderground.com for the current weather conditions in Miami.\n",
            "2. Use a virtual assistant: If you have a virtual assistant like Siri, Google Assistant, or Alexa, you can ask them to check the weather in Miami.\n",
            "3. Check social media: You can also check social media platforms like Twitter or Facebook to see if any of your friends or family members in Miami have posted about the weather.\n",
            "\n",
            "That being said, Miami is known for its sunny weather year-round, so it's likely that it's sunny in Miami even if I don't know the current conditions!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üéØ FINE-TUNED MODEL RESPONSE:\n",
            "\n",
            "To determine the current weather conditions in Miami, I would need access to real-time weather data. However, I can suggest some alternatives to help you find out.\n",
            "\n",
            "You can try the following options:\n",
            "\n",
            "1. Check online weather websites like AccuWeather or Weather.com for the current weather conditions in Miami.\n",
            "2. Use a virtual assistant like Siri, Google Assistant, or Alexa to ask about the current weather in Miami.\n",
            "3. Check the official website of the National Weather Service (NWS) for weather forecasts and conditions in Miami.\n",
            "\n",
            "If you want a general idea of the weather in Miami based on historical data, Miami has a subtropical climate with mild temperatures and high humidity levels throughout the year. The city experiences a tropical rainforest climate, with most of its rainfall occurring during the summer months (May to October).\n",
            "\n",
            "If you provide me with a specific date or time range, I can try to give you a general idea of the weather conditions in Miami.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "################################################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def compare_models(prompt: str, base_model: str, fine_tuned_model: str):\n",
        "    \"\"\"\n",
        "    Compare responses from base and fine-tuned models.\n",
        "\n",
        "    Args:\n",
        "        prompt: Test prompt\n",
        "        base_model: Base model ID\n",
        "        fine_tuned_model: Fine-tuned model ID\n",
        "    \"\"\"\n",
        "    print(f\"üß™ Testing Prompt: '{prompt}'\\n\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Base model response\n",
        "    print(\"\\nüìò BASE MODEL RESPONSE:\\n\")\n",
        "    base_response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    print(base_response.choices[0].message.content)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "    # Fine-tuned model response\n",
        "    print(\"\\nüéØ FINE-TUNED MODEL RESPONSE:\\n\")\n",
        "    ft_response = client.chat.completions.create(\n",
        "        model=fine_tuned_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to weather and calculation functions.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    print(ft_response.choices[0].message.content)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Test prompts relevant to your fine-tuning task\n",
        "test_prompts = [\n",
        "    \"What's the weather in Paris?\",\n",
        "    \"Calculate 45 plus 67\",\n",
        "    \"Is it sunny in Miami?\"\n",
        "]\n",
        "\n",
        "# Compare models\n",
        "for prompt in test_prompts:\n",
        "    compare_models(prompt, MODEL_NAME, fine_tuned_model_id)\n",
        "    print(\"\\n\" + \"#\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvYUPIX2s0A8"
      },
      "source": [
        "### Quantitative Evaluation\n",
        "\n",
        "For production, implement proper evaluation metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xKgsazBCs0A8"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model_id: str, test_set: List[Dict], verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Evaluate model on a test set.\n",
        "\n",
        "    Args:\n",
        "        model_id: Model to evaluate\n",
        "        test_set: List of test examples\n",
        "        verbose: Print detailed results\n",
        "\n",
        "    Returns:\n",
        "        Dict with evaluation metrics\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        \"total\": len(test_set),\n",
        "        \"correct\": 0,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüß™ Evaluating {model_id}...\\n\")\n",
        "\n",
        "    for i, example in enumerate(tqdm(test_set, desc=\"Evaluating\")):\n",
        "        try:\n",
        "            # Get model response\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=example[\"messages\"][:-1],  # All except the last (expected) message\n",
        "                max_tokens=150,\n",
        "                temperature=0.1  # Low temperature for consistent evaluation\n",
        "            )\n",
        "\n",
        "            predicted = response.choices[0].message.content\n",
        "            expected = example[\"messages\"][-1][\"content\"]\n",
        "\n",
        "            # Simple exact match (you can implement more sophisticated metrics)\n",
        "            if predicted.strip() == expected.strip():\n",
        "                results[\"correct\"] += 1\n",
        "            else:\n",
        "                results[\"errors\"].append({\n",
        "                    \"example\": i,\n",
        "                    \"expected\": expected,\n",
        "                    \"predicted\": predicted\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            results[\"errors\"].append({\"example\": i, \"error\": str(e)})\n",
        "\n",
        "    # Calculate metrics\n",
        "    results[\"accuracy\"] = results[\"correct\"] / results[\"total\"] if results[\"total\"] > 0 else 0\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nüìä Evaluation Results:\")\n",
        "    print(f\"Total Examples: {results['total']}\")\n",
        "    print(f\"Correct: {results['correct']}\")\n",
        "    print(f\"Accuracy: {results['accuracy']:.2%}\")\n",
        "\n",
        "    if verbose and results[\"errors\"]:\n",
        "        print(f\"\\nErrors: {len(results['errors'])}\")\n",
        "        for error in results[\"errors\"][:3]:  # Show first 3 errors\n",
        "            print(f\"\\nExample {error.get('example', 'N/A')}:\")\n",
        "            if 'expected' in error:\n",
        "                print(f\"Expected: {error['expected'][:100]}...\")\n",
        "                print(f\"Predicted: {error['predicted'][:100]}...\")\n",
        "            else:\n",
        "                print(f\"Error: {error.get('error', 'Unknown')}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# If you have a separate test set, evaluate here\n",
        "# test_results = evaluate_model(fine_tuned_model_id, test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL0oJ0Nos0A8"
      },
      "source": [
        "### Production-Ready Inference Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2fjG0y3s0A8",
        "outputId": "81021a60-36af-4335-eef1-609addad0d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What's the temperature in London?\n",
            "Response: However, I'm a large language model, I don't have real-time access to current weather information. But I can suggest some options to find the current temperature in London:\n",
            "\n",
            "1. Check online weather websites: You can visit websites like AccuWeather, Weather.com, or BBC Weather to get the current temperature in London.\n",
            "2. Use a weather app: Download a weather app on your smartphone, such as Dark Sky or Weather Underground, to get the current temperature in London.\n",
            "3. Check social media: Follow the official Twitter accounts of weather services, such as the Met Office (UK's national weather service), to get the current temperature in London.\n",
            "\n",
            "If you want, I can also provide you with the average temperature in London throughout the year, or information on the best time to visit London based on its climate. Just let me know!\n"
          ]
        }
      ],
      "source": [
        "def generate_with_fine_tuned_model(\n",
        "    prompt: str,\n",
        "    model_id: str = None,\n",
        "    system_prompt: str = \"You are a helpful assistant.\",\n",
        "    max_tokens: int = 500,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.95\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Production-ready inference function.\n",
        "\n",
        "    Args:\n",
        "        prompt: User input\n",
        "        model_id: Fine-tuned model ID\n",
        "        system_prompt: System message\n",
        "        max_tokens: Maximum response length\n",
        "        temperature: Sampling temperature\n",
        "        top_p: Nucleus sampling parameter\n",
        "\n",
        "    Returns:\n",
        "        Model response\n",
        "    \"\"\"\n",
        "    if model_id is None:\n",
        "        model_id = fine_tuned_model_id\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_id,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Test the function\n",
        "test_prompt = \"What's the temperature in London?\"\n",
        "result = generate_with_fine_tuned_model(test_prompt)\n",
        "\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Response: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfpib9kAs0A8"
      },
      "source": [
        "<a id=\"best-practices\"></a>\n",
        "## 8. Production Best Practices üéØ\n",
        "\n",
        "### 1. Dataset Quality\n",
        "- ‚úÖ Use diverse, high-quality examples\n",
        "- ‚úÖ Cover edge cases and variations\n",
        "- ‚úÖ Ensure consistency in formatting\n",
        "- ‚úÖ Minimum 50-100 examples for good results\n",
        "- ‚úÖ Include validation set for monitoring\n",
        "\n",
        "### 2. Hyperparameter Tuning\n",
        "- Start with standard preset\n",
        "- Monitor validation loss\n",
        "- Increase LoRA rank for complex tasks\n",
        "- Use early stopping to prevent overfitting\n",
        "- Experiment with learning rate for new domains\n",
        "\n",
        "### 3. Model Selection\n",
        "- **Llama 3.1-8B**: Best for instruction following\n",
        "- **Qwen 2.5**: Great for multilingual tasks\n",
        "- **Larger models**: Better for complex reasoning\n",
        "- Balance model size vs. inference cost\n",
        "\n",
        "### 4. Cost Optimization\n",
        "- Start with smaller datasets for testing\n",
        "- Use LoRA instead of full fine-tuning\n",
        "- Monitor training progress and stop early if converged\n",
        "- Use serverless deployment for variable workloads\n",
        "\n",
        "### 5. Evaluation & Monitoring\n",
        "- Implement proper evaluation metrics\n",
        "- Compare against base model\n",
        "- A/B test in production\n",
        "- Monitor for model drift\n",
        "- Collect user feedback\n",
        "\n",
        "### 6. Integration Tips\n",
        "```python\n",
        "# Save model ID securely\n",
        "FINE_TUNED_MODEL_ID = \"your-model-id\"\n",
        "\n",
        "# Use environment variables\n",
        "os.environ['FINE_TUNED_MODEL'] = FINE_TUNED_MODEL_ID\n",
        "\n",
        "# Implement retry logic\n",
        "from tenacity import retry, wait_exponential\n",
        "\n",
        "@retry(wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def call_model(prompt):\n",
        "    return client.chat.completions.create(...)\n",
        "```\n",
        "\n",
        "### 7. Common Pitfalls to Avoid\n",
        "- ‚ùå Overfitting on small datasets\n",
        "- ‚ùå Not using validation sets\n",
        "- ‚ùå Inconsistent data formatting\n",
        "- ‚ùå Skipping evaluation before deployment\n",
        "- ‚ùå Not monitoring production performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FNS7VPTs0BE"
      },
      "source": [
        "## üéì Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned an LLM with Nebius Token Factory. Here's what to explore next:\n",
        "\n",
        "1. **Expand Your Dataset**: Collect more domain-specific examples\n",
        "2. **Experiment with Models**: Try different base models\n",
        "3. **Optimize Hyperparameters**: Fine-tune LoRA configuration\n",
        "4. **Deploy to Production**: Integrate with your application\n",
        "5. **Monitor & Iterate**: Track performance and retrain as needed\n",
        "\n",
        "### Resources\n",
        "\n",
        "- üìö [Nebius Token Factory Docs](https://docs.tokenfactory.nebius.com/)\n",
        "- üç≥ [Token Factory Cookbook](https://github.com/nebius/token-factory-cookbook)\n",
        "- üí¨ [Nebius Discord Community](https://discord.com/invite/WJ2DUQRz4m)\n",
        "- üê¶ [Follow @nebiustf](https://x.com/nebiustf)\n",
        "- üìñ [Fine-Tuning Blog Post](https://nebius.com/blog/posts/fine-tuning-llms-with-nebius-ai-studio)\n",
        "\n",
        "### Connect With BuildFastWithAI\n",
        "\n",
        "- üåê [Website](https://www.buildfastwithai.com/)\n",
        "- üê¶ [Twitter](https://x.com/BuildFastWithAI)\n",
        "- üíº [LinkedIn](https://www.linkedin.com/company/build-fast-with-ai)\n",
        "- üìß [Email](mailto:satvik@buildfastwithai.com)\n",
        "\n",
        "---\n",
        "\n",
        "<div align=\"center\">\n",
        "  <p><strong>Built with ‚ù§Ô∏è by BuildFastWithAI</strong></p>\n",
        "  <p><em>Powered by Nebius Token Factory</em></p>\n",
        "  <p>‚≠ê Star the <a href=\"https://github.com/buildfastwithai/gen-ai-experiments\">Gen-AI-Experiments</a> repo if you found this helpful!</p>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}