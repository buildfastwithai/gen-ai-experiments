{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1i6z4E1h3h7PJnWgQ4yxXjNNGt4dp1qnT?usp=sharing)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "9GmpRN8J4nUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiktoken: High-Performance Tokenizer for OpenAI ModelsüöÄ\n",
        "\n",
        "`Tiktoken` is an open-source, high-performance Byte Pair Encoding (BPE) tokenizer developed by OpenAI for efficient tokenization of text data. It is designed to work seamlessly with OpenAI's language models like GPT-4 and GPT-3.5-turbo. üß†üí°\n",
        "\n",
        "### Key Features:\n",
        "- High Performance ‚ö°: `tiktoken` is optimized for speed, offering 3-6 times faster performance compared to other tokenizers. ‚è±Ô∏èüí®\n",
        "- Model-Specific Encoding üß©: Provides encodings tailored for specific models, ensuring compatibility and efficient processing. ü§ñüîß\n"
      ],
      "metadata": {
        "id": "vlRz3oX04lcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup and Installation**"
      ],
      "metadata": {
        "id": "tLz47a4W4k5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5-E3v4cz93b"
      },
      "outputs": [],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "WbRh6kAR21gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
      ],
      "metadata": {
        "id": "uq2fiVZB0cJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import tiktoken**\n"
      ],
      "metadata": {
        "id": "l3UkqIZ75ldI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "xc938tme1TJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load an encoding**"
      ],
      "metadata": {
        "id": "5U_m3_Ee5gQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")"
      ],
      "metadata": {
        "id": "BYqe1Jg41h89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "fKNMwET91tJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Turn text into tokens with encoding.encode()**"
      ],
      "metadata": {
        "id": "21sGj84i5tyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"tiktoken is great!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLePhq6r1u_y",
        "outputId": "39f15145-fd17-4284-f108-217ff6672ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[83, 8251, 2488, 382, 2212, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "pZj5yhMO1xBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"o200k_base\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMQToQ9G1zB3",
        "outputId": "27bfa813-f758-453d-da14-bf2b60957411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Turn tokens into text with encoding.decode()**\n"
      ],
      "metadata": {
        "id": "5uams9Rd6Cc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode([83, 8251, 2488, 382, 2212, 0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UQEPmee7104F",
        "outputId": "3843af7d-1981-484c-deae-c64bdccee8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiktoken is great!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [83, 8251, 2488, 382, 2212, 0]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BauBbfk41-WM",
        "outputId": "feb1ec99-0007-4f4c-87c2-3c8b8d7e2065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b't', b'ikt', b'oken', b' is', b' great', b'!']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparing encodings**\n"
      ],
      "metadata": {
        "id": "HWwWW-H76HqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # print the example string\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\", \"o200k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")"
      ],
      "metadata": {
        "id": "GFl-iGgv2AXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"antidisestablishmentarianism\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5aQpgVd2CMd",
        "outputId": "b9401c0c-e7d6-4c51-ea11-8805fea8c089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"antidisestablishmentarianism\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "cl100k_base: 6 tokens\n",
            "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
            "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n",
            "\n",
            "o200k_base: 6 tokens\n",
            "token integers: [493, 129901, 376, 160388, 21203, 2367]\n",
            "token bytes: [b'ant', b'idis', b'est', b'ablishment', b'arian', b'ism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"2 + 2 = 4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNYzzs3J2EHE",
        "outputId": "aafaee7e-7f98-4e22-f72a-6d45b720d2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"2 + 2 = 4\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "cl100k_base: 7 tokens\n",
            "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n",
            "\n",
            "o200k_base: 7 tokens\n",
            "token integers: [17, 659, 220, 17, 314, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"„ÅäË™ïÁîüÊó•„Åä„ÇÅ„Åß„Å®„ÅÜ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei8Sf0Ky2Fvj",
        "outputId": "3102a8e8-967b-47b7-e8a1-40fa1ebd53c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"„ÅäË™ïÁîüÊó•„Åä„ÇÅ„Åß„Å®„ÅÜ\"\n",
            "\n",
            "r50k_base: 14 tokens\n",
            "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
            "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
            "\n",
            "p50k_base: 14 tokens\n",
            "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
            "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
            "\n",
            "cl100k_base: 9 tokens\n",
            "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
            "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n",
            "\n",
            "o200k_base: 8 tokens\n",
            "token integers: [8930, 9697, 243, 128225, 8930, 17693, 4344, 48669]\n",
            "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Counting tokens for chat completions API calls**\n"
      ],
      "metadata": {
        "id": "-tJ67uw96OFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0125\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        \"gpt-4o-mini-2024-07-18\",\n",
        "        \"gpt-4o-2024-08-06\"\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0125\")\n",
        "    elif \"gpt-4o-mini\" in model:\n",
        "        print(\"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\")\n",
        "    elif \"gpt-4o\" in model:\n",
        "        print(\"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4o-2024-08-06\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "Xs3iatY72JD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for model in [\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"gpt-4-0613\",\n",
        "    \"gpt-4\",\n",
        "    \"gpt-4o\",\n",
        "    \"gpt-4o-mini\"\n",
        "    ]:\n",
        "    print(model)\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "    # example token count from the OpenAI API\n",
        "    response = client.chat.completions.create(model=model,\n",
        "    messages=example_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=1)\n",
        "    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i-b2SeV2WfE",
        "outputId": "6a6fbfe1-9e58-4a68-e636-fd118de4db77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo\n",
            "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4-0613\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4\n",
            "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4o\n",
            "Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\n",
            "124 prompt tokens counted by num_tokens_from_messages().\n",
            "124 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4o-mini\n",
            "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
            "124 prompt tokens counted by num_tokens_from_messages().\n",
            "124 prompt tokens counted by the OpenAI API.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Counting tokens for chat completions with tool calls**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_r1qqioF6S4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_for_tools(functions, messages, model):\n",
        "\n",
        "    # Initialize function settings to 0\n",
        "    func_init = 0\n",
        "    prop_init = 0\n",
        "    prop_key = 0\n",
        "    enum_init = 0\n",
        "    enum_item = 0\n",
        "    func_end = 0\n",
        "\n",
        "    if model in [\n",
        "        \"gpt-4o\",\n",
        "        \"gpt-4o-mini\"\n",
        "    ]:\n",
        "\n",
        "        # Set function settings for the above models\n",
        "        func_init = 7\n",
        "        prop_init = 3\n",
        "        prop_key = 3\n",
        "        enum_init = -3\n",
        "        enum_item = 3\n",
        "        func_end = 12\n",
        "    elif model in [\n",
        "        \"gpt-3.5-turbo\",\n",
        "        \"gpt-4\"\n",
        "    ]:\n",
        "        # Set function settings for the above models\n",
        "        func_init = 10\n",
        "        prop_init = 3\n",
        "        prop_key = 3\n",
        "        enum_init = -3\n",
        "        enum_item = 3\n",
        "        func_end = 12\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_for_tools() is not implemented for model {model}.\"\"\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "    func_token_count = 0\n",
        "    if len(functions) > 0:\n",
        "        for f in functions:\n",
        "            func_token_count += func_init  # Add tokens for start of each function\n",
        "            function = f[\"function\"]\n",
        "            f_name = function[\"name\"]\n",
        "            f_desc = function[\"description\"]\n",
        "            if f_desc.endswith(\".\"):\n",
        "                f_desc = f_desc[:-1]\n",
        "            line = f_name + \":\" + f_desc\n",
        "            func_token_count += len(encoding.encode(line))  # Add tokens for set name and description\n",
        "            if len(function[\"parameters\"][\"properties\"]) > 0:\n",
        "                func_token_count += prop_init  # Add tokens for start of each property\n",
        "                for key in list(function[\"parameters\"][\"properties\"].keys()):\n",
        "                    func_token_count += prop_key  # Add tokens for each set property\n",
        "                    p_name = key\n",
        "                    p_type = function[\"parameters\"][\"properties\"][key][\"type\"]\n",
        "                    p_desc = function[\"parameters\"][\"properties\"][key][\"description\"]\n",
        "                    if \"enum\" in function[\"parameters\"][\"properties\"][key].keys():\n",
        "                        func_token_count += enum_init  # Add tokens if property has enum list\n",
        "                        for item in function[\"parameters\"][\"properties\"][key][\"enum\"]:\n",
        "                            func_token_count += enum_item\n",
        "                            func_token_count += len(encoding.encode(item))\n",
        "                    if p_desc.endswith(\".\"):\n",
        "                        p_desc = p_desc[:-1]\n",
        "                    line = f\"{p_name}:{p_type}:{p_desc}\"\n",
        "                    func_token_count += len(encoding.encode(line))\n",
        "        func_token_count += func_end\n",
        "\n",
        "    messages_token_count = num_tokens_from_messages(messages, model)\n",
        "    total_tokens = messages_token_count + func_token_count\n",
        "\n",
        "    return total_tokens"
      ],
      "metadata": {
        "id": "WO7xEuWJ2f8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "      \"name\": \"get_current_weather\",\n",
        "      \"description\": \"Get the current weather in a given location\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "          },\n",
        "          \"unit\": {\"type\": \"string\",\n",
        "                   \"description\": \"The unit of temperature to return\",\n",
        "                   \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "        },\n",
        "        \"required\": [\"location\"],\n",
        "      },\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant that can answer to questions about the weather.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather like in San Francisco?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for model in [\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"gpt-4\",\n",
        "    \"gpt-4o\",\n",
        "    \"gpt-4o-mini\"\n",
        "    ]:\n",
        "    print(model)\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{num_tokens_for_tools(tools, example_messages, model)} prompt tokens counted by num_tokens_for_tools().\")\n",
        "    # example token count from the OpenAI API\n",
        "    response = client.chat.completions.create(model=model,\n",
        "          messages=example_messages,\n",
        "          tools=tools,\n",
        "          temperature=0)\n",
        "    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO64-6Wn3MBf",
        "outputId": "b33bc519-ebd5-4fd9-d8cd-9ecbcfc437d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo\n",
            "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\n",
            "105 prompt tokens counted by num_tokens_for_tools().\n",
            "105 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4\n",
            "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
            "105 prompt tokens counted by num_tokens_for_tools().\n",
            "105 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4o\n",
            "Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\n",
            "101 prompt tokens counted by num_tokens_for_tools().\n",
            "101 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4o-mini\n",
            "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
            "101 prompt tokens counted by num_tokens_for_tools().\n",
            "101 prompt tokens counted by the OpenAI API.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}