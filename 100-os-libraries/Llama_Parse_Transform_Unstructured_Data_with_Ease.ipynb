{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-zxVAH43LjFx-Qmamqx87BvnIAcLkwCt#scrollTo=QtIS_AMUtw56)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "Ato16ZPupsxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Llama Parse: Transform Unstructured Data with Ease**  \n",
        " Llama Parse is a powerful tool designed to transform unstructured data into structured formats, handling sources like PDFs, HTML, and text files. ðŸ“„ It simplifies large-scale data parsing, enabling seamless integration with workflows and making complex tasks more efficient. ðŸ’¡ Tailored for developers, it offers flexible customization options while connecting parsed data directly to LLMs. ðŸ”— With its precise, fast, and reliable data extraction capabilities, Llama Parse boosts productivity and empowers AI-driven workflows!\n"
      ],
      "metadata": {
        "id": "SJFxdo74p-PF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Building a RAG Pipeline over Legal Documents**"
      ],
      "metadata": {
        "id": "QRBnQe3jpp5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup and Installation**\n",
        "\n"
      ],
      "metadata": {
        "id": "bM6TTjr-qtNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opCPADfRpd1O"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['LLAMA_CLOUD_API_KEY']=userdata.get('LLAMA-CLOUD-API')"
      ],
      "metadata": {
        "id": "5IfOc516quz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸ“¥ Downloading and Extracting Dataset ðŸ“‚**\n"
      ],
      "metadata": {
        "id": "BGe2fbBUrFWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/user-attachments/files/16447759/data.zip -O data.zip\n",
        "!unzip -o data.zip\n",
        "!rm data.zip"
      ],
      "metadata": {
        "id": "EkdMIdG1qiXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "jysohauAqkIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸ“‚ Parsing US Legal Documents with LlamaParse âš–ï¸**"
      ],
      "metadata": {
        "id": "SCWBMHIhrk-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    parsing_instruction=\"Provided are a series of US legal documents.\",\n",
        "    use_vendor_multimodal_model=True,\n",
        "    vendor_multimodal_model_name=\"openai-gpt4o\",\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "\n",
        "def get_data_files(data_dir=DATA_DIR) -> list[str]:\n",
        "    files = []\n",
        "    for f in os.listdir(data_dir):\n",
        "        fname = os.path.join(data_dir, f)\n",
        "        if os.path.isfile(fname):\n",
        "            files.append(fname)\n",
        "    return files\n",
        "\n",
        "\n",
        "files = get_data_files()"
      ],
      "metadata": {
        "id": "XeYL0-71rXNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = parser.load_data(\n",
        "    files,\n",
        "    extra_info={\"name\": \"US legal documents provided by the Library of Congress.\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RR08Kd8rmZ8",
        "outputId": "d9be6b3b-3f30-4a13-9166-477a29e9e174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:01<00:00, 15.23s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸ” Setting Up VectorStore Index for Legal Documents ðŸ“š**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "49HeZoKLtQLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        "    Settings,\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "llm = OpenAI(\"gpt-4o\")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "if not os.path.exists(\"storage_legal\"):\n",
        "    index = VectorStoreIndex(documents, embed_model=embed_model)\n",
        "    index.storage_context.persist(persist_dir=\"./storage_legal\")\n",
        "else:\n",
        "    ctx = StorageContext.from_defaults(persist_dir=\"./storage_legal\")\n",
        "    index = load_index_from_storage(ctx)\n",
        "\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "I6rHT2iLrfml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸ“ Querying Legal Document Index for Information ðŸ”**"
      ],
      "metadata": {
        "id": "uYnpgI4_tU6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"Where did the majority of Barre Savings Bank's loans go?\"\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "id": "ghYVvWg2tE_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Why does Mr. Kubarych believe foreign markets are so important?\"\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "id": "RXRvHp1ltIE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Who is against the proposal of offshore drilling in CA and why?\"\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "id": "70FhyDzbtbqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"What is the purpose of the Ocean Science and Technology Subcommittee?\"\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "id": "QtIS_AMUtw56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Multimodal Parsing using GPT4o-mini**"
      ],
      "metadata": {
        "id": "e4ErX5zFuB-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸ“¥ Downloading Llama3.1 Blog PDF ðŸ“**"
      ],
      "metadata": {
        "id": "AofNtMT7vfhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.dropbox.com/scl/fi/8iu23epvv3473im5rq19g/llama3.1_blog.pdf?rlkey=5u417tbdox4aip33fdubvni56&st=dzozd11e&dl=1\" -O \"data/llama3.1_blog.pdf\"\n"
      ],
      "metadata": {
        "id": "nNENQZzat0Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Initialize LlamaParse**\n"
      ],
      "metadata": {
        "id": "y2mq0b7_vQLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.schema import TextNode\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "\n",
        "def get_text_nodes(json_list: List[dict]):\n",
        "    text_nodes = []\n",
        "    for idx, page in enumerate(json_list):\n",
        "        text_node = TextNode(text=page[\"md\"], metadata={\"page\": page[\"page\"]})\n",
        "        text_nodes.append(text_node)\n",
        "    return text_nodes\n",
        "\n",
        "\n",
        "def save_jsonl(data_list, filename):\n",
        "    \"\"\"Save a list of dictionaries as JSON Lines.\"\"\"\n",
        "    with open(filename, \"w\") as file:\n",
        "        for item in data_list:\n",
        "            json.dump(item, file)\n",
        "            file.write(\"\\n\")\n",
        "\n",
        "\n",
        "def load_jsonl(filename):\n",
        "    \"\"\"Load a list of dictionaries from JSON Lines.\"\"\"\n",
        "    data_list = []\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            data_list.append(json.loads(line))\n",
        "    return data_list\n",
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    use_vendor_multimodal_model=True,\n",
        "    vendor_multimodal_model_name=\"openai-gpt-4o-mini\",\n",
        "    invalidate_cache=True,\n",
        ")\n",
        "json_objs = parser.get_json_result(\"./data/llama3.1_blog.pdf\")\n",
        "json_list = json_objs[0][\"pages\"]\n",
        "docs = get_text_nodes(json_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK5gWCcSuTV4",
        "outputId": "9b298c4b-9f09-4abc-ef5a-11481f0812bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id c7cd9ead-5a69-4aad-bef0-5b33ff83346e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_jsonl([d.dict() for d in docs], \"docs.jsonl\")\n"
      ],
      "metadata": {
        "id": "LNoGgYyXuZCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "docs_dicts = load_jsonl(\"docs.jsonl\")\n",
        "docs = [Document.parse_obj(d) for d in docs_dicts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBdO4uDOuaxq",
        "outputId": "6efe0044-b609-49d1-f9a0-0654dab611ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-ea776a9a360e>:4: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  docs = [Document.parse_obj(d) for d in docs_dicts]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup GPT-4o baseline**\n"
      ],
      "metadata": {
        "id": "ryym4LSsvVPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser_gpt4o = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    use_vendor_multimodal_model=True,\n",
        "    vendor_multimodal_model=\"openai-gpt4o\",\n",
        "    # invalidate_cache=True\n",
        ")\n",
        "json_objs_gpt4o = parser_gpt4o.get_json_result(\"./data/llama3.1_blog.pdf\")\n",
        "# json_objs_gpt4o = parser.get_json_result(\"./data/llama2-p33.pdf\")\n",
        "json_list_gpt4o = json_objs_gpt4o[0][\"pages\"]\n",
        "docs_gpt4o = get_text_nodes(json_list_gpt4o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt10NhKSudQB",
        "outputId": "3a8feaab-1fd1-4bf9-d300-2464f2fe6e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id a9760515-355e-4089-b8ff-63642cee140d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_jsonl([d.dict() for d in docs_gpt4o], \"docs_gpt4o.jsonl\")\n"
      ],
      "metadata": {
        "id": "FtZqmeDXuiU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "docs_gpt4o_dicts = load_jsonl(\"docs_gpt4o.jsonl\")\n",
        "docs_gpt4o = [Document.parse_obj(d) for d in docs_gpt4o_dicts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjQu4phLufeN",
        "outputId": "4136d606-d764-40e1-b481-9a7d68f6087f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-fae74f861b4c>:4: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  docs_gpt4o = [Document.parse_obj(d) for d in docs_gpt4o_dicts]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**View Results**\n"
      ],
      "metadata": {
        "id": "R6De1hJnvaer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[4].get_content(metadata_mode=\"all\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "savpR4LxuqS5",
        "outputId": "a8f392c6-6127-4f01-b8f9-7d589c61236a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page: 5\n",
            "\n",
            "# Llama 3.1 Model Evaluation\n",
            "\n",
            "## Benchmark Results\n",
            "\n",
            "| Category        | Llama 3.1 8B | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mistral 8x22B Instruct | GPT 3.5 Turbo |\n",
            "|-----------------|---------------|----------------|----------------------|----------------|------------------------|----------------|\n",
            "| General         |               |                |                      |                |                        |                |\n",
            "| MMLU            | 73.0          | 72.3           | 60.5                 | 86.0           | 79.9                   | 69.8           |\n",
            "| MMLU PRO        | 48.3          | 36.9           | 36.9                 | 66.4           | 56.3                   | 49.2           |\n",
            "| iEval           | 80.4          | 73.6           | 57.6                 | 87.5           | 69.9                   |                |\n",
            "| Code            |               |                |                      |                |                        |                |\n",
            "| HumanEval       | 72.6          | 54.3           | 40.2                 | 80.5           | 75.6                   |                |\n",
            "| MBPP Eval       | 72.8          | 71.7           | 61.8                 | 82.0           |                        |                |\n",
            "| Mash            | 84.5          | 76.7           | 53.2                 | 95.1           | 88.2                   |                |\n",
            "| MATH            | 51.9          | 44.3           | 13.0                 | 68.0           |                        |                |\n",
            "| Reasoning       |               |                |                      |                |                        |                |\n",
            "| ARC Challenge    | 83.4          | 87.6           | 74.2                 | 28.8           |                        |                |\n",
            "| GPA             | 32.8          | 28.8           | 46.7                 | 33.3           | 33.3                   | 28.5           |\n",
            "| Tool use        |               |                |                      |                |                        |                |\n",
            "| BFCL            | 76.1          | 30.0           | 24.7                 | 56.7           |                        |                |\n",
            "| Nexus           | 38.5          |                |                      |                |                        |                |\n",
            "| Long context    |               |                |                      |                |                        |                |\n",
            "| ZeroSCROLLS/Quality | 81.0     |                |                      | 90.5           |                        |                |\n",
            "| InfiniteBench/En.MC | 65.1     |                |                      |                |                        |                |\n",
            "| NIH/Multi-needle | 98.8         | -              | -                    | 97.5           | -                      | -              |\n",
            "| Multilingual NGSM | 68.9       | 53.2           | 29.9                 | 86.9           | 71.1                   | 51.4           |\n",
            "\n",
            "## Llama 3.1 405B Human Evaluation\n",
            "\n",
            "| Comparison                          | Win Rate | Tie Rate | Loss Rate |\n",
            "|-------------------------------------|----------|----------|-----------|\n",
            "| Llama 3.1 405B vs GPT-4-0125-Preview | 23.3%    | 52.2%    | 24.5%     |\n",
            "| Llama 3.1 405B vs GPT-4o           | 19.1%    | 51.7%    | 29.2%     |\n",
            "| Llama 3.1 405B vs Claude 3.5 Sonnet | 24.9%    | 50.8%    | 24.2%     |\n"
          ]
        }
      ]
    }
  ]
}