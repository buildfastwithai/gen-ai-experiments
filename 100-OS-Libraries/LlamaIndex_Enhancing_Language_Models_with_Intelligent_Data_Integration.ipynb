{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_f17Rr1bxQ6aHJT_z2Bpg9pUFbPdj5fM#scrollTo=yT9i6zObtGnM)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "whPgAbRVh5i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“šðŸ” **LlamaIndex**: Enhancing Language Models with Intelligent Data Integration\n",
        "LlamaIndex is a powerful Python library that enables seamless data integration for language models. It allows developers to connect, index, and query structured or unstructured data sources, facilitating advanced retrieval-augmented generation (RAG) workflows for LLMs. By transforming data into an optimized format for querying, LlamaIndex simplifies building applications like chatbots, knowledge retrieval systems, and intelligent search tools. It supports various integrations, including databases, APIs, and documents, making it an essential tool for leveraging external data with language models.  \n"
      ],
      "metadata": {
        "id": "h6fkdPOgh_Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ðŸš€ðŸ“– **Building a RAG System with Mistral and LlamaIndex**  "
      ],
      "metadata": {
        "id": "mdWa3MSCi1Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Required Libraries**\n"
      ],
      "metadata": {
        "id": "FWF1HMxOt06D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-mistralai llama_index"
      ],
      "metadata": {
        "id": "WZL6K0O1mI7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Configure Mistral API**"
      ],
      "metadata": {
        "id": "4nsPnYmxilRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "os.environ[\"MISTRAL_API_KEY\"]=userdata.get('MISTRAL_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
      ],
      "metadata": {
        "id": "FCG26OMkjsA6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HieXiut0fU9j"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "llm = MistralAI(model=\"open-mixtral-8x22b\", temperature=0.1)\n",
        "embed_model = OpenAIEmbedding(model_name=\"text-embedding-ada-002\")\n",
        "\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸ“¥ Downloading the Dataset**"
      ],
      "metadata": {
        "id": "ERyf1IcnuNkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O './uber_2021.pdf'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O './lyft_2021.pdf'"
      ],
      "metadata": {
        "id": "0jfmPk0DltFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26eab315-9472-4a58-c647-a9df206a09f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-18 17:35:30--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1880483 (1.8M) [application/octet-stream]\n",
            "Saving to: â€˜./uber_2021.pdfâ€™\n",
            "\n",
            "./uber_2021.pdf     100%[===================>]   1.79M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-12-18 17:35:30 (24.3 MB/s) - â€˜./uber_2021.pdfâ€™ saved [1880483/1880483]\n",
            "\n",
            "--2024-12-18 17:35:30--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1440303 (1.4M) [application/octet-stream]\n",
            "Saving to: â€˜./lyft_2021.pdfâ€™\n",
            "\n",
            "./lyft_2021.pdf     100%[===================>]   1.37M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-12-18 17:35:31 (19.3 MB/s) - â€˜./lyft_2021.pdfâ€™ saved [1440303/1440303]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“‚ **Loading the Datasets**\n"
      ],
      "metadata": {
        "id": "rASGYDN-ub-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "uber_docs = SimpleDirectoryReader(input_files=[\"./uber_2021.pdf\"]).load_data()\n",
        "lyft_docs = SimpleDirectoryReader(input_files=[\"./lyft_2021.pdf\"]).load_data()"
      ],
      "metadata": {
        "id": "QR08KtLvnNTM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ—ï¸ **Building VectorStore Indexes for the Datasets**\n"
      ],
      "metadata": {
        "id": "mCpLNO4Ku27B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "uber_index = VectorStoreIndex.from_documents(uber_docs)\n",
        "uber_query_engine = uber_index.as_query_engine(similarity_top_k=5)\n",
        "\n",
        "lyft_index = VectorStoreIndex.from_documents(lyft_docs)\n",
        "lyft_query_engine = lyft_index.as_query_engine(similarity_top_k=5)"
      ],
      "metadata": {
        "id": "xIUFqK_1nSDe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ¤– **Querying the Dataset**"
      ],
      "metadata": {
        "id": "BduCQT3Zu_rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = uber_query_engine.query(\"What is the revenue of uber in 2021?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "u-06DSKE2Hf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = lyft_query_engine.query(\"What are lyft investments in 2021?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "2wrTXcyrnb-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ–¼ï¸ **Querying Images with a Multi-Modal LLM**\n"
      ],
      "metadata": {
        "id": "49zd9QNXvDDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Required Libraries**\n"
      ],
      "metadata": {
        "id": "qQdZyJNRvPE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-multi-modal-llms-openai\n",
        "!pip install llama-index-vector-stores-qdrant\n",
        "!pip install llama_index ftfy regex tqdm\n",
        "!pip install llama-index-embeddings-clip\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install matplotlib scikit-image"
      ],
      "metadata": {
        "id": "mrqW9xQKoH1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ–¼ï¸ **Loading Image for Multi-Modal Query**\n"
      ],
      "metadata": {
        "id": "On5SBxjZvZiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
        "\n",
        "from llama_index.core.multi_modal_llms.generic_utils import load_image_urls\n",
        "\n",
        "\n",
        "image_urls = [\n",
        "    \"https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg\",\n",
        "]\n",
        "\n",
        "image_documents = load_image_urls(image_urls)"
      ],
      "metadata": {
        "id": "AYYK6zg6pYqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”„ **Initializing OpenAI Multi-Modal LLM**\n"
      ],
      "metadata": {
        "id": "tYrKwchwvh_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_mm_llm = OpenAIMultiModal(\n",
        "    model=\"gpt-4o-mini\", max_new_tokens=300\n",
        ")"
      ],
      "metadata": {
        "id": "xzaDxlfFoKvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ–¼ï¸ **Generating Description for the Image**"
      ],
      "metadata": {
        "id": "23gF2oPKv-J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai_mm_llm.complete(\n",
        "    prompt=\"Describe the images as an alternative text\",\n",
        "    image_documents=image_documents,\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "GyMp4AvBp0DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ–¼ï¸ **Querying Multiple Images**"
      ],
      "metadata": {
        "id": "EfvR8ftmwCz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "input_image_path = Path(\"input_images\")\n",
        "if not input_image_path.exists():\n",
        "    Path.mkdir(input_image_path)"
      ],
      "metadata": {
        "id": "kHQQIcUFp2dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://docs.google.com/uc?export=download&id=1nUhsBRiSWxcVQv8t8Cvvro8HJZ88LCzj\" -O ./input_images/long_range_spec.png\n",
        "!wget \"https://docs.google.com/uc?export=download&id=19pLwx0nVqsop7lo0ubUSYTzQfMtKJJtJ\" -O ./input_images/model_y.png\n",
        "!wget \"https://docs.google.com/uc?export=download&id=1utu3iD9XEgR5Sb7PrbtMf1qw8T1WdNmF\" -O ./input_images/performance_spec.png\n",
        "!wget \"https://docs.google.com/uc?export=download&id=1dpUakWMqaXR4Jjn1kHuZfB0pAXvjn2-i\" -O ./input_images/price.png\n",
        "!wget \"https://docs.google.com/uc?export=download&id=1qNeT201QAesnAP5va1ty0Ky5Q_jKkguV\" -O ./input_images/real_wheel_spec.png"
      ],
      "metadata": {
        "id": "s5WdRBojtUQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "def plot_images(image_paths):\n",
        "    images_shown = 0\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    for img_path in image_paths:\n",
        "        if os.path.isfile(img_path):\n",
        "            image = Image.open(img_path)\n",
        "\n",
        "            plt.subplot(2, 3, images_shown + 1)\n",
        "            plt.imshow(image)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "\n",
        "            images_shown += 1\n",
        "            if images_shown >= 9:\n",
        "                break"
      ],
      "metadata": {
        "id": "Y28-CRRis-tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = []\n",
        "for img_path in os.listdir(\"./input_images\"):\n",
        "    image_paths.append(str(os.path.join(\"./input_images\", img_path)))\n",
        "plot_images(image_paths)"
      ],
      "metadata": {
        "id": "PKnWz5netAqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "\n",
        "image_documents = SimpleDirectoryReader(\"/content/input_images\").load_data()"
      ],
      "metadata": {
        "id": "xEG6ASBWtDu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai_mm_llm.complete(\n",
        "    prompt=\"Describe the images as an alternative text\",\n",
        "    image_documents=image_documents,\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "yT9i6zObtGnM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}