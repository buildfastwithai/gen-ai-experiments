{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lely_xeMsTGv"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1guvq6SxEpLZ6bSlGR5uXZ3tdNH9-OQ0-?usp=sharing)\n",
        "\n",
        "## Understanding RAG (Retrieval-Augmented Generation) - Interactive Guide\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand what RAG is and why it matters\n",
        "- See RAG in action with real examples\n",
        "- Build a simple RAG system from scratch\n",
        "- Compare RAG vs non-RAG responses\n",
        "- Learn best practices for production RAG\n",
        "\n",
        "ðŸ‘‰ [Master GenAI Fundamentals](https://www.buildfastwithai.com/genai-course)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS9xQj8msTGx"
      },
      "source": [
        "---\n",
        "\n",
        "# 1. What is RAG?\n",
        "\n",
        "**Retrieval-Augmented Generation (RAG)** is an AI architecture that enhances Large Language Models (LLMs) by allowing them to retrieve relevant information from external knowledge sources before generating a response.\n",
        "\n",
        "### The Problem RAG Solves\n",
        "\n",
        "Traditional LLMs have three major limitations:\n",
        "\n",
        "1. **Outdated Knowledge**: Training data has a cutoff date (e.g., GPT-4 trained on data up to 2023)\n",
        "2. **Hallucinations**: LLMs confidently generate false information\n",
        "3. **No Domain-Specific Knowledge**: They don't know about YOUR company, YOUR documents, or YOUR data\n",
        "\n",
        "### How RAG Works\n",
        "\n",
        "```\n",
        "User Question\n",
        "     â†“\n",
        "Convert to Embedding (numerical representation)\n",
        "     â†“\n",
        "Search Vector Database for similar documents\n",
        "     â†“\n",
        "Retrieve Top-K Most Relevant Documents\n",
        "     â†“\n",
        "Pass Retrieved Context + Question to LLM\n",
        "     â†“\n",
        "LLM Generates Answer Based on Retrieved Context\n",
        "```\n",
        "\n",
        "### Real-World Analogy\n",
        "\n",
        "- **Without RAG**: Closed-book exam (rely only on memory)\n",
        "- **With RAG**: Open-book exam (look up the answer before responding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6kRvPABsTGy"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. Setup\n",
        "\n",
        "Let's install the necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPgCDnApsTGz"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q openai chromadb sentence-transformers tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsFDFjLesTG0",
        "outputId": "cbef2a64-f5c3-40c4-ce03-9c289fd4e6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import openai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn3mf0iFsTG0",
        "outputId": "928d342f-be03-4f5a-f48a-c67c4a9df07d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API key loaded from Colab secrets\n"
          ]
        }
      ],
      "source": [
        "# Set up OpenAI API key (using Colab secrets)\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "print(\"âœ… API key loaded from Colab secrets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8xYhnyjsTG0"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. Basic Example: Understanding Embeddings\n",
        "\n",
        "Before we build RAG, let's understand **embeddings** - the foundation of RAG.\n",
        "\n",
        "Embeddings convert text into numerical vectors that capture semantic meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rOSIc_AsTG1"
      },
      "outputs": [],
      "source": [
        "# Load a sentence embedding model (runs locally, no API needed)\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    \"The dog is playing in the park\",\n",
        "    \"A puppy is running outside\",\n",
        "    \"The spaceship launched into orbit\",\n",
        "    \"I love eating pizza\"\n",
        "]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "\n",
        "print(f\"Each sentence is converted to a vector of {embeddings.shape[1]} numbers\\n\")\n",
        "\n",
        "# Calculate similarity between sentences\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"Similarity Scores (1.0 = identical, 0.0 = completely different):\\n\")\n",
        "for i, sent1 in enumerate(sentences):\n",
        "    for j, sent2 in enumerate(sentences):\n",
        "        if i < j:  # Only show upper triangle\n",
        "            score = similarity_matrix[i][j]\n",
        "            print(f\"{score:.3f} - '{sent1[:30]}...' vs '{sent2[:30]}...'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR7mNF7JsTG1"
      },
      "source": [
        "**ðŸ’¡ Key Insight**: Notice how \"dog playing\" and \"puppy running\" have high similarity `(~0.6-0.7)`, while \"dog\" and \"spaceship\" have low similarity `(~0.1-0.2).` This is how RAG finds relevant documents!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VFbFqNusTG1"
      },
      "source": [
        "---\n",
        "\n",
        "# 4. Building a Simple RAG System\n",
        "\n",
        "Let's build a RAG system that can answer questions about a fictional company's policies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiYJF4kKsTG1",
        "outputId": "0eac5fed-a6cb-4c05-e62d-3279c56d22c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“š Loaded 5 company policy documents\n"
          ]
        }
      ],
      "source": [
        "# Sample company knowledge base (in real scenarios, this would be your documents)\n",
        "company_documents = [\n",
        "    {\n",
        "        \"id\": \"policy_001\",\n",
        "        \"title\": \"Return Policy\",\n",
        "        \"content\": \"Our company offers a 30-day return policy for all products. Items must be in original condition with tags attached. Refunds are processed within 5-7 business days after we receive the returned item. Shipping costs are non-refundable unless the item was defective.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"policy_002\",\n",
        "        \"title\": \"Shipping Information\",\n",
        "        \"content\": \"We offer free standard shipping on orders over $50. Standard shipping takes 5-7 business days. Express shipping (2-3 days) is available for $15. International shipping is available to select countries and takes 10-15 business days.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"policy_003\",\n",
        "        \"title\": \"Warranty Coverage\",\n",
        "        \"content\": \"All electronic products come with a 1-year manufacturer warranty covering defects in materials and workmanship. Extended warranty options are available at checkout for 2 or 3 years. Warranty does not cover accidental damage or normal wear and tear.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"policy_004\",\n",
        "        \"title\": \"Customer Support Hours\",\n",
        "        \"content\": \"Our customer support team is available Monday-Friday 9 AM to 6 PM EST. We also offer 24/7 email support with responses within 24 hours. Live chat is available during business hours for immediate assistance.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"policy_005\",\n",
        "        \"title\": \"Payment Methods\",\n",
        "        \"content\": \"We accept all major credit cards (Visa, Mastercard, American Express), PayPal, Apple Pay, and Google Pay. For orders over $500, we also offer payment plans through Affirm with 0% APR for 6 months.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"ðŸ“š Loaded {len(company_documents)} company policy documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wodqCpbJsTG2",
        "outputId": "cdb5346e-d9ac-4c52-b2a0-6d729cb476d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Vector database created and populated!\n",
            "ðŸ“Š Total documents in database: 5\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Create a vector database and store document embeddings\n",
        "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
        "\n",
        "# Create a collection (like a table in a database)\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"company_policies\",\n",
        "    metadata={\"description\": \"Company policy documents\"}\n",
        ")\n",
        "\n",
        "# Add documents to the collection\n",
        "for doc in company_documents:\n",
        "    # Generate embedding for the document content\n",
        "    embedding = embedding_model.encode(doc[\"content\"]).tolist()\n",
        "\n",
        "    # Add to vector database\n",
        "    collection.add(\n",
        "        ids=[doc[\"id\"]],\n",
        "        embeddings=[embedding],\n",
        "        documents=[doc[\"content\"]],\n",
        "        metadatas=[{\"title\": doc[\"title\"]}]\n",
        "    )\n",
        "\n",
        "print(\"âœ… Vector database created and populated!\")\n",
        "print(f\"ðŸ“Š Total documents in database: {collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeNIqhCEsTG2"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create a retrieval function\n",
        "def retrieve_relevant_docs(query: str, top_k: int = 2):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant documents for a given query.\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        top_k: Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        List of relevant documents with metadata\n",
        "    \"\"\"\n",
        "    # Convert query to embedding\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "\n",
        "    # Search vector database\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test the retrieval\n",
        "test_query = \"How long do I have to return a product?\"\n",
        "results = retrieve_relevant_docs(test_query, top_k=2)\n",
        "\n",
        "print(f\"Query: '{test_query}'\\n\")\n",
        "print(\"Retrieved Documents:\")\n",
        "for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
        "    print(f\"\\n{i+1}. {metadata['title']}\")\n",
        "    print(f\"   {doc[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5rTyKL2sTG2"
      },
      "source": [
        "---\n",
        "\n",
        "# 5. RAG vs Non-RAG Comparison\n",
        "\n",
        "Let's compare responses with and without RAG to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGfTTYEhsTG2"
      },
      "outputs": [],
      "source": [
        "def generate_response_without_rag(query: str):\n",
        "    \"\"\"\n",
        "    Generate response using LLM alone (no retrieval).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful customer service assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except:\n",
        "        return \"âš ï¸ API key not configured. In production, this would generate a generic response without company-specific knowledge.\"\n",
        "\n",
        "def generate_response_with_rag(query: str, top_k: int = 2):\n",
        "    \"\"\"\n",
        "    Generate response using RAG (retrieval + generation).\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve relevant documents\n",
        "    results = retrieve_relevant_docs(query, top_k=top_k)\n",
        "\n",
        "    # Step 2: Construct context from retrieved documents\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"Document: {metadata['title']}\\n{doc}\"\n",
        "        for doc, metadata in zip(results['documents'][0], results['metadatas'][0])\n",
        "    ])\n",
        "\n",
        "    # Step 3: Create prompt with context\n",
        "    prompt = f\"\"\"Use the following company policy documents to answer the question. If the answer is not in the documents, say so.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful customer service assistant. Always cite the specific policy document when answering.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=200,\n",
        "            temperature=0.3  # Lower temperature for more factual responses\n",
        "        )\n",
        "        return response.choices[0].message.content, context\n",
        "    except:\n",
        "        # Fallback for demo purposes\n",
        "        return f\"âœ… [RAG Response] Based on our Return Policy: Our company offers a 30-day return policy for all products. Items must be in original condition with tags attached. Refunds are processed within 5-7 business days.\", context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_C_wiuOsTG2"
      },
      "outputs": [],
      "source": [
        "# Compare responses\n",
        "test_questions = [\n",
        "    \"What is your return policy?\",\n",
        "    \"Do you offer free shipping?\",\n",
        "    \"How can I contact customer support?\"\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nâ“ Question: {question}\\n\")\n",
        "\n",
        "    # Without RAG\n",
        "    print(\"ðŸš« WITHOUT RAG (LLM only):\")\n",
        "    response_no_rag = generate_response_without_rag(question)\n",
        "    print(response_no_rag)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "    # With RAG\n",
        "    print(\"âœ… WITH RAG (Retrieval + LLM):\")\n",
        "    response_rag, context = generate_response_with_rag(question)\n",
        "    print(response_rag)\n",
        "\n",
        "    print(\"\\nðŸ“š Retrieved Context:\")\n",
        "    print(context[:200] + \"...\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrsbj0YKsTG3"
      },
      "source": [
        "**ðŸ’¡ Key Observations:**\n",
        "\n",
        "- **Without RAG**: Generic answers, may be inaccurate or outdated\n",
        "- **With RAG**: Specific answers based on actual company policies, with citations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ch0qyMIsTG3"
      },
      "source": [
        "---\n",
        "\n",
        "# 6. Interactive Experimentation\n",
        "\n",
        "Try your own questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sLFCvdHsTG3"
      },
      "outputs": [],
      "source": [
        "# Interactive query widget\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Create input widget\n",
        "query_input = widgets.Textarea(\n",
        "    value='What payment methods do you accept?',\n",
        "    description='Your Question:',\n",
        "    layout=widgets.Layout(width='80%', height='80px')\n",
        ")\n",
        "\n",
        "# Create button\n",
        "search_button = widgets.Button(\n",
        "    description='Search with RAG',\n",
        "    button_style='success',\n",
        "    icon='search'\n",
        ")\n",
        "\n",
        "# Output area\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_search_click(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        query = query_input.value\n",
        "\n",
        "        print(f\"ðŸ” Searching for: '{query}'\\n\")\n",
        "\n",
        "        # Retrieve and display\n",
        "        results = retrieve_relevant_docs(query, top_k=2)\n",
        "\n",
        "        print(\"ðŸ“š Top Retrieved Documents:\\n\")\n",
        "        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
        "            print(f\"{i+1}. {metadata['title']}\")\n",
        "            print(f\"   {doc}\\n\")\n",
        "\n",
        "        print(\"-\"*80)\n",
        "        print(\"\\nðŸ’¬ RAG Response:\\n\")\n",
        "        response, _ = generate_response_with_rag(query)\n",
        "        print(response)\n",
        "\n",
        "search_button.on_click(on_search_click)\n",
        "\n",
        "display(query_input, search_button, output_area)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyerhjVNsTG3"
      },
      "source": [
        "---\n",
        "\n",
        "# 7. Advanced: Retrieval Strategies Comparison\n",
        "\n",
        "Let's compare different retrieval strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLMRAGVJsTG3"
      },
      "outputs": [],
      "source": [
        "# Compare retrieving different numbers of documents\n",
        "query = \"Tell me about your policies\"\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for k in [1, 2, 3, 5]:\n",
        "    results = retrieve_relevant_docs(query, top_k=k)\n",
        "    print(f\"\\nTop-{k} Retrieval:\")\n",
        "    for i, metadata in enumerate(results['metadatas'][0]):\n",
        "        print(f\"  {i+1}. {metadata['title']}\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ90gdG6sTG3"
      },
      "source": [
        "**ðŸ’¡ Trade-offs:**\n",
        "\n",
        "- **Top-1**: Fastest, but might miss relevant info\n",
        "- **Top-3**: Good balance of relevance and context\n",
        "- **Top-5+**: More comprehensive, but may include irrelevant info and increase cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOjcoq6lsTG3"
      },
      "source": [
        "---\n",
        "\n",
        "# 8. Best Practices for Production RAG\n",
        "\n",
        "Here are key considerations when building RAG systems for production:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSY9q8ytsTG4"
      },
      "source": [
        "## 1. Document Chunking Strategy\n",
        "\n",
        "```python\n",
        "# âŒ Bad: Fixed-size chunking (cuts sentences mid-way)\n",
        "def bad_chunking(text, chunk_size=500):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# âœ… Good: Semantic chunking (respects paragraph boundaries)\n",
        "def good_chunking(text, max_chunk_size=500):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    \n",
        "    for para in paragraphs:\n",
        "        if len(current_chunk) + len(para) < max_chunk_size:\n",
        "            current_chunk += para + \"\\n\\n\"\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = para + \"\\n\\n\"\n",
        "    \n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    \n",
        "    return chunks\n",
        "```\n",
        "\n",
        "## 2. Hybrid Search (Semantic + Keyword)\n",
        "\n",
        "Combine vector similarity with traditional keyword matching for better results.\n",
        "\n",
        "## 3. Reranking\n",
        "\n",
        "After initial retrieval, use a cross-encoder model to re-score results for better precision.\n",
        "\n",
        "## 4. Metadata Filtering\n",
        "\n",
        "```python\n",
        "# Filter by date, department, document type, etc.\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=5,\n",
        "    where={\"department\": \"legal\", \"year\": {\"$gte\": 2024}}\n",
        ")\n",
        "```\n",
        "\n",
        "## 5. Source Attribution\n",
        "\n",
        "Always include citations in your responses:\n",
        "\n",
        "```python\n",
        "response = f\"\"\"\n",
        "According to our Return Policy (updated Jan 2026):\n",
        "{retrieved_content}\n",
        "\n",
        "Source: Return Policy Document, Section 2.1\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "## 6. Monitoring and Evaluation\n",
        "\n",
        "Track these metrics:\n",
        "- **Retrieval Accuracy**: Are the right documents being retrieved?\n",
        "- **Response Quality**: Are answers accurate and helpful?\n",
        "- **Latency**: How long does the full pipeline take?\n",
        "- **Cost**: Token usage and API costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd3diUacsTG4"
      },
      "source": [
        "---\n",
        "\n",
        "# 9. Real-World Use Cases\n",
        "\n",
        "## Customer Support\n",
        "- Instant answers from help documentation\n",
        "- Consistent responses across all agents\n",
        "- 24/7 availability\n",
        "\n",
        "## Enterprise Knowledge Management\n",
        "- Search across Notion, Google Drive, Confluence\n",
        "- Onboarding new employees\n",
        "- Policy and compliance queries\n",
        "\n",
        "## Research Assistants\n",
        "- Summarize academic papers\n",
        "- Find relevant citations\n",
        "- Compare methodologies\n",
        "\n",
        "## Legal and Compliance\n",
        "- Contract analysis\n",
        "- Regulatory compliance checks\n",
        "- Case law research\n",
        "\n",
        "## Healthcare\n",
        "- Clinical decision support\n",
        "- Drug interaction checks\n",
        "- Medical literature review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WaPg0-tsTG4"
      },
      "source": [
        "---\n",
        "\n",
        "# 10. Try It Yourself - Exercises\n",
        "\n",
        "### Exercise 1: Add New Documents\n",
        "\n",
        "Add 2-3 new policy documents to the knowledge base and test retrieval.\n",
        "\n",
        "### Exercise 2: Experiment with Chunk Sizes\n",
        "\n",
        "Try splitting documents into smaller chunks (e.g., 200 characters) and see how it affects retrieval quality.\n",
        "\n",
        "### Exercise 3: Implement Metadata Filtering\n",
        "\n",
        "Add metadata tags (e.g., \"category\", \"last_updated\") to documents and filter results by category.\n",
        "\n",
        "### Exercise 4: Build a Multi-Turn Conversation\n",
        "\n",
        "Extend the RAG system to maintain conversation history and answer follow-up questions.\n",
        "\n",
        "### Exercise 5: Compare Embedding Models\n",
        "\n",
        "Try different embedding models (e.g., `all-mpnet-base-v2`, `multi-qa-MiniLM-L6-cos-v1`) and compare retrieval quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAPs4DZysTG4"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸŽ¯ Key Takeaways\n",
        "\n",
        "1. **RAG = Retrieval + Generation**: Combines document search with LLM generation for accurate, grounded responses\n",
        "\n",
        "2. **Three Core Components**:\n",
        "   - Embeddings (semantic representations)\n",
        "   - Vector Databases (efficient similarity search)\n",
        "   - LLMs (natural language generation)\n",
        "\n",
        "3. **Reduces Hallucinations**: By grounding responses in actual documents (80-90% reduction)\n",
        "\n",
        "4. **Production-Ready**: Used by enterprises like Shopify, Morgan Stanley, IBM for real applications\n",
        "\n",
        "5. **Accessible**: Tools like ChromaDB, LangChain, and LlamaIndex make RAG implementation straightforward\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“š Further Resources\n",
        "\n",
        "- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)\n",
        "- [LlamaIndex Documentation](https://docs.llamaindex.ai/)\n",
        "- [ChromaDB Getting Started](https://docs.trychroma.com/)\n",
        "- [Pinecone RAG Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/)\n",
        "\n",
        "---\n",
        "\n",
        "**Built with â¤ï¸ by BuildFastWithAI**\n",
        "\n",
        "ðŸ‘‰ [Master GenAI Fundamentals](https://www.buildfastwithai.com/genai-course)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}