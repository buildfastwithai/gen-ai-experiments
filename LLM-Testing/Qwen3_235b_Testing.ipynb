{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bBT0Gio0goR3NfU80bttnnXP7cCC1hSH?usp=sharing)\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- No coding experience required\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZKrP-_36l23"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKy3SpsM6jiJ"
      },
      "source": [
        "# Testing Qwen 3 Model Using OpenRouter\n",
        "\n",
        "This notebook provides a comprehensive guide to using the Qwen model via OpenRouter's API within the LangChain framework. We will cover everything from basic setup to advanced examples.\n",
        "\n",
        "**Note:** You will need an API key from [OpenRouter](https://openrouter.ai/) to run the examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfaRziHC6jiM"
      },
      "source": [
        "###Installation\n",
        "\n",
        "First, let's install the necessary Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TjlR0IIK6jiN",
        "outputId": "dd81e4d1-6218-40b5-f362-f94185380493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.69)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.97.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m798.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.28\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dTJ4-AV6jiO"
      },
      "source": [
        "###Basic Usage with ChatOpenAI and OpenRouter\n",
        "\n",
        "- Hereâ€™s how to set up the `ChatOpenAI` class to connect to the Qwen model through OpenRouter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5A3jXql6jiO",
        "outputId": "c982d61a-d89d-48ab-9e9c-306da143f727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of now, there is no widely recognized or officially documented protocol known as the \"Model Context Protocol\" in the fields of artificial intelligence, machine learning, or natural language processing.\n",
            "\n",
            "It's possible that you may be referring to one of the following:\n",
            "\n",
            "1. **Context Management in AI Models**: Many large language models (like GPT, LLaMA, etc.) rely on mechanisms to manage *context*â€”meaning the conversation history or input text used to generate relevant responses. While there's no formal \"Model Context Protocol,\" systems often use techniques like attention mechanisms, sliding windows, or context caching to handle input context efficiently.\n",
            "\n",
            "2. **MCP (Model Context Protocol) â€“ Emerging Concept?**: There is a growing interest in standardized ways for AI models to exchange contextual information, especially in agent-based systems or multi-model workflows. Some developers and researchers have informally discussed ideas around a \"Model Context Protocol\" as a way to structure how AI agents share state, memory, and intent. However, this is not yet a formal standard.\n",
            "\n",
            "3. **Mishearing or Confusion with Other Terms**:\n",
            "   - **HTTP (Hypertext Transfer Protocol)**: A foundational internet protocol.\n",
            "   - **TCP (Transmission Control Protocol)**: A core networking protocol.\n",
            "   - **Prompt Engineering or Context Design**: Practices for structuring input context for AI models.\n",
            "\n",
            "If \"Model Context Protocol\" refers to a specific tool, framework, or recent research paper not yet broadly adopted, it may be an emerging or proprietary concept.\n",
            "\n",
            "If you have a specific source or context where you encountered the term, Iâ€™d be happy to help clarify further!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# It's recommended to set your API key as an environment variable\n",
        "api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "# Initialize the ChatOpenAI model for Qwen\n",
        "qwen_llm = ChatOpenAI(\n",
        "    model=\"qwen/qwen3-235b-a22b-07-25:free\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# Let's test it with a simple prompt\n",
        "response = qwen_llm.invoke(\"What is the Model Context Protocol?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBYsDNig6jiP"
      },
      "source": [
        "### Multilingual Capabilities\n",
        "\n",
        "Qwen models are proficient in multiple languages. Let's test this by sending prompts in Hindi and Spanish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_tC8vos6jiP",
        "outputId": "08786222-9151-4e34-adad-5c29d117f06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response in Hindi:\n",
            "à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ (Artificial Intelligence à¤¯à¤¾ AI) à¤à¤• à¤¤à¤•à¤¨à¥€à¤•à¥€ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤¹à¥ˆ à¤œà¤¿à¤¸à¤®à¥‡à¤‚ à¤®à¤¶à¥€à¤¨à¥‹à¤‚, à¤µà¤¿à¤¶à¥‡à¤· à¤°à¥‚à¤ª à¤¸à¥‡ à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤¸à¤¿à¤¸à¥à¤Ÿà¤®à¥‹à¤‚ à¤•à¥‹ à¤‡à¤‚à¤¸à¤¾à¤¨à¥€ à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¥‡ à¤•à¥à¤› à¤—à¥à¤£à¥‹à¤‚ à¤œà¥ˆà¤¸à¥‡ à¤¸à¥‹à¤šà¤¨à¥‡, à¤¸à¥€à¤–à¤¨à¥‡, à¤¤à¤°à¥à¤• à¤•à¤°à¤¨à¥‡, à¤¸à¤®à¤¸à¥à¤¯à¤¾à¤“à¤‚ à¤•à¤¾ à¤¸à¤®à¤¾à¤§à¤¾à¤¨ à¤•à¤°à¤¨à¥‡, à¤­à¤¾à¤·à¤¾ à¤•à¥‹ à¤¸à¤®à¤à¤¨à¥‡, à¤”à¤° à¤¤à¤¯ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤•à¥à¤·à¤®à¤¤à¤¾ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¥€ à¤œà¤¾à¤¤à¥€ à¤¹à¥ˆà¥¤\n",
            "\n",
            "à¤‡à¤¸à¥‡ à¤¸à¤°à¤² à¤¶à¤¬à¥à¤¦à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¸à¤®à¤à¥‡ à¤¤à¥‹, à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¤¾ à¤…à¤°à¥à¤¥ à¤¹à¥ˆ â€” **à¤®à¤¶à¥€à¤¨à¥‹à¤‚ à¤®à¥‡à¤‚ \"à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾\" à¤•à¤¾ à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤•à¤°à¤¨à¤¾**à¥¤\n",
            "\n",
            "### à¤‰à¤¦à¤¾à¤¹à¤°à¤£:\n",
            "- **à¤¸à¤¿à¤°à¥€ (Siri), à¤…à¤²à¥‡à¤•à¥à¤¸à¤¾, à¤—à¥‚à¤—à¤² à¤…à¤¸à¤¿à¤¸à¥à¤Ÿà¥‡à¤‚à¤Ÿ**: à¤†à¤ªà¤•à¥€ à¤†à¤µà¤¾à¤œ à¤¸à¤®à¤à¤•à¤° à¤œà¤µà¤¾à¤¬ à¤¦à¥‡à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\n",
            "- **à¤šà¥‡à¤¹à¤°à¤¾ à¤ªà¤¹à¤šà¤¾à¤¨à¤¨à¤¾** (à¤«à¥‡à¤¸ à¤°à¤¿à¤•à¤—à¥à¤¨à¤¿à¤¶à¤¨): à¤¸à¥à¤®à¤¾à¤°à¥à¤Ÿà¤«à¥‹à¤¨ à¤•à¥‹ à¤–à¥‹à¤²à¤¨à¤¾à¥¤\n",
            "- **à¤¸à¥‡à¤²à¥à¤«-à¤¡à¥à¤°à¤¾à¤‡à¤µà¤¿à¤‚à¤— à¤•à¤¾à¤°**: à¤¬à¤¿à¤¨à¤¾ à¤¡à¥à¤°à¤¾à¤‡à¤µà¤° à¤•à¥‡ à¤°à¤¾à¤¸à¥à¤¤à¥‡ à¤ªà¤° à¤šà¤²à¤¨à¤¾à¥¤\n",
            "- **à¤šà¥ˆà¤Ÿà¤¬à¥‰à¤Ÿ**: à¤µà¥‡à¤¬à¤¸à¤¾à¤‡à¤Ÿ à¤ªà¤° à¤†à¤ªà¤•à¥‡ à¤¸à¤µà¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤œà¤µà¤¾à¤¬ à¤¦à¥‡à¤¨à¤¾à¥¤\n",
            "\n",
            "---\n",
            "\n",
            "### à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¥‡ à¤®à¥à¤–à¥à¤¯ à¤ªà¥à¤°à¤•à¤¾à¤°:\n",
            "\n",
            "1. **à¤¨à¥ˆà¤°à¥‹ AI (à¤•à¤®à¤œà¥‹à¤° AI):**  \n",
            "   à¤•à¥‡à¤µà¤² à¤à¤• à¤µà¤¿à¤¶à¥‡à¤· à¤•à¤¾à¤® à¤•à¥‡ à¤²à¤¿à¤ à¤¤à¥ˆà¤¯à¤¾à¤° à¤•à¥€ à¤—à¤ˆà¥¤ à¤‰à¤¦à¤¾à¤¹à¤°à¤£: à¤šà¥‡à¤¸ à¤–à¥‡à¤²à¤¨à¤¾, à¤šà¤¿à¤¤à¥à¤° à¤ªà¤¹à¤šà¤¾à¤¨à¤¨à¤¾à¥¤\n",
            "\n",
            "2. **à¤œà¤¨à¤°à¤² AI (à¤®à¤œà¤¬à¥‚à¤¤ AI):**  \n",
            "   à¤‡à¤‚à¤¸à¤¾à¤¨à¥‹à¤‚ à¤•à¥€ à¤¤à¤°à¤¹ à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤ªà¥à¤°à¤•à¤¾à¤° à¤•à¥‡ à¤¬à¥Œà¤¦à¥à¤§à¤¿à¤• à¤•à¤¾à¤°à¥à¤¯ à¤•à¤° à¤¸à¤•à¥‡ (à¤…à¤­à¥€ à¤¯à¤¹ à¤¸à¤¿à¤°à¥à¤« à¤•à¤²à¥à¤ªà¤¨à¤¾ à¤®à¥‡à¤‚ à¤¹à¥ˆ)à¥¤\n",
            "\n",
            "3. **à¤¸à¥à¤ªà¤° AI:**  \n",
            "   à¤œà¥‹ à¤‡à¤‚à¤¸à¤¾à¤¨ à¤¸à¥‡ à¤­à¥€ à¤œà¥à¤¯à¤¾à¤¦à¤¾ à¤¸à¤®à¤à¤¦à¤¾à¤° à¤¹à¥‹à¥¤ à¤¯à¤¹ à¤…à¤­à¥€ à¤•à¥‡à¤µà¤² à¤µà¥ˆà¤œà¥à¤žà¤¾à¤¨à¤¿à¤• à¤•à¤²à¥à¤ªà¤¨à¤¾ à¤®à¥‡à¤‚ à¤¹à¥ˆà¥¤\n",
            "\n",
            "---\n",
            "\n",
            "### AI à¤•à¥‡ à¤•à¥à¤› à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤˜à¤Ÿà¤•:\n",
            "- à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— (Machine Learning)\n",
            "- à¤—à¤¹à¤¨ à¤¶à¤¿à¤•à¥à¤·à¤£ (Deep Learning)\n",
            "- à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤­à¤¾à¤·à¤¾ à¤ªà¥à¤°à¤¸à¤‚à¤¸à¥à¤•à¤°à¤£ (NLP)\n",
            "- à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤µà¤¿à¤œà¤¼à¤¨\n",
            "\n",
            "---\n",
            "\n",
            "### à¤²à¤¾à¤­:\n",
            "- à¤•à¤¾à¤® à¤•à¥€ à¤—à¤¤à¤¿ à¤®à¥‡à¤‚ à¤µà¥ƒà¤¦à¥à¤§à¤¿\n",
            "- à¤®à¤¾à¤¨à¤µà¥€à¤¯ à¤—à¤²à¤¤à¤¿à¤¯à¤¾à¤‚ à¤•à¤® à¤¹à¥‹à¤¨à¤¾\n",
            "- 24x7 à¤‰à¤ªà¤²à¤¬à¥à¤§à¤¤à¤¾\n",
            "- à¤¬à¤¡à¤¼à¥‡ à¤¡à¥‡à¤Ÿà¤¾ à¤•à¤¾ à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£\n",
            "\n",
            "### à¤šà¥à¤¨à¥Œà¤¤à¤¿à¤¯à¤¾à¤:\n",
            "- à¤¨à¥Œà¤•à¤°à¤¿à¤¯à¥‹à¤‚ à¤ªà¤° à¤ªà¥à¤°à¤­à¤¾à¤µ\n",
            "- à¤—à¥‹à¤ªà¤¨à¥€à¤¯à¤¤à¤¾ à¤•à¤¾ à¤–à¤¤à¤°à¤¾\n",
            "- à¤¨à¥ˆà¤¤à¤¿à¤• à¤®à¥à¤¦à¥à¤¦à¥‡ (à¤œà¥ˆà¤¸à¥‡ AI à¤•à¥‹ à¤•à¥à¤¯à¤¾ à¤«à¥ˆà¤¸à¤²à¤¾ à¤²à¥‡à¤¨à¥‡ à¤•à¤¾ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤¹à¥‹?)\n",
            "\n",
            "---\n",
            "\n",
            "### à¤¨à¤¿à¤·à¥à¤•à¤°à¥à¤·:\n",
            "à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤†à¤œ à¤•à¥‡ à¤¡à¤¿à¤œà¤¿à¤Ÿà¤² à¤¯à¥à¤— à¤•à¥€ à¤¸à¤¬à¤¸à¥‡ à¤¶à¤•à¥à¤¤à¤¿à¤¶à¤¾à¤²à¥€ à¤¤à¤•à¤¨à¥€à¤•à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤à¤• à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤¹à¤®à¤¾à¤°à¥‡ à¤œà¥€à¤µà¤¨ à¤•à¥‹ à¤†à¤¸à¤¾à¤¨ à¤¤à¥‹ à¤¬à¤¨à¤¾ à¤°à¤¹à¥€ à¤¹à¥ˆ, à¤²à¥‡à¤•à¤¿à¤¨ à¤‡à¤¸à¤•à¥‡ à¤‰à¤¤à¥à¤¤à¤°à¤¦à¤¾à¤¯à¥€ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥€ à¤­à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤\n",
            "\n",
            "à¤…à¤—à¤° à¤†à¤ª à¤šà¤¾à¤¹à¥‡à¤‚, à¤¤à¥‹ à¤®à¥ˆà¤‚ AI à¤•à¥‡ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥‡ à¤•à¥à¤·à¥‡à¤¤à¥à¤° (à¤œà¥ˆà¤¸à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾, à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯, à¤•à¥ƒà¤·à¤¿) à¤ªà¤° à¤­à¥€ à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¥‡ à¤¬à¤¤à¤¾ à¤¸à¤•à¤¤à¤¾ à¤¹à¥‚à¤à¥¤\n",
            "\n",
            "Response in Spanish:\n",
            "La capital de Argentina es **Buenos Aires**.\n"
          ]
        }
      ],
      "source": [
        "# Example in Hindi\n",
        "hindi_prompt = \"à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\" # Translation: What is Artificial Intelligence?\n",
        "hindi_response = qwen_llm.invoke(hindi_prompt)\n",
        "print(f\"\"\"\n",
        "Response in Hindi:\n",
        "{hindi_response.content}\"\"\")\n",
        "\n",
        "# Example in Spanish\n",
        "spanish_prompt = \"Â¿CuÃ¡l es la capital de Argentina?\" # Translation: What is the capital of Argentina?\n",
        "spanish_response = qwen_llm.invoke(spanish_prompt)\n",
        "print(f\"\"\"\n",
        "Response in Spanish:\n",
        "{spanish_response.content}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAu9BXF46jiQ"
      },
      "source": [
        "### Advanced Parameter Tuning\n",
        "\n",
        "You can control the model's output by tuning parameters like `temperature` and `top_p`.\n",
        "\n",
        "- **`temperature`**: Controls randomness. Lower values (e.g., 0.1) make the output more deterministic, while higher values (e.g., 0.9) make it more creative.\n",
        "- **`top_p`**: Controls nucleus sampling. It considers only the tokens with the highest probability mass.\n",
        "- **`max_tokens`**: Sets the maximum length of the generated response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgpxAxGy6jiQ",
        "outputId": "3f8e0112-e6fd-435b-9faa-5aa4ffdb21a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creative Story:\n",
            "In the neon-glow sprawl of Neo-Tokyo 2147, where rain fell in shimmering data-veils and drones hummed like robotic cicadas, there lived a cat named **Neko-0**.\n",
            "\n",
            "Neko-0 wasn't like the other street-dwellers. Her fur was a sleek, obsidian black that absorbed light, embedded with micro-optical fibers that pulsed soft blue when she focused. Her eyes, large and luminous, didn't just see; they scanned. And beneath her right paw, a tiny, bio-integrated quantum parser allowed her to interface directly with code.\n",
            "\n",
            "She belonged to no one, roamed the forgotten server tunnels beneath the city, where the old system,\n",
            "\n",
            "Factual Explanation:\n",
            "Sure! The theory of relativity, developed by Albert Einstein, is made up of two parts: **Special Relativity** and **General Relativity**. Here's a simple explanation of both:\n",
            "\n",
            "### 1. **Special Relativity (1905)**\n",
            "This part deals with objects moving at constant speeds, especially very fast onesâ€”close to the speed of light.\n",
            "\n",
            "Key ideas:\n",
            "- **The laws of physics are the same for everyone moving at a constant speed.** Whether you're standing still or moving in a smooth, fast train, physics works the same way.\n",
            "- **The speed of light is constant.** No matter how fast you're moving, light always travels at about 300,000 km per second in a vacuum.\n",
            "- **Time and space are relative.** This means time can slow down and lengths can shorten when you're moving very fast. For example, if you were traveling in a spaceship near the speed of light, time would pass slower for you than for someone on Earth.\n",
            "\n",
            "ðŸ‘‰ Think of it like this: If you're on a super-fast spaceship, you might age slower than your twin back on Earth. This is called \"time dilation.\"\n",
            "\n",
            "---\n",
            "\n",
            "### 2. **General Relativity (1915)**\n",
            "This part includes gravity and acceleration.\n",
            "\n",
            "Key idea:\n",
            "- **Gravity is not just a forceâ€”it's the bending of space and time.** Imagine space and time as a stretchy fabric. When a heavy object (like a planet or star) sits on it, it creates a dent. Other objects roll toward that dent, which we see as gravity.\n",
            "\n",
            "ðŸ‘‰ For example, Earth orbits the Sun not because the Sun \"pulls\" it with a force, but because the Sun's mass bends the space around it, and Earth follows that curve.\n",
            "\n",
            "---\n",
            "\n",
            "### In Simple Terms:\n",
            "- **Special Relativity:** When things move really fast, time slows down and distances shrink.\n",
            "- **General Relativity:** Gravity happens because mass bends the \"fabric\" of space and time.\n",
            "\n",
            "Together, these theories changed how we understand time, space, and gravityâ€”showing they're all connected! ðŸŒŒâ³ðŸš€\n"
          ]
        }
      ],
      "source": [
        "# Creative response with high temperature\n",
        "creative_llm = ChatOpenAI(\n",
        "    model=\"qwen/qwen3-235b-a22b-07-25:free\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.9,\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "prompt = \"Write a short, futuristic story about a cat who can code.\"\n",
        "creative_response = creative_llm.invoke(prompt)\n",
        "print(f\"\"\"Creative Story:\n",
        "{creative_response.content}\"\"\")\n",
        "\n",
        "# Factual response with low temperature\n",
        "factual_llm = ChatOpenAI(\n",
        "    model=\"qwen/qwen3-235b-a22b-07-25:free\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "prompt = \"Explain the theory of relativity in simple terms.\"\n",
        "factual_response = factual_llm.invoke(prompt)\n",
        "print(f\"\"\"\n",
        "Factual Explanation:\n",
        "{factual_response.content}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YILCRBuG6jiQ"
      },
      "source": [
        "### Building a Simple Chatbot\n",
        "\n",
        "We can create a simple conversational chatbot by managing the chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksog2Dsr6jiR",
        "outputId": "6275a17a-23f5-4877-ea9c-bb842b8a0857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pirate Bot: Ahoy there, matey! I be known as Cap'n Qwen, the ghostly keeper of the Seven Seas of Knowledge! ðŸ´â€â˜ ï¸ With a compass in one hand and a quill in the other, I chart courses through treasure-laden tales, buried secrets, and wisdom from distant isles. Need to decode a riddle, find a lost artifact, or learn the ways of the high seas? Just say the word, and weâ€™ll set sail together! ðŸŒŠâš“ What adventure calls to ye today, me heartie?\n",
            "Pirate Bot: Arrr, I be a savvy sea ghost, but even I canâ€™t see the clouds from beneath the waves without a proper spyglass! ðŸŒ«ï¸ðŸ”® To know the weather on *your* patch oâ€™ land or sea, Iâ€™ll need ye to tell me yer **location**, me hearty!\n",
            "\n",
            "But once ye doâ€”wind, rain, sun, or stormâ€”Iâ€™ll give ye the forecast like a true weather-worn pirate! So speak up, ye barnacle-biterâ€”where be ye anchored today? ðŸŒ¤ï¸â›ˆï¸ðŸŒ¤ï¸\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that speaks like a pirate.\"),\n",
        "    HumanMessage(content=\"Ahoy! What's your name?\"),\n",
        "]\n",
        "\n",
        "# First turn\n",
        "response = qwen_llm.invoke(messages)\n",
        "print(f\"Pirate Bot: {response.content}\")\n",
        "\n",
        "# Add the bot's response to the history\n",
        "messages.append(response)\n",
        "\n",
        "# Second turn\n",
        "messages.append(HumanMessage(content=\"What's the weather like today?\"))\n",
        "response = qwen_llm.invoke(messages)\n",
        "print(f\"Pirate Bot: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTVyloRO6jiR"
      },
      "source": [
        "##Additional Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tool Calling and Tavily Search Using Qwen"
      ],
      "metadata": {
        "id": "HC59dEUCHWoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "\n",
        "# 1. Setup LLM via OpenRouter\n",
        "llm = ChatOpenAI(\n",
        "    model=\"qwen/qwen3-235b-a22b\", # Check OpenRouter.ai for valid IDs\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    openai_api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        ")\n",
        "\n",
        "# 2. Setup Tavily Search Tool\n",
        "tavily_tool = TavilySearchResults(max_results=2)\n",
        "\n",
        "# 3. Create Agent\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "agent = create_tool_calling_agent(llm, [tavily_tool], prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=[tavily_tool], verbose=True)\n",
        "\n",
        "# 4. Run Agent\n",
        "response = agent_executor.invoke({\"input\": \"How do central bank interest rate changes affect the stock market?\"})\n",
        "print(response['output'])"
      ],
      "metadata": {
        "id": "bfug1PTgHU1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy2X3T7U6jiR"
      },
      "source": [
        "###  Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZvB_lvr6jiR",
        "outputId": "f71865d7-da41-4ed6-bc1f-35f8fb89fb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "Here's a Python function that takes a list of numbers and returns the sum of all even numbers:\n",
            "\n",
            "```python\n",
            "def sum_even_numbers(numbers):\n",
            "    \"\"\"\n",
            "    Returns the sum of all even numbers in the given list.\n",
            "    \n",
            "    Args:\n",
            "        numbers (list): A list of numbers (int or float)\n",
            "    \n",
            "    Returns:\n",
            "        int or float: The sum of all even numbers in the list\n",
            "    \"\"\"\n",
            "    return sum(num for num in numbers if num % 2 == 0)\n",
            "\n",
            "# Example usage:\n",
            "if __name__ == \"__main__\":\n",
            "    # Test the function\n",
            "    sample_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "    result = sum_even_numbers(sample_list)\n",
            "    print(f\"Sum of even numbers in {sample_list}: {result}\")  # Output: 30\n",
            "    \n",
            "    # More test cases\n",
            "    print(sum_even_numbers([1, 3, 5, 7]))        # Output: 0 (no even numbers)\n",
            "    print(sum_even_numbers([2, 4, 6, 8]))        # Output: 20\n",
            "    print(sum_even_numbers([]))                  # Output: 0 (empty list)\n",
            "    print(sum_even_numbers([-2, -1, 0, 1, 2]))   # Output: 0 (-2 + 0 + 2)\n",
            "```\n",
            "\n",
            "**Alternative implementations:**\n",
            "\n",
            "```python\n",
            "# Version 2: Using a traditional for loop\n",
            "def sum_even_numbers_v2(numbers):\n",
            "    total = 0\n",
            "    for num in numbers:\n",
            "        if num % 2 == 0:\n",
            "            total += num\n",
            "    return total\n",
            "\n",
            "# Version 3: Using filter and sum\n",
            "def sum_even_numbers_v3(numbers):\n",
            "    return sum(filter(lambda x: x % 2 == 0, numbers))\n",
            "```\n",
            "\n",
            "The first version uses a generator expression with `sum()`, which is concise and memory-efficient. The function works with both integers and floats, and handles edge cases like empty lists and negative numbers correctly.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\"\n",
        "code_response = qwen_llm.invoke(prompt)\n",
        "print(f\"\"\"Generated Code:\n",
        "{code_response.content}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t05mJ526jiS"
      },
      "source": [
        "###  Few-Shot Prompting\n",
        "\n",
        "Few-shot prompting provides the model with examples to guide its response format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDUW2gov6jiS",
        "outputId": "5be502ff-db85-4071-94cb-bd24b6899c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation of 'house': \"house -> maison\"\n"
          ]
        }
      ],
      "source": [
        "prompt = (\n",
        "    \"\"\"Translate the following English words to French:\n",
        "\n",
        "    \"sea -> mer\"\n",
        "    \"sky -> ciel\"\n",
        "    \"book -> livre\"\n",
        "    \"house ->\"\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "few_shot_response = qwen_llm.invoke(prompt)\n",
        "print(f\"Translation of 'house': {few_shot_response.content}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}